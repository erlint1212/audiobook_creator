Project Directory Structure:
============================
./
    requirements.txt
    README.md
    scraper_context_fetcher.py
    qwen_tts_generator.py
    scraper_2.py
    tag_audiobook_files_opus_3.py
    grok_transelate.py
    gemini_transelate_4.py
    environment.yml
    metadata_fetcher.py
    alltalk_path_config.json
    shell_old_working.nix
    alltalk_tts_generator_chunky_17.py
    constants.py
    test_env.py
    test.log
    convert_audio_to_opus_3.py
    txt_to_epub.py
    fetch_metadata.py
    context_builder.py
    .gitignore
    shell.nix
    pipe_system_gui.py
    Novels/
        Villainous_Saintess_CH/
            Villainous_Saintess_CH.epub
            custom_scraper.py
            custom_metadata_scraper.py
            03_Audio_WAV/
            02_Translated/
            04_Audio_Opus/
            01_Raw_Text/
                ch_0188.txt
                ch_0102.txt
                ch_0263.txt
                ch_0109.txt
                ch_0338.txt
                ch_0307.txt
                ch_0390.txt
                ch_0419.txt
                ch_0213.txt
                ch_0372.txt
                ch_0028.txt
                ch_0376.txt
                ch_0302.txt
                ch_0437.txt
                ch_0399.txt
                ch_0270.txt
                ch_0415.txt
                ch_0092.txt
                ch_0218.txt
                ch_0336.txt
                ch_0077.txt
                ch_0435.txt
                ch_0203.txt
                ch_0278.txt
                ch_0125.txt
                ch_0088.txt
                ch_0421.txt
                ch_0144.txt
                ch_0311.txt
                ch_0329.txt
                ch_0281.txt
                ch_0224.txt
                ch_0347.txt
                ch_0062.txt
                ch_0185.txt
                ch_0153.txt
                ch_0382.txt
                ch_0058.txt
                ch_0358.txt
                ch_0271.txt
                ch_0233.txt
                ch_0134.txt
                ch_0416.txt
                ch_0166.txt
                ch_0130.txt
                ch_0198.txt
                ch_0146.txt
                ch_0315.txt
                ch_0067.txt
                ch_0049.txt
                ch_0442.txt
                ch_0202.txt
                ch_0216.txt
                ch_0199.txt
                ch_0074.txt
                ch_0145.txt
                ch_0397.txt
                ch_0009.txt
                ch_0097.txt
                ch_0138.txt
                ch_0321.txt
                ch_0434.txt
                ch_0187.txt
                ch_0320.txt
                ch_0094.txt
                ch_0032.txt
                ch_0400.txt
                ch_0423.txt
                ch_0388.txt
                ch_0300.txt
                ch_0050.txt
                ch_0183.txt
                ch_0012.txt
                ch_0072.txt
                ch_0194.txt
                ch_0055.txt
                ch_0064.txt
                ch_0337.txt
                ch_0022.txt
                ch_0422.txt
                ch_0175.txt
                ch_0078.txt
                ch_0370.txt
                ch_0086.txt
                ch_0242.txt
                ch_0318.txt
                ch_0308.txt
                ch_0356.txt
                ch_0286.txt
                ch_0170.txt
                ch_0046.txt
                ch_0284.txt
                ch_0027.txt
                ch_0014.txt
                ch_0217.txt
                ch_0245.txt
                ch_0007.txt
                ch_0192.txt
                ch_0033.txt
                ch_0450.txt
                ch_0380.txt
                ch_0069.txt
                ch_0111.txt
                ch_0177.txt
                ch_0127.txt
                ch_0179.txt
                ch_0349.txt
                ch_0060.txt
                ch_0114.txt
                ch_0374.txt
                ch_0384.txt
                ch_0369.txt
                ch_0451.txt
                ch_0276.txt
                ch_0165.txt
                ch_0123.txt
                ch_0080.txt
                ch_0159.txt
                ch_0248.txt
                ch_0117.txt
                ch_0246.txt
                ch_0025.txt
                ch_0352.txt
                ch_0409.txt
                ch_0381.txt
                ch_0149.txt
                ch_0013.txt
                ch_0104.txt
                ch_0256.txt
                ch_0351.txt
                ch_0065.txt
                ch_0444.txt
                ch_0335.txt
                ch_0083.txt
                ch_0096.txt
                ch_0440.txt
                ch_0375.txt
                ch_0122.txt
                ch_0231.txt
                ch_0379.txt
                ch_0082.txt
                ch_0310.txt
                ch_0424.txt
                ch_0317.txt
                ch_0290.txt
                ch_0100.txt
                ch_0366.txt
                ch_0195.txt
                ch_0006.txt
                ch_0035.txt
                ch_0340.txt
                ch_0257.txt
                ch_0076.txt
                ch_0045.txt
                ch_0108.txt
                ch_0237.txt
                ch_0143.txt
                ch_0362.txt
                ch_0408.txt
                ch_0222.txt
                ch_0363.txt
                ch_0164.txt
                ch_0365.txt
                ch_0432.txt
                ch_0085.txt
                ch_0439.txt
                ch_0184.txt
                ch_0057.txt
                ch_0091.txt
                ch_0011.txt
                ch_0093.txt
                ch_0323.txt
                ch_0402.txt
                ch_0272.txt
                ch_0131.txt
                ch_0228.txt
                ch_0394.txt
                ch_0251.txt
                ch_0038.txt
                ch_0292.txt
                ch_0066.txt
                ch_0158.txt
                ch_0291.txt
                ch_0396.txt
                ch_0034.txt
                ch_0040.txt
                ch_0070.txt
                ch_0448.txt
                ch_0152.txt
                ch_0334.txt
                ch_0226.txt
                ch_0353.txt
                ch_0106.txt
                ch_0293.txt
                ch_0156.txt
                ch_0135.txt
                ch_0265.txt
                ch_0411.txt
                ch_0113.txt
                ch_0355.txt
                ch_0254.txt
                ch_0260.txt
                ch_0436.txt
                ch_0221.txt
                ch_0294.txt
                ch_0043.txt
                ch_0031.txt
                ch_0044.txt
                ch_0345.txt
                ch_0295.txt
                ch_0017.txt
                ch_0155.txt
                ch_0208.txt
                ch_0413.txt
                ch_0333.txt
                ch_0377.txt
                ch_0023.txt
                ch_0418.txt
                ch_0259.txt
                ch_0367.txt
                ch_0330.txt
                ch_0133.txt
                ch_0391.txt
                ch_0232.txt
                ch_0262.txt
                ch_0142.txt
                ch_0387.txt
                ch_0326.txt
                ch_0360.txt
                ch_0018.txt
                ch_0024.txt
                ch_0364.txt
                ch_0116.txt
                ch_0301.txt
                ch_0393.txt
                ch_0039.txt
                ch_0430.txt
                chapters.json
                ch_0238.txt
                ch_0389.txt
                ch_0305.txt
                ch_0426.txt
                ch_0041.txt
                ch_0341.txt
                ch_0414.txt
                ch_0005.txt
                ch_0137.txt
                ch_0207.txt
                ch_0289.txt
                ch_0386.txt
                ch_0176.txt
                ch_0008.txt
                ch_0051.txt
                ch_0283.txt
                ch_0053.txt
                ch_0417.txt
                ch_0157.txt
                ch_0350.txt
                ch_0147.txt
                ch_0403.txt
                ch_0297.txt
                ch_0209.txt
                ch_0174.txt
                ch_0359.txt
                ch_0427.txt
                ch_0327.txt
                ch_0250.txt
                ch_0047.txt
                ch_0075.txt
                ch_0150.txt
                ch_0378.txt
                ch_0003.txt
                ch_0148.txt
                ch_0319.txt
                ch_0211.txt
                ch_0252.txt
                ch_0324.txt
                ch_0081.txt
                ch_0244.txt
                ch_0087.txt
                ch_0056.txt
                ch_0099.txt
                ch_0105.txt
                ch_0303.txt
                ch_0314.txt
                ch_0306.txt
                ch_0090.txt
                ch_0312.txt
                ch_0296.txt
                ch_0101.txt
                ch_0412.txt
                ch_0181.txt
                ch_0190.txt
                ch_0189.txt
                ch_0298.txt
                ch_0325.txt
                ch_0227.txt
                ch_0225.txt
                ch_0371.txt
                ch_0071.txt
                ch_0441.txt
                ch_0343.txt
                ch_0182.txt
                ch_0405.txt
                ch_0241.txt
                ch_0406.txt
                ch_0401.txt
                ch_0383.txt
                ch_0112.txt
                ch_0410.txt
                ch_0373.txt
                ch_0186.txt
                ch_0063.txt
                ch_0395.txt
                ch_0220.txt
                ch_0229.txt
                ch_0197.txt
                ch_0161.txt
                ch_0234.txt
                ch_0274.txt
                ch_0167.txt
                ch_0277.txt
                ch_0253.txt
                ch_0240.txt
                ch_0361.txt
                ch_0173.txt
                ch_0059.txt
                ch_0201.txt
                ch_0042.txt
                ch_0180.txt
                ch_0264.txt
                ch_0339.txt
                ch_0282.txt
                ch_0118.txt
                ch_0255.txt
                ch_0119.txt
                ch_0404.txt
                ch_0247.txt
                ch_0136.txt
                ch_0392.txt
                ch_0030.txt
                ch_0344.txt
                ch_0036.txt
                ch_0172.txt
                ch_0420.txt
                ch_0357.txt
                ch_0249.txt
                ch_0098.txt
                ch_0160.txt
                ch_0447.txt
                ch_0428.txt
                ch_0052.txt
                ch_0128.txt
                ch_0269.txt
                ch_0171.txt
                ch_0001.txt
                ch_0120.txt
                ch_0103.txt
                ch_0433.txt
                ch_0212.txt
                ch_0073.txt
                ch_0261.txt
                ch_0425.txt
                ch_0048.txt
                ch_0020.txt
                ch_0019.txt
                ch_0139.txt
                ch_0084.txt
                ch_0348.txt
                ch_0449.txt
                ch_0275.txt
                ch_0154.txt
                ch_0438.txt
                ch_0204.txt
                ch_0316.txt
                ch_0398.txt
                ch_0215.txt
                ch_0236.txt
                ch_0151.txt
                ch_0206.txt
                ch_0368.txt
                ch_0021.txt
                ch_0309.txt
                ch_0037.txt
                ch_0163.txt
                ch_0322.txt
                ch_0015.txt
                ch_0313.txt
                ch_0429.txt
                ch_0026.txt
                ch_0443.txt
                ch_0385.txt
                ch_0267.txt
                ch_0029.txt
                ch_0214.txt
                ch_0124.txt
                ch_0140.txt
                ch_0089.txt
                ch_0243.txt
                ch_0445.txt
                ch_0268.txt
                ch_0196.txt
                ch_0068.txt
                ch_0287.txt
                ch_0141.txt
                ch_0054.txt
                ch_0129.txt
                ch_0107.txt
                ch_0095.txt
                ch_0010.txt
                ch_0169.txt
                ch_0304.txt
                ch_0354.txt
                ch_0235.txt
                ch_0280.txt
                ch_0132.txt
                ch_0328.txt
                ch_0285.txt
                ch_0446.txt
                ch_0342.txt
                ch_0431.txt
                ch_0162.txt
                ch_0273.txt
                ch_0205.txt
                ch_0346.txt
                ch_0288.txt
                ch_0079.txt
                ch_0258.txt
                ch_0223.txt
                ch_0061.txt
                ch_0332.txt
                ch_0115.txt
                ch_0210.txt
                ch_0279.txt
                ch_0193.txt
                ch_0230.txt
                ch_0200.txt
                ch_0178.txt
                ch_0126.txt
                ch_0299.txt
                ch_0239.txt
                ch_0331.txt
                ch_0121.txt
                ch_0004.txt
                ch_0002.txt
                ch_0016.txt
                ch_0110.txt
                ch_0266.txt
                ch_0407.txt
                ch_0219.txt
                ch_0191.txt
                ch_0168.txt
            Scraper_Context/
                index_structure.html
                site_structure.html
        youngfox/
            custom_scraper.py
            metadata.json
            cover.jpg
            custom_metadata_scraper.py
            03_Audio_WAV/
            02_Translated/
            04_Audio_Opus/
            01_Raw_Text/
                ch_0188.txt
                ch_0102.txt
                ch_0263.txt
                ch_0109.txt
                ch_0307.txt
                ch_0213.txt
                ch_0028.txt
                ch_0302.txt
                ch_0270.txt
                ch_0092.txt
                ch_0218.txt
                ch_0077.txt
                ch_0203.txt
                ch_0278.txt
                ch_0125.txt
                ch_0088.txt
                ch_0144.txt
                ch_0311.txt
                ch_0281.txt
                ch_0224.txt
                ch_0062.txt
                ch_0185.txt
                ch_0153.txt
                ch_0058.txt
                ch_0271.txt
                ch_0233.txt
                ch_0134.txt
                ch_0166.txt
                ch_0130.txt
                ch_0198.txt
                ch_0146.txt
                ch_0067.txt
                ch_0049.txt
                ch_0202.txt
                ch_0216.txt
                ch_0199.txt
                ch_0074.txt
                ch_0145.txt
                ch_0009.txt
                ch_0097.txt
                ch_0138.txt
                ch_0187.txt
                ch_0094.txt
                ch_0032.txt
                ch_0300.txt
                ch_0050.txt
                ch_0183.txt
                ch_0012.txt
                ch_0072.txt
                ch_0194.txt
                ch_0055.txt
                ch_0064.txt
                ch_0022.txt
                ch_0175.txt
                ch_0078.txt
                ch_0086.txt
                ch_0242.txt
                ch_0308.txt
                ch_0286.txt
                ch_0170.txt
                ch_0046.txt
                ch_0284.txt
                ch_0027.txt
                ch_0014.txt
                ch_0217.txt
                ch_0245.txt
                ch_0007.txt
                ch_0192.txt
                ch_0033.txt
                ch_0069.txt
                ch_0111.txt
                ch_0177.txt
                ch_0127.txt
                ch_0179.txt
                ch_0060.txt
                ch_0114.txt
                ch_0276.txt
                ch_0165.txt
                ch_0123.txt
                ch_0080.txt
                ch_0159.txt
                ch_0248.txt
                ch_0117.txt
                ch_0246.txt
                ch_0025.txt
                ch_0149.txt
                ch_0013.txt
                ch_0104.txt
                ch_0256.txt
                ch_0065.txt
                ch_0083.txt
                ch_0096.txt
                ch_0122.txt
                ch_0231.txt
                ch_0082.txt
                ch_0310.txt
                ch_0290.txt
                ch_0100.txt
                ch_0195.txt
                ch_0006.txt
                ch_0035.txt
                ch_0257.txt
                ch_0076.txt
                ch_0045.txt
                ch_0108.txt
                ch_0237.txt
                ch_0143.txt
                ch_0222.txt
                ch_0164.txt
                ch_0085.txt
                ch_0184.txt
                ch_0057.txt
                ch_0091.txt
                ch_0011.txt
                ch_0093.txt
                ch_0272.txt
                ch_0131.txt
                ch_0228.txt
                ch_0251.txt
                ch_0038.txt
                ch_0292.txt
                ch_0066.txt
                ch_0158.txt
                ch_0291.txt
                ch_0034.txt
                ch_0040.txt
                ch_0070.txt
                ch_0152.txt
                ch_0226.txt
                ch_0106.txt
                ch_0293.txt
                ch_0156.txt
                ch_0135.txt
                ch_0265.txt
                ch_0113.txt
                ch_0254.txt
                ch_0260.txt
                ch_0221.txt
                ch_0294.txt
                ch_0043.txt
                ch_0031.txt
                ch_0044.txt
                ch_0295.txt
                ch_0017.txt
                ch_0155.txt
                ch_0208.txt
                ch_0023.txt
                ch_0259.txt
                ch_0133.txt
                ch_0232.txt
                ch_0262.txt
                ch_0142.txt
                ch_0018.txt
                ch_0024.txt
                ch_0116.txt
                ch_0301.txt
                ch_0039.txt
                chapters.json
                ch_0238.txt
                ch_0305.txt
                ch_0041.txt
                ch_0005.txt
                ch_0137.txt
                ch_0207.txt
                ch_0289.txt
                ch_0176.txt
                ch_0008.txt
                ch_0051.txt
                ch_0283.txt
                ch_0053.txt
                ch_0157.txt
                ch_0147.txt
                ch_0297.txt
                ch_0209.txt
                ch_0174.txt
                ch_0250.txt
                ch_0047.txt
                ch_0075.txt
                ch_0150.txt
                ch_0003.txt
                ch_0148.txt
                ch_0211.txt
                ch_0252.txt
                ch_0081.txt
                ch_0244.txt
                ch_0087.txt
                ch_0056.txt
                ch_0099.txt
                ch_0105.txt
                ch_0303.txt
                ch_0314.txt
                ch_0306.txt
                ch_0090.txt
                ch_0312.txt
                ch_0296.txt
                ch_0101.txt
                ch_0181.txt
                ch_0190.txt
                ch_0189.txt
                ch_0298.txt
                ch_0227.txt
                ch_0225.txt
                ch_0071.txt
                ch_0182.txt
                ch_0241.txt
                ch_0112.txt
                ch_0186.txt
                ch_0063.txt
                ch_0220.txt
                ch_0229.txt
                ch_0197.txt
                ch_0161.txt
                ch_0234.txt
                ch_0274.txt
                ch_0167.txt
                ch_0277.txt
                ch_0253.txt
                ch_0240.txt
                ch_0173.txt
                ch_0059.txt
                ch_0201.txt
                ch_0042.txt
                ch_0180.txt
                ch_0264.txt
                ch_0282.txt
                ch_0118.txt
                ch_0255.txt
                ch_0119.txt
                ch_0247.txt
                ch_0136.txt
                ch_0030.txt
                ch_0036.txt
                ch_0172.txt
                ch_0249.txt
                ch_0098.txt
                ch_0160.txt
                ch_0052.txt
                ch_0128.txt
                ch_0269.txt
                ch_0171.txt
                ch_0001.txt
                ch_0120.txt
                ch_0103.txt
                ch_0212.txt
                ch_0073.txt
                ch_0261.txt
                ch_0048.txt
                ch_0020.txt
                ch_0019.txt
                ch_0139.txt
                ch_0084.txt
                ch_0275.txt
                ch_0154.txt
                ch_0204.txt
                ch_0215.txt
                ch_0236.txt
                ch_0151.txt
                ch_0206.txt
                ch_0021.txt
                ch_0309.txt
                ch_0037.txt
                ch_0163.txt
                ch_0015.txt
                ch_0313.txt
                ch_0026.txt
                ch_0267.txt
                ch_0029.txt
                ch_0214.txt
                ch_0124.txt
                ch_0140.txt
                ch_0089.txt
                ch_0243.txt
                ch_0268.txt
                ch_0196.txt
                ch_0068.txt
                ch_0287.txt
                ch_0141.txt
                ch_0054.txt
                ch_0129.txt
                ch_0107.txt
                ch_0095.txt
                ch_0010.txt
                ch_0169.txt
                ch_0304.txt
                ch_0235.txt
                ch_0280.txt
                ch_0132.txt
                ch_0285.txt
                ch_0162.txt
                ch_0273.txt
                ch_0205.txt
                ch_0288.txt
                ch_0079.txt
                ch_0258.txt
                ch_0223.txt
                ch_0061.txt
                ch_0115.txt
                ch_0210.txt
                ch_0279.txt
                ch_0193.txt
                ch_0230.txt
                ch_0200.txt
                ch_0178.txt
                ch_0126.txt
                ch_0299.txt
                ch_0239.txt
                ch_0121.txt
                ch_0004.txt
                ch_0002.txt
                ch_0016.txt
                ch_0110.txt
                ch_0266.txt
                ch_0219.txt
                ch_0191.txt
                ch_0168.txt
        IAmThisMurimsCrazyBitch/
            custom_scraper.py
            03_Audio_WAV/
            02_Translated/
            04_Audio_Opus/
            01_Raw_Text/
                ch_0995.txt
                ch_0973.txt
                ch_0961.txt
                ch_0989.txt
                ch_0992.txt
                ch_0976.txt
                ch_0982.txt
                ch_0997.txt
                ch_0979.txt
                ch_0984.txt
                ch_0971.txt
                ch_0965.txt
                ch_0985.txt
                ch_0980.txt
                ch_0998.txt
                ch_0993.txt
                ch_0970.txt
                ch_0988.txt
                ch_0977.txt
                ch_0990.txt
                chapters.json
                ch_0969.txt
                ch_0986.txt
                clean_chapters.py
                ch_0981.txt
                ch_0972.txt
                ch_0966.txt
                ch_0991.txt
                ch_0983.txt
                ch_0975.txt
                ch_0996.txt
                ch_0962.txt
                ch_0968.txt
                ch_0994.txt
                ch_0987.txt
                ch_0978.txt
                ch_0974.txt
                ch_0964.txt
                ch_0963.txt
                ch_0967.txt
                backup/
                    ch_0995.txt
                    ch_0973.txt
                    ch_0961.txt
                    ch_0989.txt
                    ch_0992.txt
                    ch_0976.txt
                    ch_0982.txt
                    ch_0997.txt
                    ch_0979.txt
                    ch_0984.txt
                    ch_0971.txt
                    ch_0965.txt
                    ch_0985.txt
                    ch_0980.txt
                    ch_0998.txt
                    ch_0993.txt
                    ch_0970.txt
                    ch_0988.txt
                    ch_0977.txt
                    ch_0990.txt
                    ch_0969.txt
                    ch_0986.txt
                    ch_0981.txt
                    ch_0972.txt
                    ch_0966.txt
                    ch_0991.txt
                    ch_0983.txt
                    ch_0975.txt
                    ch_0996.txt
                    ch_0962.txt
                    ch_0968.txt
                    ch_0994.txt
                    ch_0987.txt
                    ch_0978.txt
                    ch_0974.txt
                    ch_0964.txt
                    ch_0963.txt
                    ch_0967.txt
            Scraper_Context/
                site_structure.html
        god_slaughtering_star/
            custom_scraper.py
            03_Audio_WAV/
            02_Translated/
            04_Audio_Opus/
            01_Raw_Text/
                ch_0009.txt
                ch_0012.txt
                ch_0007.txt
                ch_0013.txt
                ch_0006.txt
                ch_0011.txt
                chapters.json
                ch_0005.txt
                ch_0008.txt
                ch_0003.txt
                ch_0001.txt
                ch_0010.txt
                ch_0004.txt
                ch_0002.txt
            Scraper_Context/
                site_structure.html
    backup/
        scraper_context_fetcher.py


File Contents:
==============

--- START OF FILE: ./scraper_context_fetcher.py ---
import os
import re
import sys
from urllib.parse import urlparse

import requests

# --- FIX 1: Safer Import Handling ---
try:
    import google.generativeai as genai

    from constants import GEMINI_MODEL_NAME

    GENAI_AVAILABLE = True
except ImportError:
    GEMINI_MODEL_NAME = "gemini-3-flash-preview"
    GENAI_AVAILABLE = False


def extract_code_block(response_text):
    pattern = r"```python\s*(.*?)\s*```"
    match = re.search(pattern, response_text, re.DOTALL)
    if match:
        return match.group(1)
    return response_text


def fetch_and_generate_scraper(
    target_url, project_root_dir, reference_scraper="scraper_2.py"
):
    # Guard check for API availability
    if not GENAI_AVAILABLE:
        raise Exception(
            "Google Generative AI package is not installed. Please run: pip install google-generativeai"
        )

    context_dir = os.path.join(project_root_dir, "Scraper_Context")
    if not os.path.exists(context_dir):
        os.makedirs(context_dir)

    print(f"--- 1. Fetching HTML for: {target_url} ---")
    headers = {"User-Agent": "Mozilla/5.0"}

    html_content = ""
    try:
        response = requests.get(target_url, headers=headers, timeout=15)
        response.raise_for_status()
        html_content = response.text
        with open(
            os.path.join(context_dir, "site_structure.html"), "w", encoding="utf-8"
        ) as f:
            f.write(html_content)
    except Exception as e:
        raise Exception(f"Error fetching URL: {e}")

    print(f"--- 2. Reading Reference Scraper ---")
    reference_code = ""
    if os.path.exists(reference_scraper):
        with open(reference_scraper, "r", encoding="utf-8") as f:
            reference_code = f.read()
    else:
        raise Exception(f"Reference scraper '{reference_scraper}' not found.")

    print(f"--- 3. Sending to Gemini ({GEMINI_MODEL_NAME}) ---")
    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        raise Exception("GEMINI_API_KEY environment variable not set.")

    genai.configure(api_key=api_key)

    prompt = f"""
    You are an expert Python web scraping developer.
    
    I need you to write a NEW Python script to scrape a specific web novel.
    
    --- REFERENCE SCRAPER (scraper_2.py) ---
    The following code is a working example. Reuse the file saving, os.getenv logic, and loop structure.
    
    {reference_code}
    
    --- TARGET WEBSITE HTML ---
    Here is the HTML source code of the first chapter. 
    Use BeautifulSoup to parse this structure. 
    
    {html_content[:55000]}
    
    --- CRITICAL INSTRUCTIONS ---
    1. **CLEAN CONTENT:** The text saved to the .txt file MUST ONLY contain the Chapter Header and the Story Body.
       - **Remove** "Previous/Next" text, "Read at..." watermarks, and social media buttons from the body.
    
    2. **DEDUPLICATION:** - Check if the first line of the body content matches the Chapter Title.
       - **If it matches, remove it** from the body content to avoid duplication in the output file.
    
    3. **STRICT FORMAT:** `f.write(f"{{full_header}}\\n\\n{{cleaned_body}}")`
    
    4. **NEXT CHAPTER LOGIC (Crucial):**
       - **Priority 1:** Look for `<a href="..." rel="next">`. This is the most reliable method.
       - **Priority 2:** Look for an `<a>` tag inside a "nav" or "pager" div that contains the text "Next".
       - Ensure the loop breaks cleanly if no next link is found.

    5. Output ONLY the complete, runnable Python code.
    """

    try:
        model = genai.GenerativeModel(GEMINI_MODEL_NAME)
        # --- FIX 2: Added 120-second timeout to prevent infinite hanging ---
        response = model.generate_content(prompt, request_options={"timeout": 120})

        generated_code = extract_code_block(response.text)

        output_scraper_path = os.path.join(project_root_dir, "custom_scraper.py")
        with open(output_scraper_path, "w", encoding="utf-8") as f:
            f.write(generated_code)

        print(f"--- SUCCESS! ---")
        print(f"New scraper saved to: {output_scraper_path}")

    except Exception as e:
        # Re-raise the exception so the GUI background thread catches it
        raise Exception(f"Gemini API Error: {e}")


if __name__ == "__main__":
    if len(sys.argv) > 2:
        try:
            fetch_and_generate_scraper(sys.argv[1], sys.argv[2])
        except Exception as e:
            print(f"Fatal Error: {e}")
    else:
        print("Usage: python scraper_context_fetcher.py <url> <project_dir>")

--- END OF FILE: ./scraper_context_fetcher.py ---

--- START OF FILE: ./qwen_tts_generator.py ---
import argparse
import glob
import math
import os
import re
import shutil
import sys
import time

import soundfile as sf
import torch

if sys.platform == "win32":
    sys.stdout.reconfigure(encoding="utf-8")

import nltk
from nltk.tokenize import sent_tokenize

# --- Import AI Models ---
from qwen_tts import Qwen3TTSModel
from rvc_python.infer import RVCInference

# --- NLTK Setup ---
NLTK_SETUP_SUCCESSFUL = False
try:
    nltk.sent_tokenize("This is a test.")
    NLTK_SETUP_SUCCESSFUL = True
except LookupError:
    nltk.download("punkt", quiet=False)
    NLTK_SETUP_SUCCESSFUL = True

from pydub import AudioSegment

# --- Configuration ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

TEXT_FILES_DIR = os.getenv(
    "PROJECT_INPUT_TEXT_DIR", os.path.join(BASE_DIR, "BlleatTL_Novels")
)
AUDIO_OUTPUT_DIR = os.getenv(
    "PROJECT_AUDIO_WAV_DIR", os.path.join(BASE_DIR, "generated_audio_HalfLight")
)
PROJECT_ROOT_DIR = os.path.dirname(os.path.abspath(AUDIO_OUTPUT_DIR))

TEMP_CHUNK_DIR = os.path.join(PROJECT_ROOT_DIR, "temp_audio_chunks")

FALLBACK_TOKEN_LIMIT = 170
AVG_CHARS_PER_TOKEN = 1.9
FALLBACK_CHAR_LIMIT = FALLBACK_TOKEN_LIMIT * AVG_CHARS_PER_TOKEN

OUTPUT_FORMAT = "wav"

# Global Models
qwen_model = None
rvc_model = None

# Tone instruction for "Half Light"
ACTING_PROMPT = "Speak in a very aggressive, threatening, and visceral tone. A raspy, angry whisper-shout."


def concatenate_audio_chunks(chunk_filepaths, final_output_path):
    if not chunk_filepaths:
        return False
    print(f"  Concatenating {len(chunk_filepaths)} chunks...")
    combined = AudioSegment.empty()
    for filepath in sorted(chunk_filepaths):
        combined += AudioSegment.from_wav(filepath)

    if len(combined) > 0:
        combined.export(final_output_path, format=OUTPUT_FORMAT)
        return True
    return False


# ... [KEEP YOUR EXISTING SPLIT FUNCTIONS HERE: _estimate_tokens, _split_by_force_chars, _split_by_sentence_groups, _split_by_line_groups] ...
# (I am omitting them here for brevity, but keep the exact same chunking functions from your previous script)


def process_chapter_file(text_filepath, final_audio_output_path):
    print(f"\n--- Processing: {os.path.basename(text_filepath)} ---")
    base_name = os.path.splitext(os.path.basename(text_filepath))[0]
    sanitized_base = re.sub(r"[^\w_.-]", "_", base_name)

    chapter_temp_dir = os.path.join(TEMP_CHUNK_DIR, sanitized_base)
    os.makedirs(chapter_temp_dir, exist_ok=True)

    with open(text_filepath, "r", encoding="utf-8") as f:
        full_text_content = f.read()

    initial_text_chunks = _split_by_line_groups(
        full_text_content, FALLBACK_TOKEN_LIMIT, AVG_CHARS_PER_TOKEN
    )

    generated_audio_files = []

    for i, text_to_process in enumerate(initial_text_chunks):
        output_suffix = f"l_{i+1:03d}"
        chunk_output_basename = f"{sanitized_base}_{output_suffix}"
        local_chunk_filepath = os.path.join(
            chapter_temp_dir, f"{chunk_output_basename}.{OUTPUT_FORMAT}"
        )

        if os.path.exists(local_chunk_filepath):
            generated_audio_files.append(local_chunk_filepath)
            continue

        try:
            print(f"      [1/2] Qwen Acting for chunk {output_suffix}...")
            # 1. Generate acting with Qwen
            wavs, sr = qwen_model.generate_custom_voice(
                text=text_to_process,
                language="English",
                speaker="Ryan",  # Base male voice, fits well for shifting into Half Light
                instruct=ACTING_PROMPT,
            )

            # Save Qwen audio
            sf.write(local_chunk_filepath, wavs[0], sr)

            print(f"      [2/2] Applying Half Light RVC Skin...")
            # 2. Apply RVC to the generated audio, overwriting the file
            rvc_model.infer_file(
                local_chunk_filepath,
                local_chunk_filepath,  # Save over the original
            )

            generated_audio_files.append(local_chunk_filepath)

        except Exception as e:
            print(f"      [!!] Error: {e}")

    if concatenate_audio_chunks(generated_audio_files, final_audio_output_path):
        shutil.rmtree(chapter_temp_dir)
        return True
    return False


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Qwen3-TTS + RVC Generator")
    parser.add_argument(
        "--rvc_model_path", type=str, required=True, help="Path to your .pth file"
    )
    parser.add_argument(
        "--rvc_index_path", type=str, required=True, help="Path to your .index file"
    )
    parser.add_argument(
        "--pitch", type=int, default=-2, help="Pitch shift (try -12 for deep voices)"
    )
    args = parser.parse_args()

    # --- 1. INITIALIZE QWEN ---
    print("Loading Qwen3-TTS CustomVoice Model...")
    qwen_model = Qwen3TTSModel.from_pretrained(
        "Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice",  # Using CustomVoice for acting control
        device_map="cuda:0",
        dtype=torch.bfloat16,
        attn_implementation="flash_attention_2",
    )

    # --- 2. INITIALIZE RVC ---
    print("Loading Half Light RVC Model...")
    rvc_model = RVCInference(device="cuda:0")
    rvc_model.load_model(args.rvc_model_path)
    rvc_model.set_params(
        f0method="rmvpe",  # Best pitch extraction method
        f0up_key=args.pitch,
        index_file=args.rvc_index_path,
        index_rate=0.75,  # How strongly to enforce the voice print
        filter_radius=3,
        resample_sr=0,
        rms_mix_rate=0.25,
        protect=0.33,
    )

    # --- SETUP DIRECTORIES ---
    if not os.path.exists(TEMP_CHUNK_DIR):
        os.makedirs(TEMP_CHUNK_DIR)
    if not os.path.exists(AUDIO_OUTPUT_DIR):
        os.makedirs(AUDIO_OUTPUT_DIR)

    text_files = sorted(glob.glob(os.path.join(TEXT_FILES_DIR, "*.txt")))
    print(f"Found {len(text_files)} chapters.")

    for text_file_path in text_files:
        base_name = os.path.splitext(os.path.basename(text_file_path))[0]
        out_path = os.path.join(AUDIO_OUTPUT_DIR, f"{base_name}.{OUTPUT_FORMAT}")

        if not os.path.exists(out_path):
            process_chapter_file(text_file_path, out_path)

--- END OF FILE: ./qwen_tts_generator.py ---

--- START OF FILE: ./scraper_2.py ---
import json
import os
import re
import time

import requests
from bs4 import BeautifulSoup

# --- CONFIGURATION ---
DELAY_BETWEEN_REQUESTS = 1.0  # Seconds
# ---------------------


def get_with_retries(session, url, headers, retries=3):
    for i in range(retries):
        try:
            response = session.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            print(f"   [!] Error fetching {url}: {e}. Retrying ({i+1}/{retries})...")
            time.sleep(2 * (i + 1))
    return None


def extract_and_clean_chapter_data(content_el, ch_num):
    """
    Targets the main content area, removes scripts, styles, and unwanted
    interactive elements like glossaries or ads.
    """
    if not content_el:
        return f"Chapter {ch_num}", ""

    # 1. Remove script, style, and known junk classes
    for junk in content_el.find_all(
        ["script", "style", "div", "section", "button"],
        class_=[
            "paragraph-tools",
            "chapter__actions",
            "social-share",
            "sharedaddy",
            "navigation",
        ],
    ):
        junk.decompose()

    # 2. Remove Glossary Tooltips (common in translation sites)
    for tooltip in content_el.find_all(class_="dg-tooltip-box"):
        tooltip.decompose()

    # 3. Get text content
    cleaned_body = content_el.get_text(separator="\n\n", strip=True)

    # --- LOGIC: TITLE EXTRACTION & DEDUPLICATION ---
    story_title = f"Chapter {ch_num}"

    lines = cleaned_body.split("\n")
    while lines and not lines[0].strip():
        lines.pop(0)

    if lines:
        first_line = lines[0].strip()
        # Heuristic: If first line contains "Chapter" OR is very short < 100 chars, treat as title
        if (f"Chapter {ch_num}" in first_line) or (len(first_line) < 100):
            story_title = first_line
            # CRITICAL: Remove this line from body so it doesn't duplicate
            cleaned_body = "\n".join(lines[1:]).strip()

    final_header = f"Chapter {ch_num} - {story_title}"
    return final_header, cleaned_body


def scrape_and_save_chapters(start_url, save_directory="BlleatTL_Novels"):
    save_directory = os.getenv("PROJECT_RAW_TEXT_DIR", save_directory)

    if not os.path.exists(save_directory):
        os.makedirs(save_directory)

    json_path = os.path.join(save_directory, "chapters.json")
    session = requests.Session()
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
    }

    # --- LOAD HISTORY & SET COUNTER ---
    url_history_map = {}
    history_data = []
    if os.path.exists(json_path):
        try:
            with open(json_path, "r", encoding="utf-8") as f:
                history_data = json.load(f)
                for entry in history_data:
                    url_history_map[entry["url"]] = entry.get("next_url")
        except:
            pass

    # THE FIX: Always start counting from 1 (or continue from history length)
    ch_counter = len(history_data) + 1
    current_url = start_url

    try:
        while current_url:
            if current_url in url_history_map and url_history_map[current_url]:
                next_link = url_history_map[current_url]
                print(f"Skipping (history): {current_url}")
                current_url = next_link
                continue

            print(f"Processing: {current_url}")
            response = get_with_retries(session, current_url, headers)
            if not response:
                break

            soup = BeautifulSoup(response.content, "html.parser")

            # Extract basic info
            content_el = soup.select_one(".entry-content") or soup.find("article")

            # Find Next Link
            next_el = None
            for a in soup.find_all("a", href=True):
                if "next" in a.get_text(strip=True).lower():
                    next_el = a
                    break

            if not content_el:
                print("Content not found.")
                break

            # Use internal counter for naming
            filename = f"ch_{ch_counter:04d}.txt"
            filepath = os.path.join(save_directory, filename)

            if os.path.exists(filepath):
                print(f"   -> Exists: {filename}")
            else:
                full_header, cleaned_body = extract_and_clean_chapter_data(
                    content_el, ch_counter
                )
                with open(filepath, "w", encoding="utf-8") as f:
                    f.write(f"{full_header}\n\n{cleaned_body}")
                print(f"   -> Saved: {full_header}")

            next_url = next_el["href"] if next_el else None

            # Save History
            history_entry = {"url": current_url, "next_url": next_url, "file": filename}
            history_data.append(history_entry)
            with open(json_path, "w") as f:
                json.dump(history_data, f, indent=4)

            # Increment the counter
            ch_counter += 1

            if not next_url:
                break
            current_url = next_url
            time.sleep(DELAY_BETWEEN_REQUESTS)

    except Exception as e:
        print(f"Critical Error: {e}")


if __name__ == "__main__":
    pass

--- END OF FILE: ./scraper_2.py ---

--- START OF FILE: ./tag_audiobook_files_opus_3.py ---
import base64
import glob
import json
import mimetypes
import os
import re
import sys

# --- 1. WINDOWS UNICODE FIX ---
if sys.platform == "win32":
    sys.stdout.reconfigure(encoding="utf-8")

# --- Try importing mutagen ---
try:
    import mutagen
    from mutagen.flac import Picture
    from mutagen.oggopus import OggOpus

    MUTAGEN_AVAILABLE = True
except ImportError:
    print("Error: mutagen library not found. Please install it: pip install mutagen")
    MUTAGEN_AVAILABLE = False

# --- Configuration & Paths ---
# 1. Inputs from GUI (Environment Variables)
AUDIO_DIR = os.getenv("OPUS_OUTPUT_DIR")
TEXT_DIR = os.getenv("PROJECT_INPUT_TEXT_DIR")

# Fallbacks for standalone testing
if not AUDIO_DIR:
    print("Warning: Running standalone. Using default paths.")
    AUDIO_DIR = "generated_audio_MistakenFairy_opus"
    TEXT_DIR = "BlleatTL_Novels"

# 2. Determine Project Root (One level up from text/audio dirs)
# Structure: /Novels/Title/01_Raw_Text  -> Root is /Novels/Title
if TEXT_DIR and os.path.exists(TEXT_DIR):
    PROJECT_ROOT = os.path.dirname(os.path.abspath(TEXT_DIR))
else:
    PROJECT_ROOT = os.getcwd()

METADATA_JSON = os.path.join(PROJECT_ROOT, "metadata.json")
COVER_ART_PATH = os.path.join(PROJECT_ROOT, "cover.jpg")

# 3. Default Metadata (Overwritten if metadata.json exists)
ALBUM_META = {
    "title": "Unknown Series",
    "author": "Unknown Author",
    "year": "2026",
    "genre": "Audiobook",
}

# --- Helper Functions ---


def load_global_metadata():
    """Loads the Series Title and Author from the project's metadata.json"""
    if os.path.exists(METADATA_JSON):
        try:
            with open(METADATA_JSON, "r", encoding="utf-8") as f:
                data = json.load(f)
                if data.get("title"):
                    ALBUM_META["title"] = data["title"]
                if data.get("author"):
                    ALBUM_META["author"] = data["author"]
            print(
                f"Loaded Global Metadata: {ALBUM_META['title']} by {ALBUM_META['author']}"
            )
        except Exception as e:
            print(f"Warning: Could not read metadata.json: {e}")
    else:
        print("Warning: metadata.json not found. Using defaults.")


def get_chapter_title_from_text(track_num):
    """
    Reads the specific text file for this track number.
    The first line is assumed to be the Chapter Title (header).
    """
    if not TEXT_DIR or not os.path.exists(TEXT_DIR):
        return None

    # Try formatted name first (ch_0001.txt)
    txt_path = os.path.join(TEXT_DIR, f"ch_{track_num:04d}.txt")

    # Fallback to loose matching if exact file doesn't exist
    if not os.path.exists(txt_path):
        candidates = glob.glob(os.path.join(TEXT_DIR, f"*_{track_num:04d}.txt"))
        if candidates:
            txt_path = candidates[0]

    if os.path.exists(txt_path):
        try:
            with open(txt_path, "r", encoding="utf-8") as f:
                first_line = f.readline().strip()
                if first_line:
                    return first_line
        except Exception:
            pass

    return None


def get_track_number(filename):
    # Extracts '1' from 'ch_0001.opus' or 'Mistaken_Fairy_l_001.opus'
    # Looks for the last sequence of digits
    matches = re.findall(r"(\d+)", filename)
    if matches:
        return int(
            matches[-1]
        )  # Return the last number found (usually the chapter index)
    return None


def tag_audio_file(audio_path, track_num, chapter_title):
    try:
        audio = OggOpus(audio_path)

        # 1. Standard Tags
        audio.tags["TITLE"] = [chapter_title]
        audio.tags["ALBUM"] = [ALBUM_META["title"]]
        audio.tags["ARTIST"] = [ALBUM_META["author"]]
        audio.tags["ALBUMARTIST"] = ["AI Narrator"]  # Or use Author again
        audio.tags["GENRE"] = [ALBUM_META["genre"]]
        audio.tags["DATE"] = [ALBUM_META["year"]]
        audio.tags["TRACKNUMBER"] = [str(track_num)]

        # 2. Embed Cover Art (Opus uses METADATA_BLOCK_PICTURE)
        if os.path.exists(COVER_ART_PATH):
            mime = mimetypes.guess_type(COVER_ART_PATH)[0] or "image/jpeg"

            pic = Picture()
            with open(COVER_ART_PATH, "rb") as f:
                pic.data = f.read()

            pic.type = 3  # Cover (front)
            pic.mime = mime
            pic.desc = "Cover"

            # OggOpus requires base64 encoded picture block
            pic_data = pic.write()
            encoded_data = base64.b64encode(pic_data).decode("ascii")
            audio.tags["METADATA_BLOCK_PICTURE"] = [encoded_data]

        audio.save()
        return True
    except Exception as e:
        print(f"   Error tagging {os.path.basename(audio_path)}: {e}")
        return False


# --- Main Execution ---
if __name__ == "__main__":
    if not MUTAGEN_AVAILABLE:
        print("Mutagen is required. Install via: pip install mutagen")
        sys.exit(1)

    print(f"--- Audio Tagging ---")
    print(f"Audio Source: {AUDIO_DIR}")
    print(f"Text Source:  {TEXT_DIR}")

    if not AUDIO_DIR or not os.path.exists(AUDIO_DIR):
        print("Error: Audio directory not found.")
        sys.exit(1)

    load_global_metadata()

    # Process Opus files
    audio_files = sorted(glob.glob(os.path.join(AUDIO_DIR, "*.opus")))

    if not audio_files:
        print(f"No .opus files found in {AUDIO_DIR}")
        sys.exit(0)

    print(f"Found {len(audio_files)} files to tag.")

    success_count = 0
    for path in audio_files:
        filename = os.path.basename(path)
        track_num = get_track_number(filename)

        if track_num is None:
            print(f"   Skipping {filename} (Could not determine track number)")
            continue

        # 1. Get Specific Chapter Title
        title = get_chapter_title_from_text(track_num)
        if not title:
            title = f"Chapter {track_num}"  # Fallback

        # 2. Apply Tags
        if tag_audio_file(path, track_num, title):
            print(f"   Tagged: [{track_num}] {title}")
            success_count += 1

    print(f"\nDone. Successfully tagged {success_count} files.")

--- END OF FILE: ./tag_audiobook_files_opus_3.py ---

--- START OF FILE: ./grok_transelate.py ---
import json
import os
import re
import time

# Ensure you have the openai package installed
# pip install openai
try:
    from openai import (
        APIError,
        APITimeoutError,
        AuthenticationError,
        OpenAI,
        RateLimitError,
    )
except ImportError:
    print(
        "CRITICAL ERROR: The 'openai' package is not installed. Please install it by running: pip install openai"
    )
    exit()

# --- Configuration ---
# Updated to support the GUI Pipeline (os.getenv) with fallbacks to your original folders
INPUT_DIR = os.getenv("PROJECT_TRANS_INPUT_DIR", "SnakeFairy_CH_Qushucheng")
OUTPUT_DIR = os.getenv("PROJECT_TRANS_OUTPUT_DIR", "SnakeFairy_EN_transelated")
XAI_MODEL_NAME = "grok-4-0709"
XAI_BASE_URL = "https://api.x.ai/v1"
API_TIMEOUT_SECONDS = 300.0
GLOSSARY_JSON_FILE = "translation_glossary.json"


# --- Glossary Helper Functions ---
def load_glossary_from_json(filepath: str) -> dict:
    """
    Loads a glossary dictionary (for characters and places) from a JSON file.
    If the file doesn't exist or is invalid, it returns a new dictionary structure.
    """
    default_glossary = {"characters": {}, "places": {}}
    if not os.path.exists(filepath):
        print(
            f"Glossary JSON file not found at '{filepath}'. A new one will be created."
        )
        return default_glossary
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            data = json.load(f)
            # Ensure both top-level keys exist
            if "characters" not in data:
                data["characters"] = {}
            if "places" not in data:
                data["places"] = {}
            return data
    except (json.JSONDecodeError, IOError) as e:
        print(f"Error reading or parsing JSON file '{filepath}': {e}. Starting fresh.")
        return default_glossary


def save_glossary_to_json(filepath: str, data: dict):
    """Saves a glossary dictionary to a JSON file with pretty printing."""
    try:
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=4, ensure_ascii=False)
        print(f"Successfully saved updated glossary to '{filepath}'.")
    except IOError as e:
        print(f"Error writing to JSON file '{filepath}': {e}")


# --- Helper Function to Reformat Title ---
def reformat_chapter_title_in_text(text_content: str) -> str:
    if not text_content or not text_content.strip():
        return text_content

    lines = text_content.split("\n", 1)
    first_line = lines[0]
    rest_of_content = lines[1] if len(lines) > 1 else ""

    match = re.match(r"^(Chapter\s*\d+)\s*[:\-]?\s*(.*)", first_line, re.IGNORECASE)
    if match:
        chapter_part = match.group(1).strip()
        title_part = match.group(2).strip()
        reformatted_first_line = (
            f"{chapter_part} - {title_part}" if title_part else chapter_part
        )
        return f"{reformatted_first_line}\n{rest_of_content}"

    numeric_match = re.match(r"^(\d+)\s+(.*)", first_line)
    if numeric_match:
        try:
            chapter_number_int = int(numeric_match.group(1))
            title_part = numeric_match.group(2).strip()
            reformatted_first_line = f"Chapter {chapter_number_int} - {title_part}"
            return f"{reformatted_first_line}\n{rest_of_content}"
        except ValueError:
            pass
    return text_content


# --- MODIFIED: xAI API Translation Function with Context Optimization ---
def translate_text_with_xai(
    text_to_translate: str, known_glossary_data: dict, target_language: str = "English"
) -> (str, dict):
    api_key = os.environ.get("XAI_API_KEY")
    if not api_key:
        return "[Translation Error: 'XAI_API_KEY' environment variable not set.]", {}

    try:
        client = OpenAI(
            api_key=api_key, base_url=XAI_BASE_URL, timeout=API_TIMEOUT_SECONDS
        )
    except Exception as e:
        return (
            f"[Translation Error: Could not initialize OpenAI client for xAI - {type(e).__name__}: {e}]",
            {},
        )

    # --- OPTIMIZATION START: Dynamic Glossary Filtering ---
    # This logic matches the Gemini script: only send relevant glossary items to save context.
    filtered_glossary = {"characters": {}, "places": {}}

    # 1. Filter Characters
    for name_key, details in known_glossary_data.get("characters", {}).items():
        if name_key in text_to_translate:
            filtered_glossary["characters"][name_key] = details

    # 2. Filter Places
    for place_key, details in known_glossary_data.get("places", {}).items():
        if place_key in text_to_translate:
            filtered_glossary["places"][place_key] = details

    # 3. Minify JSON (remove whitespace)
    known_glossary_json_str = json.dumps(
        filtered_glossary, ensure_ascii=False, separators=(",", ":")
    )

    total_chars = len(known_glossary_data.get("characters", {}))
    relevant_chars = len(filtered_glossary["characters"])
    print(
        f"  Glossary Optimization: Sending {relevant_chars}/{total_chars} characters relevant to this chapter."
    )
    # --- OPTIMIZATION END ---

    print(
        f"Attempting to translate and extract glossary items from text (length: {len(text_to_translate)} chars)..."
    )

    # --- UPDATED PROMPT: Aligned with Gemini prompt for consistency ---
    prompt = (
        f"You are an expert Chinese-to-English translator and data extractor.\n"
        f"Your task is twofold:\n"
        f"1. Translate the Chinese text into high-quality, natural-sounding {target_language}. For names and places, you MUST use the 'english_name' from the 'Relevant Glossary' below if present.\n"
        f"2. Identify new character names and place names in the text NOT already in the 'Relevant Glossary', and extract their details.\n\n"
        f"--- RELEVANT GLOSSARY (Specific to this text) ---\n"
        f"{known_glossary_json_str}\n\n"
        f"--- RESPONSE FORMATTING RULES ---\n"
        f"- Your response MUST have two parts separated by '---JSON---'.\n"
        f"- PART 1 (Translation): MUST ONLY contain the final {target_language.upper()} translation.\n"
        f"- PART 2 (Data): MUST start on a new line immediately after '---JSON---' and contain a single JSON object of NEW entities. This object should have two keys: 'characters' and 'places'.\n"
        f"- Under 'characters', provide new characters with their 'pinyin', 'english_name', and 'pronoun'.\n"
        f"- Under 'places', provide new places with their 'pinyin' and 'english_name'.\n"
        f'- Example JSON Format: {{"characters": {{"": {{"pinyin": "Lan Bo", "english_name": "Lan Bo", "pronoun": "he/him"}}}}, "places": {{"": {{"pinyin": "Qinghe Shi", "english_name": "Qinghe City"}}}}}}\n'
        f'- If NO NEW entities of a type are found, that key\'s value must be an empty object: e.g., {{"characters": {{}}, "places": {{...}}}}\n\n'
        f"--- CHINESE TEXT TO PROCESS ---\n"
        f"{text_to_translate}\n"
        f"--- END OF TEXT ---\n\n"
        f"Please provide your response following all rules."
    )

    try:
        chat_completion = client.chat.completions.create(
            messages=[
                {
                    "role": "system",
                    "content": "You are a helpful assistant that follows instructions precisely.",
                },
                {"role": "user", "content": prompt},
            ],
            model=XAI_MODEL_NAME,
            temperature=0.2,
        )

        raw_response_text = (
            chat_completion.choices[0].message.content
            if chat_completion.choices and chat_completion.choices[0].message
            else ""
        )
        separator = "---JSON---"
        new_glossary_items = {}
        translation_part = raw_response_text

        if separator in raw_response_text:
            parts = raw_response_text.split(separator, 1)
            translation_part = parts[0].strip()
            json_part = parts[1].strip()

            try:
                json_part_cleaned = re.sub(
                    r"```json\s*|\s*```", "", json_part, flags=re.DOTALL
                ).strip()
                if json_part_cleaned:
                    new_glossary_items = json.loads(json_part_cleaned)
                    print(f"  Successfully parsed glossary data from API response.")
            except json.JSONDecodeError as e:
                print(f"  Warning: Failed to parse JSON from API response. Error: {e}")
        else:
            print("  Warning: JSON separator not found in API response.")

        final_translation = re.sub(
            r"\n---\s*"
            + target_language.upper()
            + r"\s*TRANSLATION\s*(END|START)\s*---|\^ENGLISH TRANSLATION ONLY:[\s\n]*",
            "",
            translation_part,
            flags=re.IGNORECASE,
        ).strip()
        print(f"Translation API call successful.")
        return final_translation, new_glossary_items

    except (APIError, APITimeoutError, AuthenticationError, RateLimitError) as e:
        error_type = type(e).__name__
        print(f"An API error occurred during translation: {error_type} - {e}")
        return f"[Translation Error ({XAI_MODEL_NAME} - {error_type})]", {}
    except Exception as e:
        error_type = type(e).__name__
        print(f"An unexpected error occurred during translation: {error_type} - {e}")
        return f"[Translation Error ({XAI_MODEL_NAME} - {error_type})]", {}


# --- MODIFIED: Main Processing Logic with Glossary ---
def process_files_for_translation():
    script_dir = os.path.dirname(os.path.abspath(__file__))
    # Using the global variables configured via os.getenv above
    input_dir_full_path = os.path.join(script_dir, INPUT_DIR)
    output_dir_full_path = os.path.join(script_dir, OUTPUT_DIR)
    glossary_json_full_path = os.path.join(script_dir, GLOSSARY_JSON_FILE)
    glossary_data = load_glossary_from_json(glossary_json_full_path)

    if not os.path.exists(input_dir_full_path):
        print(f"Error: Input directory '{input_dir_full_path}' not found.")
        return
    if not os.path.exists(output_dir_full_path):
        os.makedirs(output_dir_full_path)
        print(f"Created output directory: {output_dir_full_path}")

    files_to_process = sorted(
        [f for f in os.listdir(input_dir_full_path) if f.endswith(".txt")]
    )
    if not files_to_process:
        print(f"No .txt files found in '{input_dir_full_path}'.")
        return

    print(
        f"Found {len(files_to_process)} file(s) to process from '{input_dir_full_path}'."
    )

    for i, filename in enumerate(files_to_process):
        input_filepath = os.path.join(input_dir_full_path, filename)
        output_filepath = os.path.join(output_dir_full_path, filename)
        print(f"\n[{i+1}/{len(files_to_process)}] Checking: {filename}...")

        if os.path.exists(output_filepath):
            try:
                with open(output_filepath, "r", encoding="utf-8") as f_check:
                    content_check = f_check.read(200)
                    if (
                        "[Translation Error" not in content_check
                        and "[ERROR PROCESSING FILE" not in content_check
                    ):
                        print(
                            f"Output file '{output_filepath}' exists and is valid. Skipping."
                        )
                        continue
                    else:
                        print(
                            f"Output file contains an error marker. Will re-translate."
                        )
            except Exception:
                pass

        print(f"Processing for translation: {filename}")

        try:
            with open(input_filepath, "r", encoding="utf-8") as f:
                source_content = f.read()
            clean_source_for_api = "\n".join(
                [
                    line
                    for line in source_content.splitlines()
                    if line.strip() and re.search(r"[\u4e00-\u9fff]", line)
                ]
            ).strip()

            if not clean_source_for_api:
                translated_content = "[No Chinese content found in source]"
            else:
                translated_content, new_glossary_items = translate_text_with_xai(
                    clean_source_for_api, glossary_data, target_language="English"
                )

                if new_glossary_items:
                    new_chars = new_glossary_items.get("characters", {})
                    if new_chars:
                        print(
                            f"  Updating master list with {len(new_chars)} new character(s)."
                        )
                        for name, details in new_chars.items():
                            if name not in glossary_data["characters"]:
                                glossary_data["characters"][name] = details
                                print(f"    + Added Character: {name} -> {details}")

                    new_places = new_glossary_items.get("places", {})
                    if new_places:
                        print(
                            f"  Updating master list with {len(new_places)} new place(s)."
                        )
                        for name, details in new_places.items():
                            if name not in glossary_data["places"]:
                                glossary_data["places"][name] = details
                                print(f"    + Added Place: {name} -> {details}")

            final_content_to_write = (
                translated_content
                if translated_content.startswith("[")
                else reformat_chapter_title_in_text(translated_content)
            )
            with open(output_filepath, "w", encoding="utf-8") as f:
                f.write(final_content_to_write)
            print(f"Saved: {output_filepath}")

            # --- Save glossary after each file is processed ---
            save_glossary_to_json(glossary_json_full_path, glossary_data)

            if i < len(files_to_process) - 1:
                print(f"Pausing for 5.0 seconds...")
                time.sleep(5.0)

        except Exception as e:
            print(f"FATAL Error processing file {filename}: {e}")
            with open(output_filepath, "w", encoding="utf-8") as f_err:
                f_err.write(f"[ERROR PROCESSING FILE: {e}]")

    print(f"\n--- Translation Run Summary ---")
    print(f"Total source files checked: {len(files_to_process)}")
    print(f"-----------------------------")


if __name__ == "__main__":
    print(f"Starting Chinese to English translation process...")
    if not os.environ.get("XAI_API_KEY"):
        print("\nCRITICAL ERROR: 'XAI_API_KEY' environment variable not set.")
        exit()

    process_files_for_translation()
    print("Translation process finished.")

--- END OF FILE: ./grok_transelate.py ---

--- START OF FILE: ./gemini_transelate_4.py ---
import json
import os
import re
import sys  # Added for sys.exit()
import time

from google.generativeai.types import HarmBlockThreshold, HarmCategory

# Ensure you have the google-generativeai package installed
# pip install -U google-generativeai

# --- Configuration ---
INPUT_DIR = "SnakeFairy_CH_Qushucheng"
OUTPUT_DIR = "SnakeFairy_EN_transelated"
GLOSSARY_JSON_FILE = "translation_glossary.json"


# --- Helper Function to Load Glossary from JSON ---
def load_glossary_from_json(filepath: str) -> dict:
    """
    Loads a glossary dictionary (for characters and places) from a JSON file.
    If the file doesn't exist or is invalid, it returns a new dictionary structure.
    """
    default_glossary = {"characters": {}, "places": {}}
    if not os.path.exists(filepath):
        print(
            f"Glossary JSON file not found at '{filepath}'. A new one will be created."
        )
        return default_glossary
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            data = json.load(f)
            # Ensure both top-level keys exist
            if "characters" not in data:
                data["characters"] = {}
            if "places" not in data:
                data["places"] = {}
            return data
    except (json.JSONDecodeError, IOError) as e:
        print(f"Error reading or parsing JSON file '{filepath}': {e}. Starting fresh.")
        return default_glossary


# --- Helper Function to Save Glossary to JSON ---
def save_glossary_to_json(filepath: str, data: dict):
    """Saves a glossary dictionary to a JSON file with pretty printing."""
    try:
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=4, ensure_ascii=False)
        print(f"Successfully saved updated glossary to '{filepath}'.")
    except IOError as e:
        print(f"Error writing to JSON file '{filepath}': {e}")


# --- Helper Function to Reformat Title (unchanged) ---
def reformat_chapter_title_in_text(text_content: str) -> str:
    if not text_content or not text_content.strip():
        return text_content
    lines = text_content.split("\n", 1)
    first_line, rest_of_content = lines[0], lines[1] if len(lines) > 1 else ""
    match = re.match(r"^(Chapter\s*\d+)\s*[:\-]?\s*(.*)", first_line, re.IGNORECASE)
    if match:
        chapter_part, title_part = match.group(1).strip(), match.group(2).strip()
        reformatted_first_line = (
            f"{chapter_part} - {title_part}" if title_part else chapter_part
        )
        return f"{reformatted_first_line}\n{rest_of_content}"
    numeric_match = re.match(r"^(\d+)\s+(.*)", first_line)
    if numeric_match:
        try:
            chapter_number_int, title_part = (
                int(numeric_match.group(1)),
                numeric_match.group(2).strip(),
            )
            reformatted_first_line = f"Chapter {chapter_number_int} - {title_part}"
            return f"{reformatted_first_line}\n{rest_of_content}"
        except ValueError:
            pass
    return text_content


# --- MODIFIED: Gemini API Translation Function for Glossary ---
def translate_text_with_gemini(
    text_to_translate: str, known_glossary_data: dict, target_language: str = "English"
) -> (str, dict):
    """
    Translates text and extracts new glossary items (characters and places).
    INCLUDES RETRY LOGIC for DeadlineExceeded and AUTO-QUIT for ResourceExhausted.
    """
    try:
        import google.generativeai as genai_sdk
        from google.api_core import (
            exceptions as google_exceptions,
        )  # Import specific exceptions
    except ImportError:
        return "[Translation Error: Package not installed.]", {}

    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        return "[Translation Error: 'GEMINI_API_KEY' not set.]", {}

    genai_sdk.configure(api_key=api_key)
    model_name_for_api = "gemini-3-flash-preview"

    # --- OPTIMIZATION START: Dynamic Glossary Filtering ---
    filtered_glossary = {"characters": {}, "places": {}}

    # 1. Filter Characters
    for name_key, details in known_glossary_data.get("characters", {}).items():
        if name_key in text_to_translate:
            filtered_glossary["characters"][name_key] = details

    # 2. Filter Places
    for place_key, details in known_glossary_data.get("places", {}).items():
        if place_key in text_to_translate:
            filtered_glossary["places"][place_key] = details

    # 3. Minify JSON
    known_glossary_json_str = json.dumps(
        filtered_glossary, ensure_ascii=False, separators=(",", ":")
    )

    total_chars = len(known_glossary_data.get("characters", {}))
    relevant_chars = len(filtered_glossary["characters"])
    print(
        f"  Glossary Optimization: Sending {relevant_chars}/{total_chars} characters relevant to this chapter."
    )
    # --- OPTIMIZATION END ---

    print(
        f"Attempting to translate and extract glossary items from text (length: {len(text_to_translate)} chars)..."
    )

    prompt = (
        f"You are an expert Chinese-to-English translator and data extractor.\n"
        f"Your task is twofold:\n"
        f"1. Translate the Chinese text into high-quality, natural-sounding {target_language}. For names and places, you MUST use the 'english_name' from the 'Relevant Glossary' below if present.\n"
        f"2. Identify new character names and place names in the text NOT already in the glossary, and extract their details.\n\n"
        f"--- RELEVANT GLOSSARY (Specific to this text) ---\n"
        f"{known_glossary_json_str}\n\n"
        f"--- RESPONSE FORMATTING RULES ---\n"
        f"- Your response MUST have two parts separated by '---JSON---'.\n"
        f"- PART 1 (Translation): MUST ONLY contain the final {target_language.upper()} translation.\n"
        f"- PART 2 (Data): MUST start on a new line immediately after '---JSON---' and contain a single JSON object of NEW entities. This object should have two keys: 'characters' and 'places'.\n"
        f"- Under 'characters', provide new characters with their 'pinyin', 'english_name', and 'pronoun'.\n"
        f"- Under 'places', provide new places with their 'pinyin' and 'english_name'.\n"
        f'- Example JSON Format: {{"characters": {{"": {{"pinyin": "Lan Bo", "english_name": "Lan Bo", "pronoun": "he/him"}}}}, "places": {{"": {{"pinyin": "Qinghe Shi", "english_name": "Qinghe City"}}}}}}\n'
        f'- If NO NEW entities of a type are found, that key\'s value must be an empty object: e.g., {{"characters": {{}}, "places": {{...}}}}\n\n'
        f"--- CHINESE TEXT TO PROCESS ---\n"
        f"{text_to_translate}\n"
        f"--- END OF TEXT ---\n\n"
        f"Please provide your response following all rules."
    )

    safety_settings = {
        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,
        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,
        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH,
        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,
    }

    # --- RETRY LOGIC START ---
    max_retries = 3
    retry_delay = 10  # seconds

    for attempt in range(max_retries):
        try:
            model = genai_sdk.GenerativeModel(model_name_for_api)
            # Increased timeout to 600
            response = model.generate_content(
                prompt,
                request_options={"timeout": 600},
                generation_config=genai_sdk.types.GenerationConfig(temperature=0.2),
                safety_settings=safety_settings,
            )

            # If successful, break out of retry loop and process response
            raw_response_text = response.text
            break

        except google_exceptions.ResourceExhausted:
            print(f"\nCRITICAL: Resource Exhausted (Quota limit reached).")
            print("Auto-quitting script to prevent further errors.")
            sys.exit(0)  # Quit the program entirely

        except google_exceptions.DeadlineExceeded:
            print(
                f"  Warning: API Deadline Exceeded (Timeout). Retrying {attempt + 1}/{max_retries} in {retry_delay}s..."
            )
            time.sleep(retry_delay)
            retry_delay *= 2  # Increase delay for next retry
            if attempt == max_retries - 1:
                return (
                    f"[Translation Error: Deadline Exceeded after {max_retries} attempts]",
                    {},
                )

        except Exception as e:
            # Handle other unexpected API errors
            error_type = type(e).__name__
            print(f"  API Error ({error_type}): {e}")
            return f"[Translation Error ({model_name_for_api} - {error_type})]", {}
    # --- RETRY LOGIC END ---

    try:
        separator = "---JSON---"
        new_glossary_items = {}
        translation_part = raw_response_text

        if separator in raw_response_text:
            parts = raw_response_text.split(separator, 1)
            translation_part = parts[0].strip()
            json_part = parts[1].strip()

            try:
                json_part_cleaned = re.sub(
                    r"```json\s*|\s*```", "", json_part, flags=re.DOTALL
                ).strip()
                if json_part_cleaned:
                    new_glossary_items = json.loads(json_part_cleaned)
                    print(f"  Successfully parsed glossary data from API response.")
            except json.JSONDecodeError as e:
                print(f"  Warning: Failed to parse JSON from API response. Error: {e}")
        else:
            print("  Warning: JSON separator not found in API response.")

        final_translation = re.sub(
            r"\n---\s*"
            + target_language.upper()
            + r"\s*TRANSLATION\s*(END|START)\s*---|\^ENGLISH TRANSLATION ONLY:[\s\n]*",
            "",
            translation_part,
            flags=re.IGNORECASE,
        ).strip()
        print(f"Translation API call successful.")
        return final_translation, new_glossary_items

    except Exception as e:
        print(f"An error occurred during response parsing: {e}")
        return f"[Translation Error (Parsing)]", {}


# --- MODIFIED: Main Processing Logic for Glossary ---
def process_files_for_translation():
    script_dir = os.path.dirname(os.path.abspath(__file__))
    input_dir_full_path = os.path.join(script_dir, INPUT_DIR)
    output_dir_full_path = os.path.join(script_dir, OUTPUT_DIR)
    glossary_json_full_path = os.path.join(script_dir, GLOSSARY_JSON_FILE)
    glossary_data = load_glossary_from_json(glossary_json_full_path)

    if not os.path.exists(input_dir_full_path):
        print(f"Error: Input directory '{input_dir_full_path}' not found.")
        return
    if not os.path.exists(output_dir_full_path):
        os.makedirs(output_dir_full_path)
        print(f"Created output directory: {output_dir_full_path}")

    files_to_process = sorted(
        [f for f in os.listdir(input_dir_full_path) if f.endswith(".txt")]
    )
    if not files_to_process:
        print(f"No .txt files found in '{input_dir_full_path}'.")
        return

    print(
        f"Found {len(files_to_process)} file(s) to process from '{input_dir_full_path}'."
    )

    for i, filename in enumerate(files_to_process):
        input_filepath = os.path.join(input_dir_full_path, filename)
        output_filepath = os.path.join(output_dir_full_path, filename)
        print(f"\n[{i+1}/{len(files_to_process)}] Checking: {filename}...")

        if os.path.exists(output_filepath):
            try:
                with open(output_filepath, "r", encoding="utf-8") as f_check:
                    content_check = f_check.read(200)
                    if (
                        "[Translation Error" not in content_check
                        and "[ERROR PROCESSING FILE" not in content_check
                    ):
                        print(
                            f"Output file '{output_filepath}' exists and is valid. Skipping."
                        )
                        continue
                    else:
                        print(
                            f"Output file contains an error marker. Will re-translate."
                        )
            except Exception:
                pass

        print(f"Processing for translation: {filename}")

        try:
            with open(input_filepath, "r", encoding="utf-8") as f:
                source_content = f.read()
            clean_source_for_api = "\n".join(
                [
                    line
                    for line in source_content.splitlines()
                    if line.strip() and re.search(r"[\u4e00-\u9fff]", line)
                ]
            ).strip()

            if not clean_source_for_api:
                translated_content = "[No Chinese content found in source]"
            else:
                translated_content, new_glossary_items = translate_text_with_gemini(
                    clean_source_for_api, glossary_data, target_language="English"
                )

                if new_glossary_items:
                    new_chars = new_glossary_items.get("characters", {})
                    if new_chars:
                        print(
                            f"  Updating master list with {len(new_chars)} new character(s)."
                        )
                        for name, details in new_chars.items():
                            if name not in glossary_data["characters"]:
                                glossary_data["characters"][name] = details
                                print(f"    + Added Character: {name} -> {details}")

                    new_places = new_glossary_items.get("places", {})
                    if new_places:
                        print(
                            f"  Updating master list with {len(new_places)} new place(s)."
                        )
                        for name, details in new_places.items():
                            if name not in glossary_data["places"]:
                                glossary_data["places"][name] = details
                                print(f"    + Added Place: {name} -> {details}")

            final_content_to_write = (
                translated_content
                if translated_content.startswith("[")
                else reformat_chapter_title_in_text(translated_content)
            )
            with open(output_filepath, "w", encoding="utf-8") as f:
                f.write(final_content_to_write)
            print(f"Saved: {output_filepath}")

            # --- MODIFIED: Save glossary after each file is processed ---
            save_glossary_to_json(glossary_json_full_path, glossary_data)

            if i < len(files_to_process) - 1:
                print(f"Pausing for 5.0 seconds...")
                time.sleep(5.0)

        except Exception as e:
            print(f"FATAL Error processing file {filename}: {e}")
            with open(output_filepath, "w", encoding="utf-8") as f_err:
                f_err.write(f"[ERROR PROCESSING FILE: {e}]")

    print(f"\n--- Translation Run Summary ---")
    print(f"Total source files checked: {len(files_to_process)}")
    print(f"-----------------------------")


if __name__ == "__main__":
    print(f"Starting Chinese to English translation process...")
    if not os.environ.get("GEMINI_API_KEY"):
        print("\nCRITICAL ERROR: 'GEMINI_API_KEY' environment variable not set.")
        exit()

    process_files_for_translation()
    print("Translation process finished.")

--- END OF FILE: ./gemini_transelate_4.py ---

--- START OF FILE: ./metadata_fetcher.py ---
import json
import os
import re
import shutil
import subprocess
import sys
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup

# Try to import constants, fallback if missing
try:
    import google.generativeai as genai

    from constants import GEMINI_MODEL_NAME
except ImportError:
    GEMINI_MODEL_NAME = "gemini-3-flash-preview"


# --- HELPER: Image Downloader ---
def download_cover(img_url, save_dir):
    if not img_url:
        return
    try:
        clean_url = img_url.split("?")[0]  # Remove WP resize params
        save_path = os.path.join(save_dir, "cover.jpg")

        headers = {"User-Agent": "Mozilla/5.0"}
        r = requests.get(img_url, headers=headers, stream=True, timeout=10)
        if r.status_code == 200:
            with open(save_path, "wb") as f:
                r.raw.decode_content = True
                shutil.copyfileobj(r.raw, f)
            print(f"    [Cover] Saved to: {save_path}")
    except Exception as e:
        print(f"    [Error] Cover download failed: {e}")


# --- HELPER: Code Sanitizer (The Fix) ---
def sanitize_generated_code(code):
    """
    Post-processes AI code to remove blocking input calls that freeze the GUI.
    """
    lines = code.split("\n")
    cleaned_lines = []

    for line in lines:
        # Check for blocking input calls
        if "sys.stdin" in line or "input(" in line:
            print(f"    [Auto-Fix] Removed blocking line: {line.strip()}")
            # We replace it with a pass or comment so indentation doesn't break
            cleaned_lines.append(f"    # [Auto-Removed Blocking Input]: {line.strip()}")
            cleaned_lines.append("    pass")
        else:
            cleaned_lines.append(line)

    return "\n".join(cleaned_lines)


# --- 1. DEFAULT EXTRACTION LOGIC ---
def default_metadata_extraction(html, url):
    """
    Standard scraper trying OpenGraph and common HTML tags.
    """
    soup = BeautifulSoup(html, "html.parser")
    data = {
        "title": "Unknown Title",
        "author": "Unknown Author",
        "description": "",
        "cover_url": "",
    }

    # Title
    og_title = soup.find("meta", property="og:title")
    if og_title:
        data["title"] = (
            og_title.get("content", "").replace(" Dobytranslations", "").strip()
        )
    else:
        h1 = soup.select_one("h1.entry-title")
        if h1:
            data["title"] = h1.get_text(strip=True)

    # Cover
    # Doby specific
    thumb = soup.select_one(".sertothumb img")
    if thumb:
        data["cover_url"] = thumb.get("src", "") or thumb.get("data-src", "")
    else:
        og_image = soup.find("meta", property="og:image")
        if og_image:
            data["cover_url"] = og_image.get("content", "")

    # Description
    # Doby specific
    desc_div = soup.select_one(".sersys.entry-content")
    if not desc_div:
        desc_div = soup.select_one(".entry-content[itemprop='description']")

    if desc_div:
        # Cleanup Doby junk (New Free unlock...)
        for junk in desc_div.find_all(["h4", "strong"]):
            if "unlock" in junk.get_text().lower():
                junk.decompose()
        data["description"] = desc_div.get_text(separator="\n", strip=True)
    else:
        og_desc = soup.find("meta", property="og:description")
        if og_desc:
            data["description"] = og_desc.get("content", "")

    # Author
    author_meta = soup.find("meta", attrs={"name": "author"})
    if author_meta:
        data["author"] = author_meta.get("content", "")
    else:
        for label in soup.find_all(string=re.compile(r"Author", re.I)):
            parent = label.parent
            if parent:
                text = (
                    parent.get_text(strip=True)
                    .replace("Author", "")
                    .replace(":", "")
                    .strip()
                )
                if 1 < len(text) < 50:
                    data["author"] = text
                    break

    return data


# --- 2. AI GENERATOR LOGIC ---
def fetch_and_generate_metadata_scraper(index_url, project_dir):
    """
    Fetches HTML -> Sends to Gemini -> Writes custom_metadata_scraper.py
    """
    context_dir = os.path.join(project_dir, "Scraper_Context")
    if not os.path.exists(context_dir):
        os.makedirs(context_dir)

    print(f"    [AI] Fetching HTML source to analyze...")
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(index_url, headers=headers, timeout=15)
        html_content = response.text
        # Save for reference
        with open(
            os.path.join(context_dir, "index_structure.html"), "w", encoding="utf-8"
        ) as f:
            f.write(html_content)
    except Exception as e:
        print(f"    [AI] Error fetching URL: {e}")
        return False

    print(f"    [AI] Asking Gemini ({GEMINI_MODEL_NAME}) to write a custom scraper...")

    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        print("    [Error] GEMINI_API_KEY not set.")
        return False

    genai.configure(api_key=api_key)

    # --- UPDATED PROMPT: Explicitly forbid stdin ---
    prompt = f"""
    You are an expert Python web scraping developer.
    
    The default scraping method FAILED for this website.
    I need a robust script to extract Novel Metadata from the HTML below.
    
    --- TARGET HTML (Index Page) ---
    {html_content[:55000]} 
    
    --- INSTRUCTIONS ---
    1. Write a Python script using `BeautifulSoup`.
    2. Extract: **Title**, **Author**, **Description**, **Cover Image URL**.
    3. **CRITICAL OUTPUT**:
       - Save the data to `metadata.json` in `os.getenv('SAVE_DIR')`.
       - Download the cover image to `cover.jpg` in `os.getenv('SAVE_DIR')`.
    4. Handle `data-src` or `loading="lazy"` if present for images.
    5. **NETWORK REQUESTS**: The script MUST fetch the URL itself using `requests`. 
       - Use `os.getenv('TARGET_URL')` to get the URL.
       - **DO NOT** use `sys.stdin` or `input()`. This script runs in a background pipe and will hang if it waits for input.
    
    Output ONLY the valid Python code.
    """

    try:
        model = genai.GenerativeModel(GEMINI_MODEL_NAME)
        response = model.generate_content(prompt)

        # Extract code block
        code = response.text
        if "```python" in code:
            code = code.split("```python")[1].split("```")[0]
        elif "```" in code:
            code = code.split("```")[1]

        # --- POST-PROCESSING: SANITIZE CODE ---
        code = sanitize_generated_code(code)
        # --------------------------------------

        output_path = os.path.join(project_dir, "custom_metadata_scraper.py")
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(code)

        print(f"    [AI] Success! Generated: {output_path}")
        return True

    except Exception as e:
        print(f"    [AI] Gemini API Error: {e}")
        return False


# --- 3. MAIN CONTROLLER ---
def run_metadata_fetch(index_url, project_dir):
    print(f"--- Fetching Metadata for: {os.path.basename(project_dir)} ---")

    # A. Check for EXISTING custom script first
    custom_script = os.path.join(project_dir, "custom_metadata_scraper.py")
    if os.path.exists(custom_script):
        print(f"--- Found Custom Script. Executing... ---")
        run_custom_script(custom_script, index_url, project_dir)
        return

    # B. Try Default Method
    try:
        print("    [1] Trying Default Extraction...")
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(index_url, headers=headers, timeout=15)
        response.raise_for_status()

        data = default_metadata_extraction(response.text, index_url)

        # Validate critical data
        if not data["title"] or data["title"] == "Unknown Title":
            raise Exception("Default extractor failed to find a valid title.")

        # Save success
        json_path = os.path.join(project_dir, "metadata.json")
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=4)
        print(f"    [Meta] Success! Title: {data['title']}")

        if data["cover_url"]:
            download_cover(data["cover_url"], project_dir)

    except Exception as e:
        # C. FAILOVER: Trigger AI
        print(f"    [!] Default Method Failed: {e}")
        print(f"    [2] FAILOVER: Initializing AI Auto-Correction...")

        success = fetch_and_generate_metadata_scraper(index_url, project_dir)

        if success and os.path.exists(custom_script):
            print(f"    [3] Executing newly generated AI script...")
            run_custom_script(custom_script, index_url, project_dir)
        else:
            print("    [Error] AI Adaptation failed.")


def run_custom_script(script_path, url, save_dir):
    """Executes the custom script in a subprocess"""
    try:
        env = os.environ.copy()
        env["TARGET_URL"] = url
        env["SAVE_DIR"] = save_dir

        # Run with Unbuffered output ("-u") and capture stdout in real-time
        process = subprocess.Popen(
            [sys.executable, "-u", script_path],
            env=env,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1,
            universal_newlines=True,
        )

        # Stream logs to the GUI
        for line in process.stdout:
            print(line, end="")

        process.wait()

        if process.returncode != 0:
            print(f"    [Script Error] Return Code: {process.returncode}")

    except Exception as e:
        print(f"    [Exec Error] {e}")


if __name__ == "__main__":
    if len(sys.argv) > 3:
        u = sys.argv[1]
        d = sys.argv[2]
        mode = sys.argv[3]
        if mode == "adapt":
            fetch_and_generate_metadata_scraper(u, d)
        else:
            run_metadata_fetch(u, d)
    elif len(sys.argv) > 2:
        run_metadata_fetch(sys.argv[1], sys.argv[2])

--- END OF FILE: ./metadata_fetcher.py ---

--- START OF FILE: ./alltalk_tts_generator_chunky_17.py ---
import argparse
import glob
import json
import math
import os
import re
import shutil
import sys
import time
import traceback

import requests

# --- 1. WINDOWS UNICODE FIX ---
if sys.platform == "win32":
    sys.stdout.reconfigure(encoding="utf-8")

import nltk
from nltk.tokenize import sent_tokenize

# --- NLTK Setup ---
NLTK_SETUP_SUCCESSFUL = False
try:
    nltk.sent_tokenize("This is a test.")
    NLTK_SETUP_SUCCESSFUL = True
except LookupError:
    print("Attempting to download NLTK 'punkt' resource...")
    try:
        nltk.download("punkt", quiet=False)
        nltk.sent_tokenize("This is a test.")
        print("NLTK 'punkt' is now available.")
        NLTK_SETUP_SUCCESSFUL = True
    except Exception as download_e:
        print(f"An error occurred during 'punkt' download: {download_e}")
if not NLTK_SETUP_SUCCESSFUL:
    print("\nNLTK 'punkt' setup failed. Please resolve this issue manually and re-run.")
    exit(1)

# --- Pydub Setup ---
try:
    from pydub import AudioSegment
    from pydub.exceptions import CouldntDecodeError

    PYDUB_AVAILABLE = True
except ImportError:
    print("Warning: pydub library not found. Audio concatenation will not work.")
    PYDUB_AVAILABLE = False

# --- Configuration ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
ALLTALK_API_URL = "http://127.0.0.1:7851/api/tts-generate"
ALLTALK_BASE_URL = "http://127.0.0.1:7851"

TEXT_FILES_DIR = os.getenv(
    "PROJECT_INPUT_TEXT_DIR", os.path.join(BASE_DIR, "BlleatTL_Novels")
)
AUDIO_OUTPUT_DIR = os.getenv(
    "PROJECT_AUDIO_WAV_DIR", os.path.join(BASE_DIR, "generated_audio_MistakenFairy")
)
PROJECT_ROOT_DIR = os.path.dirname(os.path.abspath(AUDIO_OUTPUT_DIR))

TEMP_CHUNK_DIR = os.path.join(PROJECT_ROOT_DIR, "temp_audio_chunks")
LOG_FILE = os.path.join(PROJECT_ROOT_DIR, "failed_chunks.log")

CHAPTER_START = 0
CHAPTER_STOP = 0

FALLBACK_TOKEN_LIMIT = 170
AVG_CHARS_PER_TOKEN = 1.9
FALLBACK_CHAR_LIMIT = FALLBACK_TOKEN_LIMIT * AVG_CHARS_PER_TOKEN
MIN_BYTES_PER_CHAR = 1500

XTTS_SPEAKER_WAV = None
XTTS_LANGUAGE = "en"
RVC_ENABLE = False
RVC_MODEL_NAME_FOR_API = None
RVC_PITCH = -2
SPEED = 1.0
OUTPUT_FORMAT = "wav"


def _estimate_tokens(text, avg_chars_per_token=AVG_CHARS_PER_TOKEN):
    if not text:
        return 0
    return math.ceil(len(text) / max(1.0, avg_chars_per_token))


def normalize_text(text):
    replacements = {
        "": '"',
        "": '"',
        "": "'",
        "": "'",
        "": "...",
        "": "-",
        "": "-",
    }
    for old, new in replacements.items():
        text = text.replace(old, new)
    return text


def _split_by_force_chars(text_content, char_limit):
    if len(text_content) <= char_limit:
        return [text_content]
    chunks = []
    current_chunk_start = 0
    while current_chunk_start < len(text_content):
        end_index = min(current_chunk_start + int(char_limit), len(text_content))
        if end_index < len(text_content):
            space_index = text_content.rfind(" ", current_chunk_start, end_index)
            if space_index != -1 and space_index > current_chunk_start:
                end_index = space_index
        chunk = text_content[current_chunk_start:end_index].strip()
        if chunk:
            chunks.append(chunk)
        current_chunk_start = end_index + 1
        while (
            current_chunk_start < len(text_content)
            and text_content[current_chunk_start] == " "
        ):
            current_chunk_start += 1
    return chunks


def _split_by_sentence_groups(text_content, token_limit, avg_chars_token_est):
    final_tts_chunks = []
    char_limit = token_limit * avg_chars_token_est
    try:
        sentences = sent_tokenize(text_content)
    except Exception as e:
        print(f"      [!] NLTK sent_tokenize failed: {e}. Falling back to Lvl 3.")
        return _split_by_force_chars(text_content, char_limit)

    if not sentences:
        return []
    current_chunk_sentences_list = []
    current_chunk_tokens = 0

    for sentence_text in sentences:
        sentence_text = sentence_text.strip()
        if not sentence_text:
            continue
        estimated_sentence_tokens = _estimate_tokens(sentence_text, avg_chars_token_est)

        if estimated_sentence_tokens > token_limit:
            if current_chunk_sentences_list:
                final_tts_chunks.append(" ".join(current_chunk_sentences_list))
                current_chunk_sentences_list = []
                current_chunk_tokens = 0
            print(
                f"      [Lvl 2] Sentence too long ({len(sentence_text)} chars). Passing to Lvl 3."
            )
            final_tts_chunks.extend(_split_by_force_chars(sentence_text, char_limit))
        elif current_chunk_tokens + estimated_sentence_tokens <= token_limit:
            current_chunk_sentences_list.append(sentence_text)
            current_chunk_tokens += estimated_sentence_tokens
        else:
            if current_chunk_sentences_list:
                final_tts_chunks.append(" ".join(current_chunk_sentences_list))
            current_chunk_sentences_list = [sentence_text]
            current_chunk_tokens = estimated_sentence_tokens

    if current_chunk_sentences_list:
        final_tts_chunks.append(" ".join(current_chunk_sentences_list))
    return [chunk for chunk in final_tts_chunks if chunk and chunk.strip()]


def _split_by_line_groups(text_content, token_limit, avg_chars_token_est):
    final_tts_chunks = []
    if not text_content or not text_content.strip():
        return final_tts_chunks

    lines = [line.strip() for line in text_content.split("\n") if line.strip()]
    if not lines:
        return []

    current_chunk_lines_list = []
    current_chunk_tokens = 0

    for line_text in lines:
        estimated_line_tokens = _estimate_tokens(line_text, avg_chars_token_est)
        if estimated_line_tokens > token_limit:
            if current_chunk_lines_list:
                final_tts_chunks.append("\n".join(current_chunk_lines_list))
                current_chunk_lines_list = []
                current_chunk_tokens = 0
            print(
                f"      [Lvl 1] Line too long ({len(line_text)} chars). Passing to Lvl 2."
            )
            final_tts_chunks.extend(
                _split_by_sentence_groups(line_text, token_limit, avg_chars_token_est)
            )
        elif current_chunk_tokens + estimated_line_tokens <= token_limit:
            current_chunk_lines_list.append(line_text)
            current_chunk_tokens += estimated_line_tokens
        else:
            if current_chunk_lines_list:
                final_tts_chunks.append("\n".join(current_chunk_lines_list))
            current_chunk_lines_list = [line_text]
            current_chunk_tokens = estimated_line_tokens

    if current_chunk_lines_list:
        final_tts_chunks.append("\n".join(current_chunk_lines_list))
    return [chunk for chunk in final_tts_chunks if chunk and chunk.strip()]


def download_audio_chunk(server_base_url, relative_audio_url, local_temp_path):
    try:
        full_url = server_base_url.rstrip("/") + "/" + relative_audio_url.lstrip("/")
        # print(f"      Downloading: {full_url}")
        response = requests.get(full_url, stream=True, timeout=300)
        response.raise_for_status()
        with open(local_temp_path, "wb") as f:
            shutil.copyfileobj(response.raw, f)
        if os.path.exists(local_temp_path) and os.path.getsize(local_temp_path) > 100:
            return True
        else:
            print(f"      Error: Downloaded file invalid.")
            if os.path.exists(local_temp_path):
                os.remove(local_temp_path)
            return False
    except KeyboardInterrupt:
        raise
    except Exception as e:
        print(f"      Error downloading: {e}")
        return False


def concatenate_audio_chunks(chunk_filepaths, final_output_path):
    if not PYDUB_AVAILABLE:
        return False
    if not chunk_filepaths:
        return False
    print(f"  Concatenating {len(chunk_filepaths)} chunks...")
    combined = AudioSegment.empty()
    for filepath in sorted(chunk_filepaths):
        try:
            combined += AudioSegment.from_wav(filepath)
        except CouldntDecodeError:
            print(f"      Error: Corrupt chunk {filepath}. Skipping.")

    if len(combined) > 0:
        combined.export(final_output_path, format=OUTPUT_FORMAT)
        print(f"  Saved to: {final_output_path}")
        return True
    return False


def process_chapter_file(text_filepath, final_audio_output_path):
    if not XTTS_SPEAKER_WAV:
        print("[Error] XTTS_SPEAKER_WAV is not set.")
        return False

    print(f"\n--- Processing: {os.path.basename(text_filepath)} ---")
    base_filename_no_ext = os.path.splitext(os.path.basename(text_filepath))[0]
    sanitized_base = re.sub(r"[^\w_.-]", "_", base_filename_no_ext)

    chapter_temp_dir = os.path.join(TEMP_CHUNK_DIR, sanitized_base)
    os.makedirs(chapter_temp_dir, exist_ok=True)

    try:
        with open(text_filepath, "r", encoding="utf-8") as f:
            full_text_content = f.read()
        full_text_content = normalize_text(full_text_content)
        if not full_text_content.strip():
            print(f"  Skipping empty file.")
            return True
    except Exception as e:
        print(f"  Error reading file: {e}")
        return False

    initial_text_chunks = _split_by_line_groups(
        full_text_content, FALLBACK_TOKEN_LIMIT, AVG_CHARS_PER_TOKEN
    )
    if not initial_text_chunks:
        print(f"  Warning: No text chunks generated.")
        return False

    pending_jobs = []
    for i, text_content in enumerate(initial_text_chunks):
        pending_jobs.append(
            {"text": text_content, "output_suffix": f"l_{i+1:03d}", "fallback_level": 1}
        )

    generated_audio_files = []
    any_chunk_failed_or_skipped = False
    job_idx = 0

    while job_idx < len(pending_jobs):
        current_job = pending_jobs[job_idx]
        text_to_process = current_job["text"]
        output_suffix = current_job["output_suffix"]
        fallback_level = current_job.get("fallback_level", 1)

        chunk_output_basename = f"{sanitized_base}_{output_suffix}"
        local_chunk_filepath = os.path.join(
            chapter_temp_dir, f"{chunk_output_basename}.{OUTPUT_FORMAT}"
        )

        if (
            os.path.exists(local_chunk_filepath)
            and os.path.getsize(local_chunk_filepath) > 100
        ):
            generated_audio_files.append(local_chunk_filepath)
            job_idx += 1
            continue

        # --- PREPARE PAYLOAD ---
        payload = {
            "text_input": text_to_process,
            "character_voice_gen": XTTS_SPEAKER_WAV,  # Confirmed filename only
            "language": XTTS_LANGUAGE,
            "output_file_name": chunk_output_basename,
            "rvccharacter_voice_gen": RVC_MODEL_NAME_FOR_API if RVC_ENABLE else "",
            "rvccharacter_pitch": RVC_PITCH,
            "speed": SPEED,
        }

        try:
            # --- DEBUG: UNCOMMENT TO SEE EXACTLY WHAT IS SENT ---
            # print(f"DEBUG: Sending Payload: {json.dumps(payload, indent=2)}")

            response = requests.post(ALLTALK_API_URL, data=payload, timeout=720)
            response.raise_for_status()
            response_data = response.json()

            if response_data.get("output_file_url"):
                if download_audio_chunk(
                    ALLTALK_BASE_URL,
                    response_data["output_file_url"],
                    local_chunk_filepath,
                ):
                    generated_audio_files.append(local_chunk_filepath)
                else:
                    raise Exception("Download failed.")
            else:
                # Print the payload if we get an API error to debug
                print(f"[!] API Error. Payload Sent: {json.dumps(payload)}")
                raise Exception(f"API Error Response: {response_data.get('error')}")

            job_idx += 1
            time.sleep(0.1)

        except Exception as e:
            print(f"      [!!] Error: {e}")
            time.sleep(2)

            new_sub_jobs = []
            if fallback_level == 1:
                print(f"      -> Falling back to Lvl 2 (Sentence Split)")
                chunks = _split_by_sentence_groups(
                    text_to_process, FALLBACK_TOKEN_LIMIT, AVG_CHARS_PER_TOKEN
                )
                for i, c in enumerate(chunks):
                    new_sub_jobs.append(
                        {
                            "text": c,
                            "output_suffix": f"{output_suffix}_s_{i+1:02d}",
                            "fallback_level": 2,
                        }
                    )
            elif fallback_level == 2:
                print(f"      -> Falling back to Lvl 3 (Force Split)")
                chunks = _split_by_force_chars(text_to_process, FALLBACK_CHAR_LIMIT)
                for i, c in enumerate(chunks):
                    new_sub_jobs.append(
                        {
                            "text": c,
                            "output_suffix": f"{output_suffix}_f_{i+1:02d}",
                            "fallback_level": 3,
                        }
                    )

            if new_sub_jobs:
                pending_jobs = (
                    pending_jobs[:job_idx] + new_sub_jobs + pending_jobs[job_idx + 1 :]
                )
                continue
            else:
                print(f"      [Fail] Could not recover. Skipping chunk.")
                any_chunk_failed_or_skipped = True
                with open(LOG_FILE, "a", encoding="utf-8") as log_f:
                    log_f.write(f"FAILED: {output_suffix}\nText: {text_to_process}\n\n")
                job_idx += 1

    if not generated_audio_files:
        return False

    if concatenate_audio_chunks(generated_audio_files, final_audio_output_path):
        if not any_chunk_failed_or_skipped:
            try:
                shutil.rmtree(chapter_temp_dir)
            except:
                pass
        return True
    return False


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="AllTalk TTS Generator")
    parser.add_argument(
        "--voice_filename",
        type=str,
        required=True,
        help="Filename of the XTTS reference WAV",
    )
    parser.add_argument(
        "--rvc_model", type=str, default=None, help="Name/Path of the RVC model"
    )
    args = parser.parse_args()

    # --- CRITICAL FIX: SANITIZE INPUT ---
    if args.voice_filename:
        # Take the raw input and strip EVERYTHING except the filename
        raw_input = args.voice_filename
        XTTS_SPEAKER_WAV = os.path.basename(raw_input)

        print(f"--------------------------------------------------")
        print(f"DEBUG CONFIG CHECK:")
        print(f"Raw Input: '{raw_input}'")
        print(f"Sanitized: '{XTTS_SPEAKER_WAV}'")
        print(f"--------------------------------------------------")

    else:
        print("Error: --voice_filename is required.")
        sys.exit(1)

    if args.rvc_model and args.rvc_model.lower() != "none" and args.rvc_model != "":
        RVC_ENABLE = True
        RVC_MODEL_NAME_FOR_API = args.rvc_model
        print(f"[Config] RVC Enabled: {RVC_MODEL_NAME_FOR_API}")
    else:
        RVC_ENABLE = False
        RVC_MODEL_NAME_FOR_API = ""

    if not os.path.exists(TEMP_CHUNK_DIR):
        os.makedirs(TEMP_CHUNK_DIR)
    if not os.path.exists(AUDIO_OUTPUT_DIR):
        os.makedirs(AUDIO_OUTPUT_DIR)

    text_files = sorted(glob.glob(os.path.join(TEXT_FILES_DIR, "*.txt")))
    if not text_files:
        print(f"No .txt files found in {TEXT_FILES_DIR}")
        exit(1)

    print(f"Found {len(text_files)} files.")

    chapters_succeeded = 0
    chapters_failed = 0

    try:
        for idx, text_file_path in enumerate(text_files):
            if CHAPTER_STOP > 0 and (idx + 1) > CHAPTER_STOP:
                break
            base_name = os.path.splitext(os.path.basename(text_file_path))[0]
            clean_name = re.sub(r"[^\w_.-]", "_", base_name)
            out_path = os.path.join(AUDIO_OUTPUT_DIR, f"{clean_name}.{OUTPUT_FORMAT}")

            if os.path.exists(out_path) and os.path.getsize(out_path) > 1024:
                print(f"Skipping {clean_name} (Exists)")
                chapters_succeeded += 1
                continue

            if process_chapter_file(text_file_path, out_path):
                chapters_succeeded += 1
            else:
                chapters_failed += 1

    except KeyboardInterrupt:
        print("\n[STOPPED] User interrupted.")

--- END OF FILE: ./alltalk_tts_generator_chunky_17.py ---

--- START OF FILE: ./constants.py ---
GEMINI_MODEL_NAME = "gemini-3-flash-preview"
CONFIG_FILE = "alltalk_path_config.json"

--- END OF FILE: ./constants.py ---

--- START OF FILE: ./test_env.py ---
import torch
import undetected_chromedriver as uc
import whisper
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options


def print_separator():
    print("\n" + "=" * 50 + "\n")


def test_pytorch_cuda():
    print(" TESTING PYTORCH & CUDA...")
    print(f"PyTorch Version: {torch.__version__}")
    cuda_available = torch.cuda.is_available()
    print(f"CUDA Available: {cuda_available}")

    if cuda_available:
        print(f"GPU Device: {torch.cuda.get_device_name(0)}")
        # Test a simple tensor operation on GPU
        x = torch.rand(5, 3).cuda()
        print(" Success: Tensor successfully loaded to GPU.")
    else:
        print(" Warning: CUDA is not available. PyTorch is using CPU.")


def test_scraping_tools():
    print(" TESTING SCRAPING TOOLS (BeautifulSoup)...")
    html_doc = "<html><head><title>The Scraper</title></head><body><p class='title'><b>Success!</b></p></body></html>"
    soup = BeautifulSoup(html_doc, "html.parser")
    print(f"BeautifulSoup parsed title: {soup.title.string}")
    print(" Success: BeautifulSoup is working.")


def test_whisper():
    print(" TESTING WHISPER AI...")
    model_size = "tiny"  # Using tiny model for quick download and test
    print(f"Loading Whisper '{model_size}' model...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    try:
        model = whisper.load_model(model_size, device=device)
        print(f" Success: Whisper loaded on {device.upper()}.")
    except Exception as e:
        print(f" Whisper failed to load: {e}")


if __name__ == "__main__":
    print_separator()
    print(" STARTING ENVIRONMENT DIAGNOSTICS")
    print_separator()

    test_pytorch_cuda()
    print_separator()

    test_scraping_tools()
    print_separator()

    test_whisper()
    print_separator()

    print("Diagnostics complete! You are ready to scrape. ")

--- END OF FILE: ./test_env.py ---

--- START OF FILE: ./convert_audio_to_opus_3.py ---
import glob
import os
import sys

from pydub import AudioSegment

# --- 1. WINDOWS UNICODE FIX ---
if sys.platform == "win32":
    sys.stdout.reconfigure(encoding="utf-8")

# --- Configuration ---
# 1. Dynamic Inputs from GUI
# Defaults are fallbacks for testing
WAV_AUDIO_DIR = os.getenv("WAV_AUDIO_DIR", "generated_audio_MistakenFairy")
OPUS_OUTPUT_DIR = os.getenv("OPUS_OUTPUT_DIR", "generated_audio_MistakenFairy_opus")

# Opus Export Settings
# 48k is excellent for speech; 32k is the sweet spot for file size vs quality.
OPUS_BITRATE = "48k"

# Normalization Settings
ENABLE_NORMALIZATION = True
NORMALIZATION_TARGET_DBFS = -20.0  # Industry standard for clear, consistent narration.

DELETE_ORIGINAL_WAV = False  # Keep as False until you verify the Opus quality
# --- End Configuration ---


def normalize_audio(sound, target_dbfs):
    """Normalizes a pydub AudioSegment object to target dBFS."""
    if sound.dBFS == float("-inf"):
        print("   Warning: Audio segment is silent, skipping normalization.")
        return sound
    change_in_dbfs = target_dbfs - sound.dBFS
    return sound.apply_gain(change_in_dbfs)


def convert_wav_to_opus(
    wav_filepath,
    opus_filepath,
    bitrate="48k",
    apply_normalization=False,
    target_dbfs=-20.0,
):
    """Converts a WAV file to Opus, optionally normalizing and converting to mono."""
    try:
        print(
            f"Processing: {os.path.basename(wav_filepath)} -> {os.path.basename(opus_filepath)}"
        )
        audio = AudioSegment.from_wav(wav_filepath)

        # 1. Apply Normalization
        if apply_normalization:
            print(f"   Normalizing to {target_dbfs} dBFS...")
            audio = normalize_audio(audio, target_dbfs)

        # 2. Force Mono
        # Audiobooks don't need stereo. Mono cuts Opus file size in half without losing quality.
        if audio.channels > 1:
            print(f"   Info: Converting to mono for smaller file size.")
            audio = audio.set_channels(1)

        # 3. Export to Opus
        # Uses libopus codec via ffmpeg
        print(f"   Exporting Opus ({bitrate})...")
        audio.export(
            opus_filepath,
            format="opus",
            parameters=["-c:a", "libopus", "-b:a", bitrate],
        )

        print(f"   Success.")
        return True
    except Exception as e:
        print(f"   Error processing {wav_filepath}: {e}")
        return False


if __name__ == "__main__":
    print(f"--- Audio Processing & Opus Conversion ---")
    print(f"Input: {WAV_AUDIO_DIR}")
    print(f"Output: {OPUS_OUTPUT_DIR}")

    if not os.path.isdir(WAV_AUDIO_DIR):
        print(f"Error: Input directory '{WAV_AUDIO_DIR}' not found.")
        # If run via GUI, we exit cleanly so the pipe catches the error
        sys.exit(1)

    if not os.path.exists(OPUS_OUTPUT_DIR):
        os.makedirs(OPUS_OUTPUT_DIR)

    # Search for files matching any WAV pattern (handling different naming conventions)
    wav_files = sorted(glob.glob(os.path.join(WAV_AUDIO_DIR, "*.wav")))

    if not wav_files:
        print(f"No WAV files found in '{WAV_AUDIO_DIR}'.")
        sys.exit(0)  # Not an error, just nothing to do

    print(
        f"Found {len(wav_files)} WAV files. Normalization: {'ON' if ENABLE_NORMALIZATION else 'OFF'}"
    )

    processed = 0
    skipped = 0
    failed = 0

    for wav_path in wav_files:
        filename_no_ext = os.path.splitext(os.path.basename(wav_path))[0]
        opus_path = os.path.join(OPUS_OUTPUT_DIR, f"{filename_no_ext}.opus")

        # Skip if already converted
        if os.path.exists(opus_path):
            print(f"Skipping: '{filename_no_ext}.opus' already exists.")
            skipped += 1
            continue

        success = convert_wav_to_opus(
            wav_path,
            opus_path,
            bitrate=OPUS_BITRATE,
            apply_normalization=ENABLE_NORMALIZATION,
            target_dbfs=NORMALIZATION_TARGET_DBFS,
        )

        if success:
            processed += 1
            if DELETE_ORIGINAL_WAV:
                try:
                    os.remove(wav_path)
                    print(f"   Deleted original WAV.")
                except Exception as e:
                    print(f"   Warning: Could not delete WAV: {e}")
        else:
            failed += 1

    print(f"\n--- Done ---")
    print(f"Processed: {processed} | Skipped: {skipped} | Failed: {failed}")

--- END OF FILE: ./convert_audio_to_opus_3.py ---

--- START OF FILE: ./txt_to_epub.py ---
import html
import json
import os
import re
import sys
import uuid

from ebooklib import epub


def create_xhtml_chapter(chapter_title, text_content, chapter_file_name_base):
    """
    Converts plain text content to a simple XHTML chapter object.
    """
    file_name = f"{chapter_file_name_base}.xhtml"
    chapter = epub.EpubHtml(title=chapter_title, file_name=file_name, lang="en")

    # Create HTML content
    escaped_title = html.escape(chapter_title)
    xhtml_content_parts = [f"<h1>{escaped_title}</h1>"]

    # Split by blank lines to form paragraphs
    # Handle various line ending types
    paragraphs = re.split(r"\n\s*\n+", text_content.strip())

    for para_text in paragraphs:
        cleaned_para = para_text.strip()
        if cleaned_para:
            escaped_para = html.escape(cleaned_para)
            # Preserve internal line breaks within a paragraph
            escaped_para = escaped_para.replace("\r\n", "<br />\n").replace(
                "\n", "<br />\n"
            )
            xhtml_content_parts.append(f"<p>{escaped_para}</p>")

    chapter.content = "\n".join(xhtml_content_parts)
    return chapter


def create_epub_project():
    # --- CRITICAL FIX FOR WINDOWS UNICODE ERROR ---
    # Forces the console output to use UTF-8 instead of the default Windows cp1252
    if sys.platform == "win32":
        sys.stdout.reconfigure(encoding="utf-8")

    print("--- Starting EPUB Creation ---")

    # --- 1. Setup Paths ---
    # The GUI passes 'EPUB_INPUT_DIR' (e.g., Novels/MyBook/01_Raw_Text)
    # The GUI passes 'EPUB_OUTPUT_FILE' (e.g., Novels/MyBook/MyBook.epub)

    TEXT_INPUT_DIR = os.getenv("EPUB_INPUT_DIR")
    OUTPUT_FILE = os.getenv("EPUB_OUTPUT_FILE")

    # Standalone fallback (for testing without GUI)
    if not TEXT_INPUT_DIR:
        print("Warning: Running standalone. Using default relative paths.")
        TEXT_INPUT_DIR = "BlleatTL_Novels"
        OUTPUT_FILE = "Output.epub"

    if not os.path.exists(TEXT_INPUT_DIR):
        print(f"Error: Input directory not found: {TEXT_INPUT_DIR}")
        return

    # Determine Project Root (One level up from text dir)
    # If text dir is ".../Novels/Title/01_Raw_Text", root is ".../Novels/Title"
    PROJECT_ROOT = os.path.dirname(os.path.abspath(TEXT_INPUT_DIR))

    METADATA_JSON = os.path.join(PROJECT_ROOT, "metadata.json")
    COVER_IMAGE = os.path.join(PROJECT_ROOT, "cover.jpg")
    CHAPTERS_JSON = os.path.join(TEXT_INPUT_DIR, "chapters.json")

    # --- 2. Load Metadata ---
    # Default values
    book_meta = {
        "title": os.getenv("EPUB_TITLE", "Unknown Title"),
        "author": "Unknown Author",
        "description": "Generated by Auto-Audiobook Pipeline",
    }

    # Override with metadata.json if exists
    if os.path.exists(METADATA_JSON):
        print(f"Loading metadata from: {METADATA_JSON}")
        try:
            with open(METADATA_JSON, "r", encoding="utf-8") as f:
                loaded_meta = json.load(f)
                # Only update keys that contain data
                if loaded_meta.get("title"):
                    book_meta["title"] = loaded_meta["title"]
                if loaded_meta.get("author"):
                    book_meta["author"] = loaded_meta["author"]
                if loaded_meta.get("description"):
                    book_meta["description"] = loaded_meta["description"]
        except Exception as e:
            print(f"Warning: Failed to parse metadata.json: {e}")

    # Safe Print to avoid crash if reconfigure fails for some reason
    try:
        print(f"Book Title: {book_meta['title']}")
        print(f"Book Author: {book_meta['author']}")
    except UnicodeEncodeError:
        print(f"Book Author: [Complex Characters Hidden]")

    # --- 3. Setup EPUB Book Object ---
    book = epub.EpubBook()
    book.set_identifier(str(uuid.uuid4()))
    book.set_title(book_meta["title"])
    book.set_language("en")
    book.add_author(book_meta["author"])

    if book_meta["description"]:
        book.add_metadata("DC", "description", book_meta["description"])

    # Handle Cover
    if os.path.exists(COVER_IMAGE):
        try:
            with open(COVER_IMAGE, "rb") as f:
                book.set_cover("cover.jpg", f.read())
            print(f"Attached cover image: {COVER_IMAGE}")
        except Exception as e:
            print(f"Error attaching cover: {e}")
    else:
        print("No cover.jpg found in project root.")

    # --- 4. Load Chapter Order ---
    chapter_entries = []

    # Try loading strictly ordered list from scraper
    if os.path.exists(CHAPTERS_JSON):
        try:
            with open(CHAPTERS_JSON, "r", encoding="utf-8") as f:
                data = json.load(f)
                chapter_entries = [entry for entry in data if entry.get("file")]
            print(f"Loaded order from chapters.json ({len(chapter_entries)} chapters)")
        except Exception as e:
            print(f"Error loading chapters.json: {e}")

    # Fallback: Alphabetical sort of text files
    if not chapter_entries:
        print("Falling back to alphabetical file sort.")
        txt_files = sorted(
            [f for f in os.listdir(TEXT_INPUT_DIR) if f.lower().endswith(".txt")]
        )
        chapter_entries = [{"file": f} for f in txt_files]

    if not chapter_entries:
        print("No .txt files found to compile.")
        return

    # --- 5. Process Chapters ---
    epub_chapters = []
    toc_links = []

    print(f"Processing text files...")
    for i, entry in enumerate(chapter_entries):
        txt_filename = entry["file"]
        txt_filepath = os.path.join(TEXT_INPUT_DIR, txt_filename)

        if not os.path.exists(txt_filepath):
            print(f"  [Skipped] Missing file: {txt_filename}")
            continue

        try:
            with open(txt_filepath, "r", encoding="utf-8") as f:
                lines = f.readlines()
                if not lines:
                    continue

                # Heuristic: The first line is often the Title (saved by scraper)
                # If the first line is very short, treat it as title. Otherwise default.
                first_line = lines[0].strip()
                if len(first_line) < 200:
                    final_title = first_line
                    body_content = "".join(lines[1:]).strip()
                else:
                    final_title = f"Chapter {i+1}"
                    body_content = "".join(lines).strip()

            # Create XHTML Chapter Object
            chapter_obj = create_xhtml_chapter(
                final_title, body_content, f"chapter_{i+1:04d}"
            )
            book.add_item(chapter_obj)
            epub_chapters.append(chapter_obj)
            toc_links.append(
                epub.Link(chapter_obj.file_name, final_title, f"chapter_{i+1:04d}")
            )

        except Exception as e:
            print(f"  [Error] Failed to process {txt_filename}: {e}")

    # --- 6. Finalize EPUB ---
    book.toc = tuple(toc_links)
    book.add_item(epub.EpubNcx())
    book.add_item(epub.EpubNav())

    # CSS Styling
    css = """
    body { margin: 5%; font-family: serif; font-size: 1.1em; line-height: 1.6; }
    h1 { text-align: center; margin-top: 2em; margin-bottom: 1em; font-weight: bold; border-bottom: 1px solid #ddd; padding-bottom: 0.5em;}
    p { text-indent: 1.5em; margin-bottom: 0.5em; text-align: justify; }
    """
    style_item = epub.EpubItem(
        uid="style_default",
        file_name="style/default.css",
        media_type="text/css",
        content=css,
    )
    book.add_item(style_item)

    book.spine = ["nav"] + epub_chapters
    for ch in epub_chapters:
        ch.add_item(style_item)

    # Write file
    try:
        epub.write_epub(OUTPUT_FILE, book, {})
        print(f"SUCCESS! EPUB saved to: {OUTPUT_FILE}")
    except Exception as e:
        print(f"Error saving EPUB file: {e}")


if __name__ == "__main__":
    create_epub_project()

--- END OF FILE: ./txt_to_epub.py ---

--- START OF FILE: ./fetch_metadata.py ---
import json
import os
import shutil

import requests
from bs4 import BeautifulSoup

# --- CONFIGURATION ---
# The URL of the main page (Novel Info / Table of Contents)
NOVEL_INDEX_URL = "https://www.blleattl.site/story/fairy/"
# ---------------------


def fetch_and_save_metadata(index_url, project_dir):
    print(f"--- Fetching Metadata from: {index_url} ---")

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    }

    try:
        response = requests.get(index_url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, "html.parser")

        # --- 1. Extract Metadata using OpenGraph (Most Reliable) ---
        metadata = {
            "title": "Unknown Title",
            "author": "Unknown Author",
            "cover_url": None,
            "description": "",
        }

        # Title
        og_title = soup.find("meta", property="og:title")
        if og_title:
            metadata["title"] = og_title["content"].strip()
        else:
            # Fallback to standard H1
            h1 = soup.find("h1")
            if h1:
                metadata["title"] = h1.get_text(strip=True)

        # Cover Image
        og_image = soup.find("meta", property="og:image")
        if og_image:
            metadata["cover_url"] = og_image["content"]

        # Author (Site specific fallback)
        # Try to find common "Author" labels
        author_el = soup.find(string=lambda t: t and "Author" in t)
        if author_el:
            # Often extracting parent text helps, e.g. <div>Author: Name</div>
            parent_text = author_el.find_parent().get_text(strip=True)
            # Simple cleanup: remove "Author" and colons
            metadata["author"] = (
                parent_text.replace("Author", "").replace(":", "").strip()
            )

        # Description
        og_desc = soup.find("meta", property="og:description")
        if og_desc:
            metadata["description"] = og_desc["content"].strip()

        print(
            f"Found Metadata:\n Title: {metadata['title']}\n Author: {metadata['author']}\n Cover: {metadata['cover_url']}"
        )

        # --- 2. Download Cover Image ---
        cover_filename = None
        if metadata["cover_url"]:
            try:
                # Resolve relative URLs if necessary
                if not metadata["cover_url"].startswith("http"):
                    from urllib.parse import urljoin

                    metadata["cover_url"] = urljoin(index_url, metadata["cover_url"])

                print(f"Downloading cover image...")
                img_resp = requests.get(
                    metadata["cover_url"], headers=headers, stream=True
                )
                if img_resp.status_code == 200:
                    ext = os.path.splitext(metadata["cover_url"])[1].split("?")[
                        0
                    ]  # Get .jpg/.png
                    if not ext:
                        ext = ".jpg"

                    cover_filename = f"cover{ext}"
                    cover_path = os.path.join(project_dir, cover_filename)

                    with open(cover_path, "wb") as f:
                        shutil.copyfileobj(img_resp.raw, f)

                    print(f"Cover saved to: {cover_path}")
                    metadata["local_cover_path"] = cover_filename
            except Exception as e:
                print(f"Failed to download cover: {e}")

        # --- 3. Save to JSON ---
        json_path = os.path.join(project_dir, "project_metadata.json")
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(metadata, f, indent=4)

        print(f"--- Success! Metadata saved to {json_path} ---")

    except Exception as e:
        print(f"Error fetching metadata: {e}")


if __name__ == "__main__":
    # Determine base directory dynamically
    BASE_DIR = os.path.dirname(os.path.abspath(__file__))

    # We save metadata in the Project Root (BASE_DIR), not the text folder
    fetch_and_save_metadata(NOVEL_INDEX_URL, BASE_DIR)

--- END OF FILE: ./fetch_metadata.py ---

--- START OF FILE: ./pipe_system_gui.py ---
import glob
import json
import os
import shutil
import subprocess
import sys
import threading
import tkinter as tk
from tkinter import filedialog, messagebox, scrolledtext, simpledialog, ttk

# --- CONFIGURATION ---
NOVELS_ROOT_DIR = "Novels"
CONFIG_FILE = "alltalk_path_config.json"

try:
    from constants import GEMINI_MODEL_NAME
except ImportError:
    GEMINI_MODEL_NAME = "gemini-3-flash-preview"

SCRIPTS = {
    "Scraper": "scraper_2.py",
    "Metadata": "metadata_fetcher.py",
    "Translate (Gemini)": "gemini_transelate_4.py",
    "Translate (Grok)": "grok_transelate.py",
    # We dynamically set TTS based on engine choice now
    "TTS Generator": "alltalk_tts_generator_chunky_17.py",
    "Audio Converter": "convert_audio_to_opus_3.py",
    "Tag Audio": "tag_audiobook_files_opus_3.py",
    "EPUB Creator": "txt_to_epub.py",
}


class PipelineGUI:
    def __init__(self, root):
        self.root = root
        self.root.title("Audiobook Pipe System")
        self.root.geometry("900x950")

        if not os.path.exists(NOVELS_ROOT_DIR):
            os.makedirs(NOVELS_ROOT_DIR)

        self.current_project = tk.StringVar()
        self.index_url = tk.StringVar()

        # Source Selection
        self.input_source_var = tk.StringVar(value="Raw")

        # TTS Engine Switcher
        self.tts_engine_var = tk.StringVar(value="AllTalk")

        # AllTalk Config Vars
        self.alltalk_path_var = tk.StringVar()
        self.selected_voice_var = tk.StringVar()
        self.selected_rvc_var = tk.StringVar()

        # Qwen+RVC Config Vars
        self.qwen_pth_var = tk.StringVar()
        self.qwen_index_var = tk.StringVar()
        self.qwen_pitch_var = tk.IntVar(value=-2)

        self.pipeline_vars = {
            "scraper": tk.BooleanVar(value=True),
            "translate": tk.BooleanVar(value=False),
            "epub": tk.BooleanVar(value=True),
            "tts": tk.BooleanVar(value=True),
            "convert": tk.BooleanVar(value=True),
            "tag": tk.BooleanVar(value=True),
        }
        self.trans_engine = tk.StringVar(value="Translate (Gemini)")

        # Adapt Vars
        self.adapt_url_var = tk.StringVar()
        self.adapt_type_var = tk.StringVar(value="Chapter Scraper")

        self.current_process = None
        self.stop_requested = False

        self.load_config()
        self.create_ui()
        self.refresh_project_list()

        if self.alltalk_path_var.get():
            self.scan_alltalk_content()

    def create_ui(self):
        # --- TOP BAR ---
        top_frame = ttk.LabelFrame(self.root, text="Project Management")
        top_frame.pack(fill="x", padx=10, pady=5)

        ttk.Label(top_frame, text="Project:").pack(side="left", padx=5)
        self.project_dropdown = ttk.Combobox(
            top_frame, textvariable=self.current_project, state="readonly", width=25
        )
        self.project_dropdown.pack(side="left", padx=5)
        self.project_dropdown.bind("<<ComboboxSelected>>", self.on_project_change)

        ttk.Button(
            top_frame, text="New", command=self.create_new_project, width=5
        ).pack(side="left", padx=2)
        ttk.Button(
            top_frame, text="Folder", command=self.open_project_folder, width=6
        ).pack(side="left", padx=2)

        ttk.Label(top_frame, text=" | Index URL:").pack(side="left", padx=5)
        ttk.Entry(top_frame, textvariable=self.index_url, width=30).pack(
            side="left", padx=5
        )
        ttk.Button(top_frame, text="Get Meta", command=self.run_metadata_fetch).pack(
            side="left", padx=2
        )

        # --- TABS ---
        tabs = ttk.Notebook(self.root)
        self.tab_run = ttk.Frame(tabs)
        self.tab_adapt = ttk.Frame(tabs)
        tabs.add(self.tab_run, text="Run Pipeline")
        tabs.add(self.tab_adapt, text="AI Adapter")
        tabs.pack(expand=True, fill="both", padx=10, pady=5)

        # --- TAB 1: PIPELINE ---

        # 1. TTS Engine Setup
        tts_frame = ttk.LabelFrame(self.tab_run, text="TTS Generation Setup")
        tts_frame.pack(fill="x", padx=10, pady=5)

        # Engine Selector
        eng_sel_frame = ttk.Frame(tts_frame)
        eng_sel_frame.pack(fill="x", padx=5, pady=5)
        ttk.Label(eng_sel_frame, text="Select Engine:").pack(side="left", padx=5)
        ttk.Radiobutton(
            eng_sel_frame,
            text="AllTalk (External API)",
            variable=self.tts_engine_var,
            value="AllTalk",
            command=self.toggle_tts_ui,
        ).pack(side="left", padx=10)
        ttk.Radiobutton(
            eng_sel_frame,
            text="Qwen + RVC (Local GPU)",
            variable=self.tts_engine_var,
            value="Qwen",
            command=self.toggle_tts_ui,
        ).pack(side="left", padx=10)

        # 1A. AllTalk Controls (Frame)
        self.alltalk_frame = ttk.Frame(tts_frame)
        self.alltalk_frame.pack(fill="x", padx=5, pady=5)

        at_path_frame = ttk.Frame(self.alltalk_frame)
        at_path_frame.pack(fill="x", pady=2)
        ttk.Label(at_path_frame, text="AllTalk Root Dir:").pack(side="left", padx=5)
        ttk.Entry(at_path_frame, textvariable=self.alltalk_path_var).pack(
            side="left", padx=5, fill="x", expand=True
        )
        ttk.Button(at_path_frame, text="Browse", command=self.browse_alltalk).pack(
            side="left"
        )
        ttk.Button(
            at_path_frame, text="Scan Voices", command=self.scan_alltalk_content
        ).pack(side="left", padx=5)

        at_opts_frame = ttk.Frame(self.alltalk_frame)
        at_opts_frame.pack(fill="x", pady=2)
        ttk.Label(at_opts_frame, text="XTTS Voice:").pack(side="left", padx=5)
        self.voice_combo = ttk.Combobox(
            at_opts_frame,
            textvariable=self.selected_voice_var,
            state="readonly",
            width=25,
        )
        self.voice_combo.pack(side="left", padx=5)
        ttk.Label(at_opts_frame, text="RVC Model:").pack(side="left", padx=(15, 5))
        self.rvc_combo = ttk.Combobox(
            at_opts_frame,
            textvariable=self.selected_rvc_var,
            state="readonly",
            width=30,
        )
        self.rvc_combo.pack(side="left", padx=5)

        # 1B. Qwen Controls (Frame)
        self.qwen_frame = ttk.Frame(tts_frame)

        q_pth_frame = ttk.Frame(self.qwen_frame)
        q_pth_frame.pack(fill="x", pady=2)
        ttk.Label(q_pth_frame, text="RVC .pth File:").pack(side="left", padx=5)
        ttk.Entry(q_pth_frame, textvariable=self.qwen_pth_var).pack(
            side="left", padx=5, fill="x", expand=True
        )
        ttk.Button(
            q_pth_frame, text="Browse", command=lambda: self.browse_qwen_file("pth")
        ).pack(side="left", padx=5)

        q_idx_frame = ttk.Frame(self.qwen_frame)
        q_idx_frame.pack(fill="x", pady=2)
        ttk.Label(q_idx_frame, text="RVC .index File:").pack(side="left", padx=5)
        ttk.Entry(q_idx_frame, textvariable=self.qwen_index_var).pack(
            side="left", padx=5, fill="x", expand=True
        )
        ttk.Button(
            q_idx_frame, text="Browse", command=lambda: self.browse_qwen_file("index")
        ).pack(side="left", padx=5)

        q_pitch_frame = ttk.Frame(self.qwen_frame)
        q_pitch_frame.pack(fill="x", pady=2)
        ttk.Label(q_pitch_frame, text="Pitch Shift (-12 to 12):").pack(
            side="left", padx=5
        )
        ttk.Spinbox(
            q_pitch_frame, from_=-24, to=24, textvariable=self.qwen_pitch_var, width=5
        ).pack(side="left", padx=5)

        # Initialize View
        self.toggle_tts_ui()

        # 2. Source Selection Frame
        source_frame = ttk.LabelFrame(
            self.tab_run, text="Source Content for TTS & EPUB"
        )
        source_frame.pack(fill="x", padx=10, pady=5)
        ttk.Radiobutton(
            source_frame,
            text="Original Scraped Text (01_Raw_Text)",
            variable=self.input_source_var,
            value="Raw",
        ).pack(side="left", padx=20, pady=5)
        ttk.Radiobutton(
            source_frame,
            text="Translated Text (02_Translated)",
            variable=self.input_source_var,
            value="Translated",
        ).pack(side="left", padx=20, pady=5)

        # 3. Steps Selection
        chk_frame = ttk.LabelFrame(self.tab_run, text="Select Steps")
        chk_frame.pack(fill="x", padx=10, pady=5)

        steps = [
            ("1. Scrape Chapters", "scraper"),
            ("2. Translate (Optional)", "translate"),
            ("3. Create EPUB", "epub"),
            ("4. Generate TTS", "tts"),
            ("5. Convert to Opus", "convert"),
            ("6. Tag Audio", "tag"),
        ]

        for i, (text, key) in enumerate(steps):
            row = i // 2
            col = i % 2
            if key == "translate":
                f = ttk.Frame(chk_frame)
                f.grid(row=row, column=col, sticky="w", padx=10, pady=2)
                ttk.Checkbutton(f, text=text, variable=self.pipeline_vars[key]).pack(
                    side="left"
                )
                ttk.Combobox(
                    f,
                    textvariable=self.trans_engine,
                    values=["Translate (Gemini)", "Translate (Grok)"],
                    state="readonly",
                    width=15,
                ).pack(side="left", padx=5)
            else:
                ttk.Checkbutton(
                    chk_frame, text=text, variable=self.pipeline_vars[key]
                ).grid(row=row, column=col, sticky="w", padx=10, pady=2)

        # 4. Control Buttons
        btn_frame = ttk.Frame(self.tab_run)
        btn_frame.pack(pady=10, fill="x", padx=50)
        self.btn_run = ttk.Button(
            btn_frame, text="START PROCESSING", command=self.start_pipeline_thread
        )
        self.btn_run.pack(side="left", fill="x", expand=True, padx=5)
        self.btn_stop = ttk.Button(
            btn_frame,
            text="STOP / TERMINATE",
            command=self.stop_process,
            state="disabled",
        )
        self.btn_stop.pack(side="right", fill="x", expand=True, padx=5)

        # 5. Logs
        log_label_frame = ttk.LabelFrame(self.tab_run, text="Process Logs")
        log_label_frame.pack(fill="both", expand=True, padx=10, pady=5)
        self.log_area = scrolledtext.ScrolledText(
            log_label_frame, height=15, state="normal", font=("Consolas", 9)
        )
        self.log_area.pack(fill="both", expand=True, padx=5, pady=5)
        self.log_area.bind("<Key>", self.prevent_typing)
        ttk.Button(
            log_label_frame, text="Copy All Logs", command=self.copy_all_logs
        ).pack(pady=2)

        # --- TAB 2: ADAPTER ---
        lbl = ttk.Label(
            self.tab_adapt,
            text="Use AI to write custom scripts for new websites.",
            font=("Arial", 10, "bold"),
        )
        lbl.pack(pady=10)
        adapt_frame = ttk.Frame(self.tab_adapt)
        adapt_frame.pack(pady=5)
        ttk.Label(adapt_frame, text="Target URL:").grid(
            row=0, column=0, padx=5, sticky="e"
        )
        ttk.Entry(adapt_frame, textvariable=self.adapt_url_var, width=50).grid(
            row=0, column=1, padx=5
        )
        ttk.Label(adapt_frame, text="Generate For:").grid(
            row=1, column=0, padx=5, sticky="e"
        )
        ttk.Combobox(
            adapt_frame,
            textvariable=self.adapt_type_var,
            values=["Chapter Scraper", "Metadata Scraper"],
            state="readonly",
        ).grid(row=1, column=1, padx=5, sticky="w")
        ttk.Button(
            self.tab_adapt,
            text="Ask Gemini to Write Script",
            command=self.run_adapt_tool,
        ).pack(pady=15)
        self.adapt_status = ttk.Label(self.tab_adapt, text="Ready", foreground="gray")
        self.adapt_status.pack()

    def toggle_tts_ui(self):
        """Switches the UI visibility based on the selected TTS Engine."""
        if self.tts_engine_var.get() == "AllTalk":
            self.qwen_frame.pack_forget()
            self.alltalk_frame.pack(fill="x", padx=5, pady=5)
        else:
            self.alltalk_frame.pack_forget()
            self.qwen_frame.pack(fill="x", padx=5, pady=5)

    def browse_qwen_file(self, file_type):
        """File browser for Qwen RVC models."""
        ext = "*.pth" if file_type == "pth" else "*.index"
        f = filedialog.askopenfilename(
            title=f"Select RVC {file_type} file",
            filetypes=[(f"RVC {file_type.upper()}", ext)],
        )
        if f:
            if file_type == "pth":
                self.qwen_pth_var.set(f)
            else:
                self.qwen_index_var.set(f)

    # --- ALLTALK LOGIC ---
    def load_config(self):
        try:
            if os.path.exists(CONFIG_FILE):
                with open(CONFIG_FILE, "r") as f:
                    data = json.load(f)
                    self.alltalk_path_var.set(data.get("alltalk_path", ""))
                    self.qwen_pth_var.set(data.get("qwen_pth", ""))
                    self.qwen_index_var.set(data.get("qwen_index", ""))
                    self.qwen_pitch_var.set(data.get("qwen_pitch", -2))
        except Exception as e:
            print(f"Config Error: {e}")

    def save_config(self):
        try:
            with open(CONFIG_FILE, "w") as f:
                json.dump(
                    {
                        "alltalk_path": self.alltalk_path_var.get(),
                        "qwen_pth": self.qwen_pth_var.get(),
                        "qwen_index": self.qwen_index_var.get(),
                        "qwen_pitch": self.qwen_pitch_var.get(),
                    },
                    f,
                )
        except Exception as e:
            print(f"Config Save Error: {e}")

    def browse_alltalk(self):
        d = filedialog.askdirectory(title="Select AllTalk Root Directory")
        if d:
            self.alltalk_path_var.set(d)
            self.save_config()
            self.scan_alltalk_content()

    def scan_alltalk_content(self):
        base = self.alltalk_path_var.get()
        if not base or not os.path.exists(base):
            return

        voices_dir = os.path.join(base, "voices")
        if os.path.exists(voices_dir):
            wavs = glob.glob(os.path.join(voices_dir, "*.wav"))
            voice_names = [os.path.basename(w) for w in wavs]
            self.voice_combo["values"] = voice_names
            if voice_names:
                self.voice_combo.current(0)
            else:
                self.voice_combo.set("No .wav files found")
        else:
            self.voice_combo.set("Voices dir not found")

        rvc_search_roots = [
            os.path.join(base, "models", "rvc_voices"),
            os.path.join(base, "rvc_models"),
            os.path.join(base, "models", "rvc"),
        ]

        rvc_models_found = ["None"]
        valid_root = next((p for p in rvc_search_roots if os.path.exists(p)), None)

        if valid_root:
            try:
                subdirs = [
                    d
                    for d in os.listdir(valid_root)
                    if os.path.isdir(os.path.join(valid_root, d))
                ]
                for subdir in subdirs:
                    subdir_path = os.path.join(valid_root, subdir)
                    pth_files = glob.glob(os.path.join(subdir_path, "*.pth"))
                    for pth in pth_files:
                        rvc_models_found.append(
                            os.path.join(subdir, os.path.basename(pth))
                        )
            except Exception as e:
                print(f"Error scanning RVC folders: {e}")

        self.rvc_combo["values"] = rvc_models_found
        self.rvc_combo.current(1 if len(rvc_models_found) > 1 else 0)

    # --- GENERAL LOGIC ---
    def prevent_typing(self, event):
        if (event.state & 4) and event.keysym.lower() in ["c", "a"]:
            return None
        if event.keysym in [
            "Up",
            "Down",
            "Left",
            "Right",
            "Home",
            "End",
            "Prior",
            "Next",
        ]:
            return None
        return "break"

    def log(self, msg):
        # print(msg)
        # self.log_area.insert(tk.END, msg + "\n")
        # self.log_area.see(tk.END)
        # Schedule the update on the main thread (Thread-Safe)
        self.root.after(0, lambda: self._log_internal(msg))

    def _log_internal(self, msg):
        print(msg)
        # Check if the widget still exists before writing
        try:
            self.log_area.insert(tk.END, msg + "\n")
            self.log_area.see(tk.END)
        except Exception:
            pass  # Window was likely closed

    def copy_all_logs(self):
        content = self.log_area.get("1.0", tk.END)
        self.root.clipboard_clear()
        self.root.clipboard_append(content)
        messagebox.showinfo("Copied", "Logs copied to clipboard.")

    def refresh_project_list(self):
        projects = [
            d
            for d in os.listdir(NOVELS_ROOT_DIR)
            if os.path.isdir(os.path.join(NOVELS_ROOT_DIR, d))
        ]
        self.project_dropdown["values"] = sorted(projects)
        if projects and not self.current_project.get():
            self.current_project.set(projects[0])

    def create_new_project(self):
        name = simpledialog.askstring("New Project", "Enter Novel Name:")
        if name:
            safe_name = (
                "".join([c for c in name if c.isalnum() or c in (" ", "_")])
                .strip()
                .replace(" ", "_")
            )
            path = os.path.join(NOVELS_ROOT_DIR, safe_name)
            if not os.path.exists(path):
                os.makedirs(os.path.join(path, "01_Raw_Text"))
                os.makedirs(os.path.join(path, "02_Translated"))
                os.makedirs(os.path.join(path, "03_Audio_WAV"))
                os.makedirs(os.path.join(path, "04_Audio_Opus"))
                self.log(f"Created project: {safe_name}")
                self.refresh_project_list()
                self.current_project.set(safe_name)
            else:
                messagebox.showerror("Error", "Project exists.")

    def on_project_change(self, event):
        self.log(f"Selected: {self.current_project.get()}")

    def open_project_folder(self):
        proj = self.current_project.get()
        if not proj:
            return
        path = os.path.abspath(os.path.join(NOVELS_ROOT_DIR, proj))
        os.startfile(path) if os.name == "nt" else subprocess.call(["xdg-open", path])

    def get_env_for_project(self):
        proj = self.current_project.get()
        base = os.path.abspath(os.path.join(NOVELS_ROOT_DIR, proj))

        dir_raw = os.path.join(base, "01_Raw_Text")
        dir_trans = os.path.join(base, "02_Translated")
        dir_wav = os.path.join(base, "03_Audio_WAV")
        dir_opus = os.path.join(base, "04_Audio_Opus")

        tts_input = (
            dir_trans if self.input_source_var.get() == "Translated" else dir_raw
        )

        env = os.environ.copy()
        env["PROJECT_RAW_TEXT_DIR"] = dir_raw
        env["PROJECT_TRANS_INPUT_DIR"] = dir_raw
        env["PROJECT_TRANS_OUTPUT_DIR"] = dir_trans
        env["PROJECT_INPUT_TEXT_DIR"] = tts_input
        env["PROJECT_AUDIO_WAV_DIR"] = dir_wav
        env["WAV_AUDIO_DIR"] = dir_wav
        env["OPUS_OUTPUT_DIR"] = dir_opus
        env["EPUB_INPUT_DIR"] = tts_input
        env["EPUB_OUTPUT_FILE"] = os.path.join(base, f"{proj}.epub")
        env["EPUB_TITLE"] = proj.replace("_", " ")
        return env

    def stop_process(self):
        if self.current_process and self.current_process.poll() is None:
            self.stop_requested = True
            self.log("\n!!! STOPPING PROCESS... !!!")
            try:
                self.current_process.terminate()
            except:
                pass

    def run_script(self, script_key):
        if self.stop_requested:
            return False

        # --- DYNAMIC SCRIPT PATH ---
        script_path = SCRIPTS.get(script_key)

        # 1. Custom Scraper Override
        if script_key == "Scraper" and self.current_project.get():
            custom_path = os.path.join(
                NOVELS_ROOT_DIR, self.current_project.get(), "custom_scraper.py"
            )
            if os.path.exists(custom_path):
                script_path = custom_path
                self.log(f"--- Using Custom Chapter Scraper ---")

        # 2. Dynamic Translation Script
        if script_key.startswith("Translate"):
            script_path = SCRIPTS.get(self.trans_engine.get())

        # 3. Dynamic TTS Script
        if script_key == "TTS Generator":
            if self.tts_engine_var.get() == "Qwen":
                script_path = "qwen_tts_generator.py"  # Set to local generator

        if not script_path or not os.path.exists(script_path):
            self.log(f"Error: {script_path} not found.")
            return False

        self.log(f"--- Running {os.path.basename(script_path)} ---")
        env = self.get_env_for_project()
        cmd = [
            sys.executable,
            "-u",
            script_path,
        ]  # Add "-u" to force unbuffered output so logs appear instantly

        # --- Inject Arguments for TTS Generator ---
        if script_key == "TTS Generator":
            # A. AllTalk Arguments
            if self.tts_engine_var.get() == "AllTalk":
                full_voice_val = self.selected_voice_var.get()
                voice_filename = os.path.basename(full_voice_val)
                rvc = self.selected_rvc_var.get()

                if not voice_filename or "No" in voice_filename:
                    self.log("Error: Invalid voice selection.")
                    return False

                cmd.extend(["--voice_filename", voice_filename])
                if rvc and rvc != "None":
                    cmd.extend(["--rvc_model", rvc])

            # B. Qwen+RVC Arguments
            else:
                pth = self.qwen_pth_var.get()
                idx = self.qwen_index_var.get()
                pitch = str(self.qwen_pitch_var.get())

                if not os.path.exists(pth) or not os.path.exists(idx):
                    self.log("Error: Qwen RVC model paths are invalid.")
                    return False

                cmd.extend(
                    ["--rvc_model_path", pth, "--rvc_index_path", idx, "--pitch", pitch]
                )
                self.save_config()  # Save paths for next time

        try:
            self.current_process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                env=env,
                text=True,
                bufsize=1,
                universal_newlines=True,
            )
            for line in self.current_process.stdout:
                if self.stop_requested:
                    self.current_process.terminate()
                    break
                self.log(line.strip())

            self.current_process.wait()
            if self.stop_requested:
                self.log("--- STOPPED ---")
                return False
            return self.current_process.returncode == 0
        except Exception as e:
            self.log(f"Error: {e}")
            return False
        finally:
            self.current_process = None

    def start_pipeline_thread(self):
        if not self.current_project.get():
            messagebox.showwarning("Warning", "Select a project.")
            return
        self.stop_requested = False
        self.btn_run.config(state="disabled")
        self.btn_stop.config(state="normal")
        threading.Thread(target=self.run_pipeline).start()

    def run_pipeline(self):
        try:
            if self.pipeline_vars["scraper"].get():
                if not self.run_script("Scraper"):
                    raise Exception("Scraping Failed")
            if self.pipeline_vars["translate"].get():
                if not self.run_script("Translate"):
                    raise Exception("Translation Failed")
            if self.pipeline_vars["epub"].get():
                if not self.run_script("EPUB Creator"):
                    raise Exception("EPUB Failed")
            if self.pipeline_vars["tts"].get():
                if not self.run_script("TTS Generator"):
                    raise Exception("TTS Failed")
            if self.pipeline_vars["convert"].get():
                if not self.run_script("Audio Converter"):
                    raise Exception("Conversion Failed")
            if self.pipeline_vars["tag"].get():
                if not self.run_script("Tag Audio"):
                    raise Exception("Tagging Failed")

            self.log("=== COMPLETED ===")
        except Exception as e:
            self.log(f"=== PIPELINE ENDED: {e} ===")
        finally:
            self.btn_run.config(state="normal")
            self.btn_stop.config(state="disabled")

    # --- METADATA & ADAPTER LOGIC ---
    def run_metadata_fetch(self):
        proj = self.current_project.get()
        url = self.index_url.get().strip()
        if not proj or not url:
            return messagebox.showwarning("Info", "Select Project and enter Index URL.")
        self.log(f"--- Fetching Metadata ---")

        def _worker():
            try:
                proj_dir = os.path.join(NOVELS_ROOT_DIR, proj)
                proc = subprocess.Popen(
                    [sys.executable, "-u", SCRIPTS["Metadata"], url, proj_dir],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                    text=True,
                    bufsize=1,
                    universal_newlines=True,
                )
                for line in proc.stdout:
                    self.log(line.strip())
                proc.wait()
                if proc.returncode == 0:
                    self.log("Metadata Fetch Complete.")
            except Exception as e:
                self.log(f"Meta Error: {e}")

        threading.Thread(target=_worker).start()

    def run_adapt_tool(self):
        proj = self.current_project.get()
        url = self.adapt_url_var.get().strip()
        mode = self.adapt_type_var.get()

        if not proj or not url:
            return messagebox.showerror("Error", "Select project and URL.")
        if not os.environ.get("GEMINI_API_KEY"):
            return messagebox.showerror("Error", "GEMINI_API_KEY missing.")

        self.adapt_status.config(
            text=f"Generating {mode}... wait...", foreground="blue"
        )

        def _worker():
            try:
                proj_dir = os.path.join(NOVELS_ROOT_DIR, proj)
                if mode == "Chapter Scraper":
                    import scraper_context_fetcher

                    scraper_context_fetcher.fetch_and_generate_scraper(url, proj_dir)
                    target = "custom_scraper.py"
                else:
                    import metadata_fetcher

                    metadata_fetcher.fetch_and_generate_metadata_scraper(url, proj_dir)
                    target = "custom_metadata_scraper.py"

                if os.path.exists(os.path.join(proj_dir, target)):
                    self.root.after(
                        0,
                        lambda: self.adapt_status.config(
                            text=f"Success! {target} created.", foreground="green"
                        ),
                    )
                else:
                    self.root.after(
                        0,
                        lambda: self.adapt_status.config(
                            text="Generation failed.", foreground="red"
                        ),
                    )
            except Exception as e:
                # FIX: Capture the exception string immediately.
                # 'e' is deleted from the local scope when the except block exits.
                error_msg = str(e)
                self.root.after(
                    0,
                    lambda: self.adapt_status.config(
                        text=f"Error: {error_msg}", foreground="red"
                    ),
                )

        threading.Thread(target=_worker).start()


if __name__ == "__main__":
    root = tk.Tk()
    app = PipelineGUI(root)
    root.mainloop()

--- END OF FILE: ./pipe_system_gui.py ---

--- START OF FILE: ./Novels/Villainous_Saintess_CH/custom_scraper.py ---
import os
import json
import time
import requests
from bs4 import BeautifulSoup

# --- CONFIGURATION ---
DELAY_BETWEEN_REQUESTS = 1.5  # Seconds to wait between chapters
START_URL = "https://katreadingcafe.com/young-fox-chapter-0-prologue-the-forsaken-knight/"
SAVE_DIR_NAME = "KatReadingCafe_Novels"
# ---------------------

def get_with_retries(session, url, headers, retries=3):
    for i in range(retries):
        try:
            response = session.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            print(f"   [!] Error fetching {url}: {e}. Retrying ({i+1}/{retries})...")
            time.sleep(2 * (i + 1))
    return None

def extract_and_clean_chapter_data(soup, ch_num):
    """
    Parses the BeautifulSoup object to extract the chapter title and body.
    Removes ads, social sharing, and navigation elements.
    """
    # 1. Identify the Title
    # Try standard WordPress title or the heading inside content
    title_el = soup.select_one('h1.entry-title')
    story_title = title_el.get_text(strip=True) if title_el else f"Chapter {ch_num}"

    # 2. Identify Content Area
    content_el = soup.select_one('.entry-content')
    if not content_el:
        return f"Chapter {ch_num}", "Content could not be found."

    # 3. Remove Clutter
    # Classes common to this site's theme: social sharing, ads, navigation buttons
    junk_selectors = [
        'script', 'style', '.sharedaddy', '.jp-relatedposts', 
        '.wp-block-buttons', '.navigation', '.post-navigation',
        '.entry-meta', '.social-share', 'button', 'section.related-posts'
    ]
    for selector in junk_selectors:
        for junk in content_el.select(selector):
            junk.decompose()

    # 4. Remove internal navigation links (e.g., "Previous | Table of Contents | Next")
    # Often these are simple <a> tags or <em> tags at the top/bottom
    for nav_candidate in content_el.find_all(['a', 'p', 'em']):
        text = nav_candidate.get_text().lower()
        if 'previous chapter' in text or 'next chapter' in text or 'table of contents' in text:
            nav_candidate.decompose()

    # 5. Extract Text Content
    cleaned_body = content_el.get_text(separator='\n\n', strip=True)
    
    # --- DEDUPLICATION LOGIC ---
    # Split by lines to check if the first line is the title we already have
    lines = cleaned_body.split('\n')
    while lines and not lines[0].strip():
        lines.pop(0)
        
    if lines:
        first_line = lines[0].strip()
        # If the first line is very similar to the title or just says "Chapter X"
        if (story_title.lower() in first_line.lower()) or \
           (first_line.lower().startswith(f"chapter {ch_num}")) or \
           (len(first_line) < 100 and "chapter" in first_line.lower()):
            # Remove the duplicate title line
            cleaned_body = "\n".join(lines[1:]).strip()
            
    final_header = f"Chapter {ch_num} - {story_title}"
    return final_header, cleaned_body

def find_next_url(soup):
    """
    Strategy: 
    1. Look for rel="next" attribute.
    2. Look for anchor tags containing the word "Next".
    """
    # Priority 1: WordPress standard rel="next"
    next_link = soup.find('a', rel='next')
    if next_link and next_link.get('href'):
        return next_link['href']

    # Priority 2: Text search within the entry content or navigation area
    for a in soup.find_all('a', href=True):
        link_text = a.get_text(strip=True).lower()
        if link_text == 'next' or 'next chapter' in link_text:
            # Avoid picking up sidebars by checking parent container (optional but safer)
            return a['href']

    return None

def scrape_novel():
    save_directory = os.getenv("PROJECT_RAW_TEXT_DIR", SAVE_DIR_NAME)
    if not os.path.exists(save_directory):
        os.makedirs(save_directory)

    json_path = os.path.join(save_directory, "chapters.json")
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36'
    })

    # --- LOAD HISTORY ---
    history_data = []
    url_history_map = {}
    if os.path.exists(json_path):
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                history_data = json.load(f)
                for entry in history_data:
                    url_history_map[entry['url']] = entry.get('next_url')
        except Exception as e:
            print(f"Error loading history: {e}")

    ch_counter = len(history_data) + 1
    current_url = START_URL

    print(f"[*] Starting scraper at: {current_url}")

    try:
        while current_url:
            # Resume/Skip logic
            if current_url in url_history_map and url_history_map[current_url]:
                print(f"Skipping (already scraped): {current_url}")
                current_url = url_history_map[current_url]
                continue

            print(f"Processing: {current_url}")
            response = get_with_retries(session, current_url, session.headers)
            if not response:
                print("Failed to retrieve content. Stopping.")
                break

            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extract
            full_header, cleaned_body = extract_and_clean_chapter_data(soup, ch_counter)
            next_url = find_next_url(soup)

            # Save File
            filename = f"ch_{ch_counter:04d}.txt"
            filepath = os.path.join(save_directory, filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(f"{full_header}\n\n{cleaned_body}")
            
            print(f"   -> Saved: {full_header}")

            # Update History
            history_entry = {
                "url": current_url,
                "next_url": next_url,
                "file": filename,
                "title": full_header
            }
            history_data.append(history_entry)
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(history_data, f, indent=4)

            # Loop Increment
            if not next_url or next_url == current_url:
                print("No more chapters found.")
                break

            current_url = next_url
            ch_counter += 1
            time.sleep(DELAY_BETWEEN_REQUESTS)

    except KeyboardInterrupt:
        print("\nScraper paused by user.")
    except Exception as e:
        print(f"\nCritical Error: {e}")

if __name__ == '__main__':
    scrape_novel()
--- END OF FILE: ./Novels/Villainous_Saintess_CH/custom_scraper.py ---

--- START OF FILE: ./Novels/Villainous_Saintess_CH/custom_metadata_scraper.py ---

import os
import json
import requests
from bs4 import BeautifulSoup

def scrape_novel_metadata():
    # Configuration from environment variables
    save_dir = os.getenv('SAVE_DIR', '.')
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    # The HTML provided is stored in a variable for processing
    # In a production script, this would be the response.text
    html_content = """<!DOCTYPE html>...""" # Truncated for brevity, using the provided content
    
    # Using the HTML provided in the prompt
    # Note: Since the prompt contains the HTML, we'll assume it's passed into this context.
    # For the sake of a functional script, we'll use a placeholder variable 'html_input'.
    soup = BeautifulSoup(html_input, 'html.parser')

    # 1. Extract Title
    # Try the <title> tag first as fallback, then standard theme selectors
    title = ""
    title_tag = soup.find('h1', class_='entry-title')
    if title_tag:
        title = title_tag.get_text(strip=True)
    else:
        # Fallback to <title> tag and strip site name
        full_title = soup.find('title').get_text(strip=True) if soup.find('title') else ""
        title = full_title.split('  ')[0].split(' - ')[0].strip()

    # 2. Extract Author
    # Typical structure for this theme: .infox .spe span: "Author: Name"
    author = "Unknown"
    author_selectors = soup.find_all('span')
    for span in author_selectors:
        text = span.get_text()
        if 'Author' in text and ':' in text:
            author = text.split(':')[-1].strip()
            break
    
    # 3. Extract Description
    description = ""
    desc_container = soup.select_one('.desc, .entry-content, [itemprop="description"]')
    if desc_container:
        # Remove "More/Less" toggle text often found in this theme
        for extra in desc_container.select('.sysmore, .colap'):
            extra.decompose()
        description = desc_container.get_text(separator="\n", strip=True)

    # 4. Extract Cover Image URL
    cover_url = ""
    # Standard WordPress/LightNovel theme selectors
    img_tag = soup.select_one('.thumb img, .bigcontent img, .wp-post-image')
    if img_tag:
        # Check data-src (lazy load), then src
        cover_url = img_tag.get('data-src') or img_tag.get('src')
        if cover_url and cover_url.startswith('//'):
            cover_url = 'https:' + cover_url

    # Save Metadata to JSON
    metadata = {
        "title": title,
        "author": author,
        "description": description,
        "cover_url": cover_url
    }
    
    metadata_path = os.path.join(save_dir, 'metadata.json')
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=4, ensure_ascii=False)

    # Download Cover Image
    if cover_url:
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Referer': 'https://katreadingcafe.com/'
            }
            img_data = requests.get(cover_url, headers=headers, timeout=15).content
            cover_path = os.path.join(save_dir, 'cover.jpg')
            with open(cover_path, 'wb') as handler:
                handler.write(img_data)
        except Exception as e:
            print(f"Failed to download image: {e}")

if __name__ == "__main__":
    # This block assumes the HTML from the prompt is accessible
    # In the actual environment, replace the 'html_input' with the target content
    import sys
    html_input = sys.stdin.read()
    scrape_novel_metadata()

--- END OF FILE: ./Novels/Villainous_Saintess_CH/custom_metadata_scraper.py ---

--- START OF FILE: ./Novels/youngfox/custom_scraper.py ---
import os
import json
import time
import requests
from bs4 import BeautifulSoup

# --- CONFIGURATION ---
DELAY_BETWEEN_REQUESTS = 0.1  # Seconds to wait between chapters
START_URL = "https://katreadingcafe.com/young-fox-chapter-0-prologue-the-forsaken-knight/"
SAVE_DIR_NAME = "KatReadingCafe_Novels"
# ---------------------

def get_with_retries(session, url, headers, retries=3):
    for i in range(retries):
        try:
            response = session.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            print(f"   [!] Error fetching {url}: {e}. Retrying ({i+1}/{retries})...")
            time.sleep(2 * (i + 1))
    return None

def extract_and_clean_chapter_data(soup, ch_num):
    """
    Parses the BeautifulSoup object to extract the chapter title and body.
    Removes ads, social sharing, and navigation elements.
    """
    # 1. Identify the Title
    # Try standard WordPress title or the heading inside content
    title_el = soup.select_one('h1.entry-title')
    story_title = title_el.get_text(strip=True) if title_el else f"Chapter {ch_num}"

    # 2. Identify Content Area
    content_el = soup.select_one('.entry-content')
    if not content_el:
        return f"Chapter {ch_num}", "Content could not be found."

    # 3. Remove Clutter
    # Classes common to this site's theme: social sharing, ads, navigation buttons
    junk_selectors = [
        'script', 'style', '.sharedaddy', '.jp-relatedposts', 
        '.wp-block-buttons', '.navigation', '.post-navigation',
        '.entry-meta', '.social-share', 'button', 'section.related-posts'
    ]
    for selector in junk_selectors:
        for junk in content_el.select(selector):
            junk.decompose()

    # 4. Remove internal navigation links (e.g., "Previous | Table of Contents | Next")
    # Often these are simple <a> tags or <em> tags at the top/bottom
    for nav_candidate in content_el.find_all(['a', 'p', 'em']):
        text = nav_candidate.get_text().lower()
        if 'previous chapter' in text or 'next chapter' in text or 'table of contents' in text:
            nav_candidate.decompose()

    # 5. Extract Text Content
    cleaned_body = content_el.get_text(separator='\n\n', strip=True)
    
    # --- DEDUPLICATION LOGIC ---
    # Split by lines to check if the first line is the title we already have
    lines = cleaned_body.split('\n')
    while lines and not lines[0].strip():
        lines.pop(0)
        
    if lines:
        first_line = lines[0].strip()
        # If the first line is very similar to the title or just says "Chapter X"
        if (story_title.lower() in first_line.lower()) or \
           (first_line.lower().startswith(f"chapter {ch_num}")) or \
           (len(first_line) < 100 and "chapter" in first_line.lower()):
            # Remove the duplicate title line
            cleaned_body = "\n".join(lines[1:]).strip()
            
    final_header = f"Chapter {ch_num} - {story_title}"
    return final_header, cleaned_body

def find_next_url(soup):
    """
    Strategy: 
    1. Look for rel="next" attribute.
    2. Look for anchor tags containing the word "Next".
    """
    # Priority 1: WordPress standard rel="next"
    next_link = soup.find('a', rel='next')
    if next_link and next_link.get('href'):
        return next_link['href']

    # Priority 2: Text search within the entry content or navigation area
    for a in soup.find_all('a', href=True):
        link_text = a.get_text(strip=True).lower()
        if link_text == 'next' or 'next chapter' in link_text:
            # Avoid picking up sidebars by checking parent container (optional but safer)
            return a['href']

    return None

def scrape_novel():
    print("DEBUG: Custom script has started...", flush=True)
    save_directory = os.getenv("PROJECT_RAW_TEXT_DIR", SAVE_DIR_NAME)
    if not os.path.exists(save_directory):
        os.makedirs(save_directory)

    json_path = os.path.join(save_directory, "chapters.json")
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36'
    })

    # --- LOAD HISTORY ---
    history_data = []
    url_history_map = {}
    if os.path.exists(json_path):
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                history_data = json.load(f)
                for entry in history_data:
                    url_history_map[entry['url']] = entry.get('next_url')
        except Exception as e:
            print(f"Error loading history: {e}")

    ch_counter = len(history_data) + 1
    current_url = START_URL

    print(f"[*] Starting scraper at: {current_url}")

    try:
        while current_url:
            # Resume/Skip logic
            if current_url in url_history_map and url_history_map[current_url]:
                print(f"Skipping (already scraped): {current_url}")
                current_url = url_history_map[current_url]
                continue

            print(f"Processing: {current_url}")
            response = get_with_retries(session, current_url, session.headers)
            if not response:
                print("Failed to retrieve content. Stopping.")
                break

            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extract
            full_header, cleaned_body = extract_and_clean_chapter_data(soup, ch_counter)
            next_url = find_next_url(soup)

            # Save File
            filename = f"ch_{ch_counter:04d}.txt"
            filepath = os.path.join(save_directory, filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(f"{full_header}\n\n{cleaned_body}")
            
            print(f"   -> Saved: {full_header}")

            # Update History
            history_entry = {
                "url": current_url,
                "next_url": next_url,
                "file": filename,
                "title": full_header
            }
            history_data.append(history_entry)
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(history_data, f, indent=4)

            # Loop Increment
            if not next_url or next_url == current_url:
                print("No more chapters found.")
                break

            current_url = next_url
            ch_counter += 1
            time.sleep(DELAY_BETWEEN_REQUESTS)

    except KeyboardInterrupt:
        print("\nScraper paused by user.")
    except Exception as e:
        print(f"\nCritical Error: {e}")

if __name__ == '__main__':
    scrape_novel()

--- END OF FILE: ./Novels/youngfox/custom_scraper.py ---

--- START OF FILE: ./Novels/youngfox/custom_metadata_scraper.py ---
import os
import json
import requests
import sys
from bs4 import BeautifulSoup

def scrape_novel_metadata():
    print("DEBUG: Custom script has started...", flush=True)
    
    # --- 1. CONFIGURATION ---
    # The GUI passes the target URL via an environment variable
    target_url = os.getenv('TARGET_URL')
    save_dir = os.getenv('SAVE_DIR', '.')

    if not target_url:
        print("Error: TARGET_URL environment variable is missing.")
        return

    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    print(f"DEBUG: Fetching {target_url}...", flush=True)

    # --- 2. FETCH HTML ---
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Referer': 'https://www.google.com/'
    }
    
    try:
        response = requests.get(target_url, headers=headers, timeout=15)
        response.raise_for_status()
        html_content = response.text
    except Exception as e:
        print(f"Error fetching URL: {e}")
        return

    soup = BeautifulSoup(html_content, 'html.parser')

    # --- 3. EXTRACTION ---
    
    # Title
    title = "Unknown Title"
    # Try H1 first
    h1 = soup.find('h1')
    if h1:
        title = h1.get_text(strip=True)
    else:
        # Fallback to <title> tag
        if soup.title:
            title = soup.title.get_text(strip=True).split('-')[0].strip()

    # Author
    author = "Unknown Author"
    # Common pattern: Look for text "Author:" or structure
    for span in soup.find_all(['span', 'p', 'div']):
        text = span.get_text(strip=True)
        if "Author" in text and ":" in text:
            parts = text.split(":")
            if len(parts) > 1:
                potential_author = parts[1].strip()
                if len(potential_author) < 50: # Sanity check
                    author = potential_author
                    break
    
    # Description
    description = ""
    # Try common description containers
    desc_container = soup.select_one('.desc, .entry-content, .description, [itemprop="description"]')
    if desc_container:
        # Clean up "read more" buttons
        for extra in desc_container.select('.sysmore, .colap, button'):
            extra.decompose()
        description = desc_container.get_text(separator="\n", strip=True)

    # Cover Image
    cover_url = ""
    # Try standard image locations
    img_tag = soup.select_one('.thumb img, .book-img img, .wp-post-image, [property="og:image"]')
    if img_tag:
        if img_tag.has_attr('content'): # Meta tag
            cover_url = img_tag['content']
        else:
            cover_url = img_tag.get('data-src') or img_tag.get('src')
    
    # Fix relative URLs
    if cover_url and not cover_url.startswith(('http', '//')):
        from urllib.parse import urljoin
        cover_url = urljoin(target_url, cover_url)
    elif cover_url and cover_url.startswith('//'):
        cover_url = 'https:' + cover_url

    print(f"DEBUG: Found Title: {title}")
    print(f"DEBUG: Found Cover: {cover_url}")

    # --- 4. SAVE METADATA ---
    metadata = {
        "title": title,
        "author": author,
        "description": description,
        "cover_url": cover_url
    }
    
    metadata_path = os.path.join(save_dir, 'metadata.json')
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=4, ensure_ascii=False)

    # --- 5. DOWNLOAD COVER ---
    if cover_url:
        try:
            print("DEBUG: Downloading cover...", flush=True)
            img_data = requests.get(cover_url, headers=headers, timeout=15).content
            cover_path = os.path.join(save_dir, 'cover.jpg')
            with open(cover_path, 'wb') as handler:
                handler.write(img_data)
        except Exception as e:
            print(f"Failed to download image: {e}")

if __name__ == "__main__":
    # Removed sys.stdin.read() - The script now fetches the URL itself
    scrape_novel_metadata()

--- END OF FILE: ./Novels/youngfox/custom_metadata_scraper.py ---

--- START OF FILE: ./Novels/IAmThisMurimsCrazyBitch/custom_scraper.py ---
import json
import os
import time
import requests
from bs4 import BeautifulSoup

# --- CONFIGURATION ---
DELAY_BETWEEN_REQUESTS = 1.5
# ---------------------

def get_with_retries(session, url, headers, retries=3):
    for i in range(retries):
        try:
            response = session.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            print(f"   [!] Error fetching {url}: {e}. Retrying ({i+1}/{retries})...")
            time.sleep(3 * (i + 1))
    return None

def extract_and_clean_chapter_data(soup, file_index):
    """
    Extracts text from .chapter-formatting, injects footnotes inline,
    and ignores UI artifacts.
    """
    # 1. Get Title
    # Found in <h1 class="chapter__title">
    title_el = soup.select_one('h1.chapter__title') or soup.select_one('.entry-title')
    raw_title = title_el.get_text(strip=True) if title_el else f"Chapter {file_index}"

    # 2. Get the specific text container
    # The Fictioneer theme puts the actual story text inside this specific class.
    # By selecting this, we automatically exclude the 'Subscribe', 'Next', 'Previous' buttons
    # which live in sibling containers like .chapter__actions.
    content_div = soup.select_one('.chapter-formatting')

    if not content_div:
        return f"Chapter {file_index} - {raw_title}", "[Content Missing]"

    # 3. Inline Footnote Injection
    # The footnotes are usually at the bottom in <ol id="legacy-footnotes">
    footnote_list = soup.select_one('#legacy-footnotes') or soup.select_one('.legacy-footnote-list')
    
    if footnote_list:
        # Create a map of {ref_id: footnote_text}
        # Example ref_id: '#legacy-fnref-1' -> content
        notes_map = {}
        for li in footnote_list.find_all('li'):
            # The return link () adds noise, let's remove it from the note text
            for ret in li.find_all('a', class_='footnote-return'):
                ret.decompose()
            
            # Key is usually the ID of the LI or linked via the anchor inside the body
            # We map the ID of the li to its text
            if li.has_attr('id'):
                notes_map[li['id']] = li.get_text(strip=True)

        # Now find the superscripts in the body and replace them
        for sup in content_div.find_all('sup', class_='legacy-footnote'):
            link = sup.find('a')
            if link and link.has_attr('href'):
                # href="#legacy-footnote-1" -> key="legacy-footnote-1"
                note_key = link['href'].replace('#', '')
                
                if note_key in notes_map:
                    note_text = notes_map[note_key]
                    # Replace the [1] with [^(The note text)]
                    sup.replace_with(f"[^({note_text})]")

    # 4. Clean remaining junk inside the text block
    # Removing paragraph tools (the floating buttons on hover)
    for junk in content_div.find_all(["script", "style", "button", "div"], class_="paragraph-tools"):
        junk.decompose()

    # 5. Extract Text with proper spacing
    # We iterate over paragraphs to ensure newlines are preserved correctly
    text_parts = []
    for tag in content_div.find_all(['p', 'h2', 'h3', 'h4', 'hr']):
        if tag.name == 'hr':
            text_parts.append("-" * 10) # Visual separator
        else:
            text = tag.get_text(" ", strip=True) # Join internal tags with space
            if text:
                text_parts.append(text)

    cleaned_body = "\n\n".join(text_parts)

    # 6. Formatting Header
    # Matches your "Good Output" format: Chapter X - Chapter Title
    final_header = f"Chapter {file_index} - {raw_title}\n\n{raw_title}\n\nby\n\nArchive-nim"
    
    return final_header, cleaned_body

def scrape_and_save_chapters(start_url, start_chapter_num=1, save_directory="UnderHeaven_Novels"):
    save_directory = os.getenv("PROJECT_RAW_TEXT_DIR", save_directory)

    if not os.path.exists(save_directory):
        os.makedirs(save_directory)

    json_path = os.path.join(save_directory, "chapters.json")
    session = requests.Session()
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": "https://under-heaven.com/",
    }

    # --- LOAD HISTORY ---
    history_data = []
    if os.path.exists(json_path):
        try:
            with open(json_path, "r", encoding="utf-8") as f:
                history_data = json.load(f)
        except:
            pass

    visited_urls = {entry['url'] for entry in history_data}
    
    ch_counter = start_chapter_num + len(history_data)
    current_url = start_url

    try:
        while current_url:
            # Check history
            if current_url in visited_urls and len(history_data) > 0:
                existing_entry = next((item for item in history_data if item["url"] == current_url), None)
                if existing_entry and existing_entry.get("next_url"):
                    print(f"Skipping (history): {current_url}")
                    current_url = existing_entry["next_url"]
                    continue

            print(f"Processing: {current_url}")
            response = get_with_retries(session, current_url, headers)
            if not response:
                print("   [!] Failed to retrieve page. Ending loop.")
                break

            soup = BeautifulSoup(response.content, "html.parser")

            # --- FIND NEXT LINK ---
            next_url = None
            # The theme uses specific classes for the big buttons at the bottom
            next_btn = soup.select_one("a._next") 
            
            # Fallback to the Chapter Index modal list if button is missing
            if not next_btn:
                current_li = soup.select_one("#chapter-index-list li.current")
                if current_li:
                    next_li = current_li.find_next_sibling("li")
                    if next_li:
                        next_btn = next_li.find("a")

            if next_btn and next_btn.has_attr('href'):
                next_url = next_btn['href']
                if next_url.startswith("/"):
                    next_url = "https://under-heaven.com" + next_url

            # --- PROCESS CONTENT ---
            full_header, cleaned_body = extract_and_clean_chapter_data(soup, ch_counter)
            
            filename = f"ch_{ch_counter:04d}.txt"
            filepath = os.path.join(save_directory, filename)

            if not os.path.exists(filepath):
                with open(filepath, "w", encoding="utf-8") as f:
                    f.write(f"{full_header}\n\n{cleaned_body}")
                print(f"   -> Saved: {filename}")
            else:
                print(f"   -> File exists: {filename}")

            # Update History
            if current_url not in visited_urls:
                history_entry = {"url": current_url, "next_url": next_url, "file": filename}
                history_data.append(history_entry)
                visited_urls.add(current_url)
                with open(json_path, "w", encoding="utf-8") as f:
                    json.dump(history_data, f, indent=4)

            # Move to next
            if not next_url or next_url == current_url or next_url == "#":
                print("   [i] No more chapters found.")
                break
                
            current_url = next_url
            ch_counter += 1
            time.sleep(DELAY_BETWEEN_REQUESTS)

    except Exception as e:
        print(f"Critical Error: {e}")

if __name__ == "__main__":
    START_URL = "https://under-heaven.com/story/i-am-this-murims-crazy-btch/iatmcb-961/"
    
    # Starts naming files from ch_0961.txt
    scrape_and_save_chapters(START_URL, start_chapter_num=961)

--- END OF FILE: ./Novels/IAmThisMurimsCrazyBitch/custom_scraper.py ---

--- START OF FILE: ./Novels/IAmThisMurimsCrazyBitch/01_Raw_Text/clean_chapters.py ---
import os

def clean_chapter_files():
    # Get the current working directory
    current_directory = os.getcwd()
    
    print(f"Scanning directory: {current_directory}...\n")

    # Loop through every file in the directory
    for filename in os.listdir(current_directory):
        # We only want to process .txt files
        if filename.endswith(".txt"):
            try:
                # Open the file and read all lines
                # encoding='utf-8' is important for special characters
                with open(filename, 'r', encoding='utf-8') as file:
                    lines = file.readlines()

                # LOGIC: Check if this file actually needs cleaning.
                # We look for "Archive-nim" in the first few lines to confirm the pattern.
                needs_cleaning = False
                split_index = 0
                
                # Check the first 15 lines for the "Archive-nim" signature
                for i, line in enumerate(lines[:15]):
                    if "Archive-nim" in line:
                        needs_cleaning = True
                        # The story usually starts 2 lines after "Archive-nim"
                        # (One line for the name, one empty line)
                        split_index = i + 2 
                        break

                if needs_cleaning:
                    # EXTRACT THE TITLE
                    # Based on your example, the clean title is on Line 3 (index 2)
                    # Example: "Chapter 961: Passing Through..."
                    clean_title = lines[2].strip()

                    # EXTRACT THE STORY
                    # We take everything after the "Archive-nim" section
                    if split_index < len(lines):
                        story_content = lines[split_index:]
                    else:
                        story_content = []

                    # PREPARE NEW CONTENT
                    # Title + two newlines + the rest of the story
                    new_content = [clean_title + "\n", "\n"] + story_content

                    # OVERWRITE THE FILE
                    with open(filename, 'w', encoding='utf-8') as file:
                        file.writelines(new_content)
                    
                    print(f"[FIXED] {filename}")
                else:
                    print(f"[SKIPPED] {filename} (Pattern not found)")

            except Exception as e:
                print(f"[ERROR] Could not process {filename}: {e}")

if __name__ == "__main__":
    clean_chapter_files()
    input("\nProcess complete! Press Enter to exit.")

--- END OF FILE: ./Novels/IAmThisMurimsCrazyBitch/01_Raw_Text/clean_chapters.py ---

--- START OF FILE: ./Novels/god_slaughtering_star/custom_scraper.py ---
import os
import re
import json
import time
import requests
from bs4 import BeautifulSoup

# --- CONFIGURATION ---
DELAY_BETWEEN_REQUESTS = 1.5  # Seconds to avoid rate limiting
# ---------------------

def get_with_retries(session, url, headers, retries=3):
    for i in range(retries):
        try:
            response = session.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            print(f"   [!] Error fetching {url}: {e}. Retrying ({i+1}/{retries})...")
            time.sleep(2 * (i + 1))
    return None

def extract_and_clean_chapter_data(soup, ch_num):
    """
    Targets the main content area, removes scripts, styles, and unwanted 
    interactive elements like navigation buttons or social shares.
    """
    # Specific selectors for Kat Reading Cafe / Lightnovel theme
    content_el = soup.select_one('.entry-content') or soup.select_one('.epcontent') or soup.find('article')
    
    if not content_el:
        return f"Chapter {ch_num}", "Content not found."

    # 1. Remove script, style, and known junk classes
    # 'naveps' and 'navimedia' are the previous/next buttons usually found inside content
    for junk in content_el.find_all(['script', 'style', 'div', 'section', 'button', 'noscript'], 
                                   class_=['paragraph-tools', 'social-share', 'sharedaddy', 
                                          'navigation', 'naveps', 'navimedia', 'ephead', 'entry-title']):
        junk.decompose()
        
    # 2. Get text content
    cleaned_body = content_el.get_text(separator='\n\n', strip=True)
    
    # 3. Handle specific watermarks/junk text often found at start/end
    junk_patterns = [
        r"Read latest chapters at.*",
        r"Previous.*Next",
        r"Next Chapter.*",
        r"Previous Chapter.*"
    ]
    for pattern in junk_patterns:
        cleaned_body = re.sub(pattern, "", cleaned_body, flags=re.IGNORECASE)

    # --- LOGIC: TITLE EXTRACTION & DEDUPLICATION ---
    # Try to get title from H1 first
    title_el = soup.find('h1')
    raw_title = title_el.get_text(strip=True) if title_el else f"Chapter {ch_num}"
    
    # Split body into lines to check for duplication
    lines = [line.strip() for line in cleaned_body.split('\n') if line.strip()]
        
    if lines:
        first_line = lines[0]
        # Deduplication: If first line of body is basically the title, remove it
        # Examples: "Chapter 1", "Chapter 1 - The Beginning", or matches H1
        if (f"Chapter {ch_num}" in first_line) or (len(first_line) < 100 and raw_title in first_line) or (first_line == raw_title):
            cleaned_body = "\n\n".join(lines[1:]).strip()
            
    final_header = f"Chapter {ch_num} - {raw_title}"
    return final_header, cleaned_body

def parse_chapter_number(url, raw_title):
    # Priority 1: Search in Title (e.g., "Chapter 1")
    match = re.search(r'(?:ch|chapter|c)\.?\s*(\d+)', raw_title, re.IGNORECASE)
    if match: return int(match.group(1))
    
    # Priority 2: Search in URL slug (e.g., "...chapter-1/")
    match_url = re.search(r'chapter-(\d+)', url, re.IGNORECASE)
    if match_url: return int(match_url.group(1))
        
    return 0

def find_next_url(soup):
    """
    Finds the 'Next' chapter link using priority logic.
    """
    # Priority 1: rel="next"
    next_link = soup.find('a', rel='next')
    if next_link and next_link.get('href'):
        return next_link['href']
    
    # Priority 2: Text based search in navigation containers
    nav_containers = soup.select('.naveps, .navimedia, .nvs, .pager')
    for container in nav_containers:
        links = container.find_all('a', href=True)
        for a in links:
            if 'next' in a.get_text(strip=True).lower():
                return a['href']
                
    # Fallback: Any link with "Next" text that isn't a comment link
    for a in soup.find_all('a', href=True):
        text = a.get_text(strip=True).lower()
        if text == 'next' or text == 'next chapter' or text == 'next >':
            # Basic sanity check: avoid homepage or index links
            if 'chapter' in a['href'] or 'god-slaughtering-star' in a['href']:
                return a['href']
                
    return None

def scrape_and_save_chapters(start_url, save_directory="Scraped_Novel"):
    # Use Env Var if available
    save_directory = os.getenv("PROJECT_RAW_TEXT_DIR", save_directory)

    if not os.path.exists(save_directory):
        os.makedirs(save_directory)

    json_path = os.path.join(save_directory, "chapters.json")
    session = requests.Session()
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
    }

    # --- LOAD HISTORY ---
    url_history_map = {}
    if os.path.exists(json_path):
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                history_data = json.load(f)
                for entry in history_data:
                    url_history_map[entry['url']] = entry.get('next_url')
        except: pass

    current_url = start_url

    try:
        while current_url:
            # Prevent endless loops/history skipping
            if current_url in url_history_map and url_history_map[current_url]:
                next_link = url_history_map[current_url]
                print(f"Skipping (history): {current_url}")
                current_url = next_link
                continue

            print(f"Processing: {current_url}")
            response = get_with_retries(session, current_url, headers)
            if not response: break

            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Extract basic info
            title_el = soup.find('h1')
            raw_title = title_el.get_text(strip=True) if title_el else "Unknown"
            ch_num = parse_chapter_number(current_url, raw_title)
            
            # File logic
            filename = f"ch_{ch_num:04d}.txt"
            filepath = os.path.join(save_directory, filename)

            if os.path.exists(filepath):
                print(f"   -> Already exists locally: {filename}")
            else:
                full_header, cleaned_body = extract_and_clean_chapter_data(soup, ch_num)
                # STRICT FORMAT: Header, blank line, Body
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(f"{full_header}\n\n{cleaned_body}")
                print(f"   -> Saved: {full_header}")

            # Find Next URL
            next_url = find_next_url(soup)
            
            # Update History JSON
            history_entry = {"url": current_url, "next_url": next_url, "file": filename}
            data = []
            if os.path.exists(json_path):
                try:
                    with open(json_path, 'r', encoding='utf-8') as f: data = json.load(f)
                except: data = []
            
            if not any(d['url'] == current_url for d in data):
                data.append(history_entry)
                with open(json_path, 'w', encoding='utf-8') as f: 
                    json.dump(data, f, indent=4)

            if not next_url or next_url == current_url:
                print("No more chapters found.")
                break
                
            current_url = next_url
            time.sleep(DELAY_BETWEEN_REQUESTS)

    except Exception as e:
        print(f"Critical Error during scraping: {e}")

if __name__ == '__main__':
    # Initial URL for 'God Slaughtering Star' Chapter 1
    START_URL = "https://katreadingcafe.com/god-slaughtering-star-chapter-1-i-became-the-wife-of-the-chieftain/"
    scrape_and_save_chapters(START_URL, "God_Slaughtering_Star_Text")
--- END OF FILE: ./Novels/god_slaughtering_star/custom_scraper.py ---

--- START OF FILE: ./backup/scraper_context_fetcher.py ---
import os
import requests
import sys
import re
from urllib.parse import urlparse
try:
    import google.generativeai as genai
    from constants import GEMINI_MODEL_NAME
except ImportError:
    GEMINI_MODEL_NAME = "gemini-3-flash-preview"

def extract_code_block(response_text):
    pattern = r"```python\s*(.*?)\s*```"
    match = re.search(pattern, response_text, re.DOTALL)
    if match: return match.group(1)
    return response_text

def fetch_and_generate_scraper(target_url, project_root_dir, reference_scraper="scraper_2.py"):
    context_dir = os.path.join(project_root_dir, "Scraper_Context")
    if not os.path.exists(context_dir): os.makedirs(context_dir)

    print(f"--- 1. Fetching HTML for: {target_url} ---")
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    html_content = ""
    try:
        response = requests.get(target_url, headers=headers, timeout=15)
        response.raise_for_status()
        html_content = response.text
        with open(os.path.join(context_dir, "site_structure.html"), "w", encoding="utf-8") as f:
            f.write(html_content)
    except Exception as e:
        print(f"Error fetching URL: {e}")
        return

    print(f"--- 2. Reading Reference Scraper ---")
    reference_code = ""
    if os.path.exists(reference_scraper):
        with open(reference_scraper, "r", encoding="utf-8") as f:
            reference_code = f.read()
    else:
        print(f"Error: Reference scraper '{reference_scraper}' not found.")
        return

    print(f"--- 3. Sending to Gemini ({GEMINI_MODEL_NAME}) ---")
    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        print("Error: GEMINI_API_KEY environment variable not set.")
        return

    genai.configure(api_key=api_key)
    
    # Construct the Prompt
    prompt = f"""
    You are an expert Python web scraping developer.
    
    I need you to write a NEW Python script to scrape a specific web novel.
    
    --- REFERENCE SCRAPER (scraper_2.py) ---
    The following code is a working example. Reuse the file saving, os.getenv logic, and loop structure.
    
    {reference_code}
    
    --- TARGET WEBSITE HTML ---
    Here is the HTML source code of the first chapter. 
    Use BeautifulSoup to parse this structure. 
    
    {html_content[:55000]}
    
    --- CRITICAL INSTRUCTIONS ---
    1. **CLEAN CONTENT:** The text saved to the .txt file MUST ONLY contain the Chapter Header and the Story Body.
       - **Remove** "Previous/Next" text, "Read at..." watermarks, and social media buttons from the body.
    
    2. **DEDUPLICATION:** - Check if the first line of the body content matches the Chapter Title.
       - **If it matches, remove it** from the body content to avoid duplication in the output file.
    
    3. **STRICT FORMAT:** `f.write(f"{{full_header}}\\n\\n{{cleaned_body}}")`
    
    4. **NEXT CHAPTER LOGIC (Crucial):**
       - **Priority 1:** Look for `<a href="..." rel="next">`. This is the most reliable method.
       - **Priority 2:** Look for an `<a>` tag inside a "nav" or "pager" div that contains the text "Next".
       - Ensure the loop breaks cleanly if no next link is found.

    5. Output ONLY the complete, runnable Python code.
    """

    try:
        model = genai.GenerativeModel(GEMINI_MODEL_NAME)
        response = model.generate_content(prompt)
        
        generated_code = extract_code_block(response.text)
        
        output_scraper_path = os.path.join(project_root_dir, "custom_scraper.py")
        with open(output_scraper_path, "w", encoding="utf-8") as f:
            f.write(generated_code)
            
        print(f"--- SUCCESS! ---")
        print(f"New scraper saved to: {output_scraper_path}")
        
    except Exception as e:
        print(f"Gemini API Error: {e}")

if __name__ == "__main__":
    if len(sys.argv) > 2:
        fetch_and_generate_scraper(sys.argv[1], sys.argv[2])
    else:
        print("Usage: python scraper_context_fetcher.py <url> <project_dir>")

--- END OF FILE: ./backup/scraper_context_fetcher.py ---
