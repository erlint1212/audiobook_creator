Project Directory Structure:
============================
./
    requirements.txt
    README.md
    scraper_context_fetcher.py
    qwen_tts_generator.py
    scraper_2.py
    tag_audiobook_files_opus_3.py
    grok_transelate.py
    gemini_transelate_4.py
    environment.yml
    metadata_fetcher.py
    alltalk_tts_generator_chunky_17.py
    constants.py
    test_env.py
    convert_audio_to_opus_3.py
    txt_to_epub.py
    fetch_metadata.py
    context_builder.py
    .gitignore
    shell.nix
    pipe_system_gui.py


File Contents:
==============

--- START OF FILE: ./scraper_context_fetcher.py ---
import os
import requests
import sys
import re
from urllib.parse import urlparse
try:
    import google.generativeai as genai
    from constants import GEMINI_MODEL_NAME
except ImportError:
    GEMINI_MODEL_NAME = "gemini-3-flash-preview"

def extract_code_block(response_text):
    pattern = r"```python\s*(.*?)\s*```"
    match = re.search(pattern, response_text, re.DOTALL)
    if match: return match.group(1)
    return response_text

def fetch_and_generate_scraper(target_url, project_root_dir, reference_scraper="scraper_2.py"):
    context_dir = os.path.join(project_root_dir, "Scraper_Context")
    if not os.path.exists(context_dir): os.makedirs(context_dir)

    print(f"--- 1. Fetching HTML for: {target_url} ---")
    headers = {'User-Agent': 'Mozilla/5.0'}
    
    html_content = ""
    try:
        response = requests.get(target_url, headers=headers, timeout=15)
        response.raise_for_status()
        html_content = response.text
        with open(os.path.join(context_dir, "site_structure.html"), "w", encoding="utf-8") as f:
            f.write(html_content)
    except Exception as e:
        print(f"Error fetching URL: {e}")
        return

    print(f"--- 2. Reading Reference Scraper ---")
    reference_code = ""
    if os.path.exists(reference_scraper):
        with open(reference_scraper, "r", encoding="utf-8") as f:
            reference_code = f.read()
    else:
        print(f"Error: Reference scraper '{reference_scraper}' not found.")
        return

    print(f"--- 3. Sending to Gemini ({GEMINI_MODEL_NAME}) ---")
    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        print("Error: GEMINI_API_KEY environment variable not set.")
        return

    genai.configure(api_key=api_key)
    
    # Construct the Prompt
    prompt = f"""
    You are an expert Python web scraping developer.
    
    I need you to write a NEW Python script to scrape a specific web novel.
    
    --- REFERENCE SCRAPER (scraper_2.py) ---
    The following code is a working example. Reuse the file saving, os.getenv logic, and loop structure.
    
    {reference_code}
    
    --- TARGET WEBSITE HTML ---
    Here is the HTML source code of the first chapter. 
    Use BeautifulSoup to parse this structure. 
    
    {html_content[:55000]}
    
    --- CRITICAL INSTRUCTIONS ---
    1. **CLEAN CONTENT:** The text saved to the .txt file MUST ONLY contain the Chapter Header and the Story Body.
       - **Remove** "Previous/Next" text, "Read at..." watermarks, and social media buttons from the body.
    
    2. **DEDUPLICATION:** - Check if the first line of the body content matches the Chapter Title.
       - **If it matches, remove it** from the body content to avoid duplication in the output file.
    
    3. **STRICT FORMAT:** `f.write(f"{{full_header}}\\n\\n{{cleaned_body}}")`
    
    4. **NEXT CHAPTER LOGIC (Crucial):**
       - **Priority 1:** Look for `<a href="..." rel="next">`. This is the most reliable method.
       - **Priority 2:** Look for an `<a>` tag inside a "nav" or "pager" div that contains the text "Next".
       - Ensure the loop breaks cleanly if no next link is found.

    5. Output ONLY the complete, runnable Python code.
    """

    try:
        model = genai.GenerativeModel(GEMINI_MODEL_NAME)
        response = model.generate_content(prompt)
        
        generated_code = extract_code_block(response.text)
        
        output_scraper_path = os.path.join(project_root_dir, "custom_scraper.py")
        with open(output_scraper_path, "w", encoding="utf-8") as f:
            f.write(generated_code)
            
        print(f"--- SUCCESS! ---")
        print(f"New scraper saved to: {output_scraper_path}")
        
    except Exception as e:
        print(f"Gemini API Error: {e}")

if __name__ == "__main__":
    if len(sys.argv) > 2:
        fetch_and_generate_scraper(sys.argv[1], sys.argv[2])
    else:
        print("Usage: python scraper_context_fetcher.py <url> <project_dir>")

--- END OF FILE: ./scraper_context_fetcher.py ---

--- START OF FILE: ./qwen_tts_generator.py ---
import os
import glob
import time
import math
import re
import shutil
import sys
import argparse
import torch
import soundfile as sf

# --- 1. WINDOWS UNICODE FIX ---
if sys.platform == "win32":
    sys.stdout.reconfigure(encoding='utf-8')

import nltk
from nltk.tokenize import sent_tokenize

# --- Import Qwen3-TTS ---
from qwen_tts import Qwen3TTSModel

# --- NLTK Setup ---
NLTK_SETUP_SUCCESSFUL = False
try:
    nltk.sent_tokenize("This is a test.")
    NLTK_SETUP_SUCCESSFUL = True
except LookupError:
    print("Attempting to download NLTK 'punkt' resource...")
    try:
        nltk.download('punkt', quiet=False)
        nltk.sent_tokenize("This is a test.")
        print("NLTK 'punkt' is now available.")
        NLTK_SETUP_SUCCESSFUL = True
    except Exception as download_e:
        print(f"An error occurred during 'punkt' download: {download_e}")
if not NLTK_SETUP_SUCCESSFUL:
    print("\nNLTK 'punkt' setup failed. Please resolve this issue manually and re-run.")
    exit(1)

# --- Pydub Setup ---
try:
    from pydub import AudioSegment
    from pydub.exceptions import CouldntDecodeError
    PYDUB_AVAILABLE = True
except ImportError:
    print("Warning: pydub library not found. Audio concatenation will not work.")
    PYDUB_AVAILABLE = False

# --- Configuration ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

TEXT_FILES_DIR = os.getenv("PROJECT_INPUT_TEXT_DIR", os.path.join(BASE_DIR, "BlleatTL_Novels")) 
AUDIO_OUTPUT_DIR = os.getenv("PROJECT_AUDIO_WAV_DIR", os.path.join(BASE_DIR, "generated_audio_MistakenFairy"))
PROJECT_ROOT_DIR = os.path.dirname(os.path.abspath(AUDIO_OUTPUT_DIR))

TEMP_CHUNK_DIR = os.path.join(PROJECT_ROOT_DIR, "temp_audio_chunks")
LOG_FILE = os.path.join(PROJECT_ROOT_DIR, "failed_chunks.log")

CHAPTER_START = 0
CHAPTER_STOP = 0

FALLBACK_TOKEN_LIMIT = 170 
AVG_CHARS_PER_TOKEN = 1.9
FALLBACK_CHAR_LIMIT = FALLBACK_TOKEN_LIMIT * AVG_CHARS_PER_TOKEN

QWEN_SPEAKER_WAV = None
QWEN_REF_TEXT = None  # Qwen needs the transcript of the reference audio
OUTPUT_FORMAT = "wav"

# Global Model Variable
qwen_model = None
voice_clone_prompt = None # Cached voice prompt

def _estimate_tokens(text, avg_chars_per_token=AVG_CHARS_PER_TOKEN):
    if not text: return 0
    return math.ceil(len(text) / max(1.0, avg_chars_per_token))

def normalize_text(text):
    replacements = {
        '“': '"',  '”': '"',  '‘': "'",  '’': "'",
        '…': '...', '—': '-',   '–': '-',
    }
    for old, new in replacements.items():
        text = text.replace(old, new)
    return text

def _split_by_force_chars(text_content, char_limit):
    if len(text_content) <= char_limit:
        return [text_content]
    chunks = []
    current_chunk_start = 0
    while current_chunk_start < len(text_content):
        end_index = min(current_chunk_start + int(char_limit), len(text_content))
        if end_index < len(text_content):
            space_index = text_content.rfind(' ', current_chunk_start, end_index)
            if space_index != -1 and space_index > current_chunk_start:
                end_index = space_index
        chunk = text_content[current_chunk_start:end_index].strip()
        if chunk:
            chunks.append(chunk)
        current_chunk_start = end_index + 1 
        while current_chunk_start < len(text_content) and text_content[current_chunk_start] == ' ':
            current_chunk_start += 1
    return chunks

def _split_by_sentence_groups(text_content, token_limit, avg_chars_token_est):
    final_tts_chunks = []
    char_limit = token_limit * avg_chars_token_est
    try:
        sentences = sent_tokenize(text_content)
    except Exception as e:
        print(f"      [!] NLTK sent_tokenize failed: {e}. Falling back to Lvl 3.")
        return _split_by_force_chars(text_content, char_limit)

    if not sentences: return []
    current_chunk_sentences_list = []
    current_chunk_tokens = 0
    
    for sentence_text in sentences:
        sentence_text = sentence_text.strip()
        if not sentence_text: continue
        estimated_sentence_tokens = _estimate_tokens(sentence_text, avg_chars_token_est)

        if estimated_sentence_tokens > token_limit:
            if current_chunk_sentences_list:
                final_tts_chunks.append(" ".join(current_chunk_sentences_list))
                current_chunk_sentences_list = []
                current_chunk_tokens = 0
            final_tts_chunks.extend(_split_by_force_chars(sentence_text, char_limit))
        elif current_chunk_tokens + estimated_sentence_tokens <= token_limit:
            current_chunk_sentences_list.append(sentence_text)
            current_chunk_tokens += estimated_sentence_tokens
        else:
            if current_chunk_sentences_list:
                final_tts_chunks.append(" ".join(current_chunk_sentences_list))
            current_chunk_sentences_list = [sentence_text]
            current_chunk_tokens = estimated_sentence_tokens

    if current_chunk_sentences_list:
        final_tts_chunks.append(" ".join(current_chunk_sentences_list))
    return [chunk for chunk in final_tts_chunks if chunk and chunk.strip()]

def _split_by_line_groups(text_content, token_limit, avg_chars_token_est):
    final_tts_chunks = []
    if not text_content or not text_content.strip():
        return final_tts_chunks

    lines = [line.strip() for line in text_content.split('\n') if line.strip()]
    if not lines: return []

    current_chunk_lines_list = []
    current_chunk_tokens = 0
    
    for line_text in lines:
        estimated_line_tokens = _estimate_tokens(line_text, avg_chars_token_est)
        if estimated_line_tokens > token_limit:
            if current_chunk_lines_list:
                final_tts_chunks.append("\n".join(current_chunk_lines_list))
                current_chunk_lines_list = []
                current_chunk_tokens = 0
            final_tts_chunks.extend(_split_by_sentence_groups(line_text, token_limit, avg_chars_token_est))
        elif current_chunk_tokens + estimated_line_tokens <= token_limit:
            current_chunk_lines_list.append(line_text)
            current_chunk_tokens += estimated_line_tokens
        else:
            if current_chunk_lines_list:
                final_tts_chunks.append("\n".join(current_chunk_lines_list))
            current_chunk_lines_list = [line_text]
            current_chunk_tokens = estimated_line_tokens

    if current_chunk_lines_list:
        final_tts_chunks.append("\n".join(current_chunk_lines_list))
    return [chunk for chunk in final_tts_chunks if chunk and chunk.strip()]

def concatenate_audio_chunks(chunk_filepaths, final_output_path):
    if not PYDUB_AVAILABLE: return False
    if not chunk_filepaths: return False
    print(f"  Concatenating {len(chunk_filepaths)} chunks...")
    combined = AudioSegment.empty()
    for filepath in sorted(chunk_filepaths): 
        try:
            combined += AudioSegment.from_wav(filepath) 
        except CouldntDecodeError:
            print(f"      Error: Corrupt chunk {filepath}. Skipping.")
    
    if len(combined) > 0:
        combined.export(final_output_path, format=OUTPUT_FORMAT) 
        print(f"  Saved to: {final_output_path}")
        return True
    return False

def process_chapter_file(text_filepath, final_audio_output_path):
    print(f"\n--- Processing: {os.path.basename(text_filepath)} ---")
    base_filename_no_ext = os.path.splitext(os.path.basename(text_filepath))[0]
    sanitized_base = re.sub(r'[^\w_.-]', '_', base_filename_no_ext)
    
    chapter_temp_dir = os.path.join(TEMP_CHUNK_DIR, sanitized_base) 
    os.makedirs(chapter_temp_dir, exist_ok=True)

    try:
        with open(text_filepath, 'r', encoding='utf-8') as f:
            full_text_content = f.read()
        full_text_content = normalize_text(full_text_content)
        if not full_text_content.strip():
            print(f"  Skipping empty file.")
            return True
    except Exception as e:
        print(f"  Error reading file: {e}")
        return False

    initial_text_chunks = _split_by_line_groups(full_text_content, FALLBACK_TOKEN_LIMIT, AVG_CHARS_PER_TOKEN)
    if not initial_text_chunks:
        print(f"  Warning: No text chunks generated.")
        return False

    pending_jobs = []
    for i, text_content in enumerate(initial_text_chunks):
        pending_jobs.append({
            "text": text_content,
            "output_suffix": f"l_{i+1:03d}", 
            "fallback_level": 1
        })

    generated_audio_files = [] 
    any_chunk_failed_or_skipped = False 
    job_idx = 0
    
    while job_idx < len(pending_jobs):
        current_job = pending_jobs[job_idx]
        text_to_process = current_job["text"]
        output_suffix = current_job["output_suffix"]
        fallback_level = current_job.get("fallback_level", 1)
        
        chunk_output_basename = f"{sanitized_base}_{output_suffix}"
        local_chunk_filepath = os.path.join(chapter_temp_dir, f"{chunk_output_basename}.{OUTPUT_FORMAT}")

        # Skip if already generated
        if os.path.exists(local_chunk_filepath) and os.path.getsize(local_chunk_filepath) > 100:
            generated_audio_files.append(local_chunk_filepath)
            job_idx += 1
            continue

        try:
            # --- QWEN LOCAL INFERENCE ---
            print(f"      Generating chunk {output_suffix}...")
            wavs, sr = qwen_model.generate_voice_clone(
                text=text_to_process,
                language="English",
                voice_clone_prompt=voice_clone_prompt,
            )
            
            # Save the raw numpy array to WAV
            sf.write(local_chunk_filepath, wavs[0], sr)
            
            if os.path.exists(local_chunk_filepath):
                generated_audio_files.append(local_chunk_filepath)
            
            job_idx += 1 

        except Exception as e:
            print(f"      [!!] Error during Qwen generation: {e}")
            
            new_sub_jobs = []
            if fallback_level == 1:
                print(f"      -> Falling back to Lvl 2 (Sentence Split)")
                chunks = _split_by_sentence_groups(text_to_process, FALLBACK_TOKEN_LIMIT, AVG_CHARS_PER_TOKEN)
                for i, c in enumerate(chunks):
                    new_sub_jobs.append({"text": c, "output_suffix": f"{output_suffix}_s_{i+1:02d}", "fallback_level": 2})
            elif fallback_level == 2:
                print(f"      -> Falling back to Lvl 3 (Force Split)")
                chunks = _split_by_force_chars(text_to_process, FALLBACK_CHAR_LIMIT)
                for i, c in enumerate(chunks):
                    new_sub_jobs.append({"text": c, "output_suffix": f"{output_suffix}_f_{i+1:02d}", "fallback_level": 3})

            if new_sub_jobs:
                pending_jobs = pending_jobs[:job_idx] + new_sub_jobs + pending_jobs[job_idx+1:]
                continue 
            else:
                print(f"      [Fail] Could not recover. Skipping chunk.")
                any_chunk_failed_or_skipped = True
                with open(LOG_FILE, "a", encoding="utf-8") as log_f:
                    log_f.write(f"FAILED: {output_suffix}\nText: {text_to_process}\n\n")
                job_idx += 1

    if not generated_audio_files: return False
    
    if concatenate_audio_chunks(generated_audio_files, final_audio_output_path):
        if not any_chunk_failed_or_skipped:
            try: shutil.rmtree(chapter_temp_dir)
            except: pass
        return True
    return False

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Qwen3-TTS Chapter Generator")
    parser.add_argument("--voice_wav", type=str, required=True, help="Path to your reference WAV file")
    parser.add_argument("--ref_text", type=str, required=True, help="The exact spoken text in the reference WAV")
    args = parser.parse_args()

    QWEN_SPEAKER_WAV = args.voice_wav
    QWEN_REF_TEXT = args.ref_text

    if not os.path.exists(QWEN_SPEAKER_WAV):
        print(f"Error: WAV file not found at {QWEN_SPEAKER_WAV}")
        sys.exit(1)

    # --- INITIALIZE QWEN MODEL ONCE ---
    print("Loading Qwen3-TTS Model into GPU memory...")
    qwen_model = Qwen3TTSModel.from_pretrained(
        "Qwen/Qwen3-TTS-12Hz-1.7B-Base",
        device_map="cuda:0",
        dtype=torch.bfloat16,
        attn_implementation="flash_attention_2",
    )

    print("Analyzing reference audio to create voice profile...")
    # Cache the prompt so we don't have to re-analyze the WAV for every single chunk
    voice_clone_prompt = qwen_model.create_voice_clone_prompt(
        ref_audio=QWEN_SPEAKER_WAV,
        ref_text=QWEN_REF_TEXT,
    )

    # --- SETUP DIRECTORIES ---
    if not os.path.exists(TEMP_CHUNK_DIR): os.makedirs(TEMP_CHUNK_DIR)
    if not os.path.exists(AUDIO_OUTPUT_DIR): os.makedirs(AUDIO_OUTPUT_DIR)
    
    text_files = sorted(glob.glob(os.path.join(TEXT_FILES_DIR, "*.txt")))
    if not text_files:
        print(f"No .txt files found in {TEXT_FILES_DIR}"); exit(1)

    print(f"Found {len(text_files)} chapters.")
    
    chapters_succeeded = 0
    chapters_failed = 0

    try: 
        for idx, text_file_path in enumerate(text_files):
            if CHAPTER_STOP > 0 and (idx + 1) > CHAPTER_STOP: break
            base_name = os.path.splitext(os.path.basename(text_file_path))[0]
            clean_name = re.sub(r'[^\w_.-]', '_', base_name)
            out_path = os.path.join(AUDIO_OUTPUT_DIR, f"{clean_name}.{OUTPUT_FORMAT}")

            if os.path.exists(out_path) and os.path.getsize(out_path) > 1024:
                print(f"Skipping {clean_name} (Exists)")
                chapters_succeeded += 1
                continue
            
            if process_chapter_file(text_file_path, out_path):
                chapters_succeeded += 1
            else:
                chapters_failed += 1
            
    except KeyboardInterrupt: 
        print("\n[STOPPED] User interrupted.")

--- END OF FILE: ./qwen_tts_generator.py ---

--- START OF FILE: ./scraper_2.py ---
import os
import re
import json
import time
import requests
from bs4 import BeautifulSoup

# --- CONFIGURATION ---
DELAY_BETWEEN_REQUESTS = 1.0  # Seconds
# ---------------------

def get_with_retries(session, url, headers, retries=3):
    for i in range(retries):
        try:
            response = session.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            print(f"   [!] Error fetching {url}: {e}. Retrying ({i+1}/{retries})...")
            time.sleep(2 * (i + 1))
    return None

def extract_and_clean_chapter_data(content_el, ch_num):
    """
    Targets the main content area, removes scripts, styles, and unwanted 
    interactive elements like glossaries or ads.
    """
    if not content_el:
        return f"Chapter {ch_num}", ""

    # 1. Remove script, style, and known junk classes
    for junk in content_el.find_all(['script', 'style', 'div', 'section', 'button'], 
                                   class_=['paragraph-tools', 'chapter__actions', 'social-share', 'sharedaddy', 'navigation']):
        junk.decompose()
        
    # 2. Remove Glossary Tooltips (common in translation sites)
    for tooltip in content_el.find_all(class_='dg-tooltip-box'):
        tooltip.decompose()

    # 3. Get text content
    cleaned_body = content_el.get_text(separator='\n\n', strip=True)
    
    # --- LOGIC: TITLE EXTRACTION & DEDUPLICATION ---
    story_title = f"Chapter {ch_num}"
    
    # Check if the first paragraph is actually the title
    lines = cleaned_body.split('\n')
    # Filter out empty lines from the start
    while lines and not lines[0].strip():
        lines.pop(0)
        
    if lines:
        first_line = lines[0].strip()
        # Heuristic: If first line contains "Chapter" and the number, OR is very short < 100 chars, treat as title
        if (f"Chapter {ch_num}" in first_line) or (len(first_line) < 100):
            story_title = first_line
            # CRITICAL: Remove this line from body so it doesn't duplicate in the .txt file
            cleaned_body = "\n".join(lines[1:]).strip()
            
    final_header = f"Chapter {ch_num} - {story_title}"
    return final_header, cleaned_body

def parse_chapter_number(url, raw_title):
    # Try from Title first
    match = re.search(r'(?:ch|chapter|c)\.?\s*(\d+)', raw_title, re.IGNORECASE)
    if match: return int(match.group(1))
    
    # Try from URL slug
    match_url = re.search(r'-(\d+)/?$', url)
    if match_url: return int(match_url.group(1))
        
    return 0

def scrape_and_save_chapters(start_url, save_directory="BlleatTL_Novels"):
    # Use Env Var if available (from GUI)
    save_directory = os.getenv("PROJECT_RAW_TEXT_DIR", save_directory)

    if not os.path.exists(save_directory):
        os.makedirs(save_directory)

    json_path = os.path.join(save_directory, "chapters.json")
    session = requests.Session()
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
    }

    # --- LOAD HISTORY ---
    url_history_map = {}
    if os.path.exists(json_path):
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                history_data = json.load(f)
                for entry in history_data:
                    url_history_map[entry['url']] = entry.get('next_url')
        except: pass

    current_url = start_url

    try:
        while current_url:
            if current_url in url_history_map and url_history_map[current_url]:
                next_link = url_history_map[current_url]
                print(f"Skipping (history): {current_url}")
                current_url = next_link
                continue

            print(f"Processing: {current_url}")
            response = get_with_retries(session, current_url, headers)
            if not response: break

            soup = BeautifulSoup(response.content, 'html.parser')
            
            # --- GENERIC FALLBACK SELECTORS ---
            # The AI Generator will replace these with specific ones, 
            # but the logic structure below remains.
            title_el = soup.find('h1')
            content_el = soup.select_one('.entry-content') or soup.find('article')
            
            # Find Next Link
            next_el = None
            for a in soup.find_all('a', href=True):
                if 'next' in a.get_text(strip=True).lower():
                    next_el = a
                    break

            if not content_el:
                print("Content not found.")
                break

            raw_title = title_el.get_text(strip=True) if title_el else "Unknown"
            ch_num = parse_chapter_number(current_url, raw_title)
            filename = f"ch_{ch_num:04d}.txt"
            filepath = os.path.join(save_directory, filename)

            if os.path.exists(filepath):
                print(f"   -> Exists: {filename}")
            else:
                full_header, cleaned_body = extract_and_clean_chapter_data(content_el, ch_num)
                # STRICT FORMAT: Header, blank line, Body
                with open(filepath, 'w', encoding='utf-8') as f:
                    f.write(f"{full_header}\n\n{cleaned_body}")
                print(f"   -> Saved: {full_header}")

            next_url = next_el['href'] if next_el else None
            
            # Save History
            history_entry = {"url": current_url, "next_url": next_url, "file": filename}
            data = []
            if os.path.exists(json_path):
                with open(json_path, 'r') as f: data = json.load(f)
            
            if not any(d['url'] == current_url for d in data):
                data.append(history_entry)
                with open(json_path, 'w') as f: json.dump(data, f, indent=4)

            if not next_url: break
            current_url = next_url
            time.sleep(DELAY_BETWEEN_REQUESTS)

    except Exception as e:
        print(f"Critical Error: {e}")

if __name__ == '__main__':
    # Default behavior if run directly
    pass

--- END OF FILE: ./scraper_2.py ---

--- START OF FILE: ./tag_audiobook_files_opus_3.py ---
import os
import glob
import re
import json
import mimetypes
import base64
import sys

# --- 1. WINDOWS UNICODE FIX ---
if sys.platform == "win32":
    sys.stdout.reconfigure(encoding='utf-8')

# --- Try importing mutagen ---
try:
    import mutagen
    from mutagen.oggopus import OggOpus
    from mutagen.flac import Picture
    MUTAGEN_AVAILABLE = True
except ImportError:
    print("Error: mutagen library not found. Please install it: pip install mutagen")
    MUTAGEN_AVAILABLE = False

# --- Configuration & Paths ---
# 1. Inputs from GUI (Environment Variables)
AUDIO_DIR = os.getenv("OPUS_OUTPUT_DIR")
TEXT_DIR = os.getenv("PROJECT_INPUT_TEXT_DIR")

# Fallbacks for standalone testing
if not AUDIO_DIR: 
    print("Warning: Running standalone. Using default paths.")
    AUDIO_DIR = "generated_audio_MistakenFairy_opus"
    TEXT_DIR = "BlleatTL_Novels"

# 2. Determine Project Root (One level up from text/audio dirs)
# Structure: /Novels/Title/01_Raw_Text  -> Root is /Novels/Title
if TEXT_DIR and os.path.exists(TEXT_DIR):
    PROJECT_ROOT = os.path.dirname(os.path.abspath(TEXT_DIR))
else:
    PROJECT_ROOT = os.getcwd()

METADATA_JSON = os.path.join(PROJECT_ROOT, "metadata.json")
COVER_ART_PATH = os.path.join(PROJECT_ROOT, "cover.jpg")

# 3. Default Metadata (Overwritten if metadata.json exists)
ALBUM_META = {
    "title": "Unknown Series",
    "author": "Unknown Author",
    "year": "2026",
    "genre": "Audiobook"
}

# --- Helper Functions ---

def load_global_metadata():
    """Loads the Series Title and Author from the project's metadata.json"""
    if os.path.exists(METADATA_JSON):
        try:
            with open(METADATA_JSON, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if data.get("title"): ALBUM_META["title"] = data["title"]
                if data.get("author"): ALBUM_META["author"] = data["author"]
            print(f"Loaded Global Metadata: {ALBUM_META['title']} by {ALBUM_META['author']}")
        except Exception as e:
            print(f"Warning: Could not read metadata.json: {e}")
    else:
        print("Warning: metadata.json not found. Using defaults.")

def get_chapter_title_from_text(track_num):
    """
    Reads the specific text file for this track number.
    The first line is assumed to be the Chapter Title (header).
    """
    if not TEXT_DIR or not os.path.exists(TEXT_DIR):
        return None

    # Try formatted name first (ch_0001.txt)
    txt_path = os.path.join(TEXT_DIR, f"ch_{track_num:04d}.txt")
    
    # Fallback to loose matching if exact file doesn't exist
    if not os.path.exists(txt_path):
        candidates = glob.glob(os.path.join(TEXT_DIR, f"*_{track_num:04d}.txt"))
        if candidates: txt_path = candidates[0]

    if os.path.exists(txt_path):
        try:
            with open(txt_path, 'r', encoding='utf-8') as f:
                first_line = f.readline().strip()
                if first_line:
                    return first_line
        except Exception:
            pass
            
    return None

def get_track_number(filename):
    # Extracts '1' from 'ch_0001.opus' or 'Mistaken_Fairy_l_001.opus'
    # Looks for the last sequence of digits
    matches = re.findall(r'(\d+)', filename)
    if matches:
        return int(matches[-1]) # Return the last number found (usually the chapter index)
    return None

def tag_audio_file(audio_path, track_num, chapter_title):
    try:
        audio = OggOpus(audio_path)
        
        # 1. Standard Tags
        audio.tags['TITLE'] = [chapter_title]
        audio.tags['ALBUM'] = [ALBUM_META['title']]
        audio.tags['ARTIST'] = [ALBUM_META['author']]
        audio.tags['ALBUMARTIST'] = ["AI Narrator"] # Or use Author again
        audio.tags['GENRE'] = [ALBUM_META['genre']]
        audio.tags['DATE'] = [ALBUM_META['year']]
        audio.tags['TRACKNUMBER'] = [str(track_num)]

        # 2. Embed Cover Art (Opus uses METADATA_BLOCK_PICTURE)
        if os.path.exists(COVER_ART_PATH):
            mime = mimetypes.guess_type(COVER_ART_PATH)[0] or 'image/jpeg'
            
            pic = Picture()
            with open(COVER_ART_PATH, 'rb') as f:
                pic.data = f.read()
            
            pic.type = 3 # Cover (front)
            pic.mime = mime
            pic.desc = 'Cover'
            
            # OggOpus requires base64 encoded picture block
            pic_data = pic.write()
            encoded_data = base64.b64encode(pic_data).decode('ascii')
            audio.tags['METADATA_BLOCK_PICTURE'] = [encoded_data]

        audio.save()
        return True
    except Exception as e:
        print(f"   Error tagging {os.path.basename(audio_path)}: {e}")
        return False

# --- Main Execution ---
if __name__ == "__main__":
    if not MUTAGEN_AVAILABLE:
        print("Mutagen is required. Install via: pip install mutagen")
        sys.exit(1)

    print(f"--- Audio Tagging ---")
    print(f"Audio Source: {AUDIO_DIR}")
    print(f"Text Source:  {TEXT_DIR}")
    
    if not AUDIO_DIR or not os.path.exists(AUDIO_DIR):
        print("Error: Audio directory not found.")
        sys.exit(1)

    load_global_metadata()
    
    # Process Opus files
    audio_files = sorted(glob.glob(os.path.join(AUDIO_DIR, "*.opus")))
    
    if not audio_files:
        print(f"No .opus files found in {AUDIO_DIR}")
        sys.exit(0)

    print(f"Found {len(audio_files)} files to tag.")
    
    success_count = 0
    for path in audio_files:
        filename = os.path.basename(path)
        track_num = get_track_number(filename)
        
        if track_num is None:
            print(f"   Skipping {filename} (Could not determine track number)")
            continue

        # 1. Get Specific Chapter Title
        title = get_chapter_title_from_text(track_num)
        if not title:
            title = f"Chapter {track_num}" # Fallback
        
        # 2. Apply Tags
        if tag_audio_file(path, track_num, title):
            print(f"   Tagged: [{track_num}] {title}")
            success_count += 1
    
    print(f"\nDone. Successfully tagged {success_count} files.")

--- END OF FILE: ./tag_audiobook_files_opus_3.py ---

--- START OF FILE: ./grok_transelate.py ---
import os
import time
import re
import json

# Ensure you have the openai package installed
# pip install openai
try:
    from openai import OpenAI, APIError, APITimeoutError, AuthenticationError, RateLimitError
except ImportError:
    print("CRITICAL ERROR: The 'openai' package is not installed. Please install it by running: pip install openai")
    exit()

# --- Configuration ---
# Updated to support the GUI Pipeline (os.getenv) with fallbacks to your original folders
INPUT_DIR = os.getenv("PROJECT_TRANS_INPUT_DIR", "SnakeFairy_CH_Qushucheng")
OUTPUT_DIR = os.getenv("PROJECT_TRANS_OUTPUT_DIR", "SnakeFairy_EN_transelated")
XAI_MODEL_NAME = "grok-4-0709"
XAI_BASE_URL = "https://api.x.ai/v1"
API_TIMEOUT_SECONDS = 300.0 
GLOSSARY_JSON_FILE = "translation_glossary.json"

# --- Glossary Helper Functions ---
def load_glossary_from_json(filepath: str) -> dict:
    """
    Loads a glossary dictionary (for characters and places) from a JSON file.
    If the file doesn't exist or is invalid, it returns a new dictionary structure.
    """
    default_glossary = {"characters": {}, "places": {}}
    if not os.path.exists(filepath):
        print(f"Glossary JSON file not found at '{filepath}'. A new one will be created.")
        return default_glossary
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
            # Ensure both top-level keys exist
            if "characters" not in data:
                data["characters"] = {}
            if "places" not in data:
                data["places"] = {}
            return data
    except (json.JSONDecodeError, IOError) as e:
        print(f"Error reading or parsing JSON file '{filepath}': {e}. Starting fresh.")
        return default_glossary

def save_glossary_to_json(filepath: str, data: dict):
    """Saves a glossary dictionary to a JSON file with pretty printing."""
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=4, ensure_ascii=False)
        print(f"Successfully saved updated glossary to '{filepath}'.")
    except IOError as e:
        print(f"Error writing to JSON file '{filepath}': {e}")

# --- Helper Function to Reformat Title ---
def reformat_chapter_title_in_text(text_content: str) -> str:
    if not text_content or not text_content.strip():
        return text_content

    lines = text_content.split('\n', 1)
    first_line = lines[0]
    rest_of_content = lines[1] if len(lines) > 1 else ""

    match = re.match(r'^(Chapter\s*\d+)\s*[:\-–—]?\s*(.*)', first_line, re.IGNORECASE)
    if match:
        chapter_part = match.group(1).strip()
        title_part = match.group(2).strip()
        reformatted_first_line = f"{chapter_part} - {title_part}" if title_part else chapter_part
        return f"{reformatted_first_line}\n{rest_of_content}"

    numeric_match = re.match(r'^(\d+)\s+(.*)', first_line)
    if numeric_match:
        try:
            chapter_number_int = int(numeric_match.group(1))
            title_part = numeric_match.group(2).strip()
            reformatted_first_line = f"Chapter {chapter_number_int} - {title_part}"
            return f"{reformatted_first_line}\n{rest_of_content}"
        except ValueError:
            pass
    return text_content

# --- MODIFIED: xAI API Translation Function with Context Optimization ---
def translate_text_with_xai(text_to_translate: str, known_glossary_data: dict, target_language: str = "English") -> (str, dict):
    api_key = os.environ.get("XAI_API_KEY")
    if not api_key:
        return "[Translation Error: 'XAI_API_KEY' environment variable not set.]", {}

    try:
        client = OpenAI(
            api_key=api_key,
            base_url=XAI_BASE_URL,
            timeout=API_TIMEOUT_SECONDS
        )
    except Exception as e:
        return f"[Translation Error: Could not initialize OpenAI client for xAI - {type(e).__name__}: {e}]", {}

    # --- OPTIMIZATION START: Dynamic Glossary Filtering ---
    # This logic matches the Gemini script: only send relevant glossary items to save context.
    filtered_glossary = {"characters": {}, "places": {}}
    
    # 1. Filter Characters
    for name_key, details in known_glossary_data.get("characters", {}).items():
        if name_key in text_to_translate:
            filtered_glossary["characters"][name_key] = details
            
    # 2. Filter Places
    for place_key, details in known_glossary_data.get("places", {}).items():
        if place_key in text_to_translate:
            filtered_glossary["places"][place_key] = details

    # 3. Minify JSON (remove whitespace)
    known_glossary_json_str = json.dumps(filtered_glossary, ensure_ascii=False, separators=(',', ':'))
    
    total_chars = len(known_glossary_data.get("characters", {}))
    relevant_chars = len(filtered_glossary["characters"])
    print(f"  Glossary Optimization: Sending {relevant_chars}/{total_chars} characters relevant to this chapter.")
    # --- OPTIMIZATION END ---

    print(f"Attempting to translate and extract glossary items from text (length: {len(text_to_translate)} chars)...")

    # --- UPDATED PROMPT: Aligned with Gemini prompt for consistency ---
    prompt = (
        f"You are an expert Chinese-to-English translator and data extractor.\n"
        f"Your task is twofold:\n"
        f"1. Translate the Chinese text into high-quality, natural-sounding {target_language}. For names and places, you MUST use the 'english_name' from the 'Relevant Glossary' below if present.\n"
        f"2. Identify new character names and place names in the text NOT already in the 'Relevant Glossary', and extract their details.\n\n"
        f"--- RELEVANT GLOSSARY (Specific to this text) ---\n"
        f"{known_glossary_json_str}\n\n"
        f"--- RESPONSE FORMATTING RULES ---\n"
        f"- Your response MUST have two parts separated by '---JSON---'.\n"
        f"- PART 1 (Translation): MUST ONLY contain the final {target_language.upper()} translation.\n"
        f"- PART 2 (Data): MUST start on a new line immediately after '---JSON---' and contain a single JSON object of NEW entities. This object should have two keys: 'characters' and 'places'.\n"
        f"- Under 'characters', provide new characters with their 'pinyin', 'english_name', and 'pronoun'.\n"
        f"- Under 'places', provide new places with their 'pinyin' and 'english_name'.\n"
        f"- Example JSON Format: {{\"characters\": {{\"兰波\": {{\"pinyin\": \"Lan Bo\", \"english_name\": \"Lan Bo\", \"pronoun\": \"he/him\"}}}}, \"places\": {{\"清河市\": {{\"pinyin\": \"Qinghe Shi\", \"english_name\": \"Qinghe City\"}}}}}}\n"
        f"- If NO NEW entities of a type are found, that key's value must be an empty object: e.g., {{\"characters\": {{}}, \"places\": {{...}}}}\n\n"
        f"--- CHINESE TEXT TO PROCESS ---\n"
        f"{text_to_translate}\n"
        f"--- END OF TEXT ---\n\n"
        f"Please provide your response following all rules."
    )

    try:
        chat_completion = client.chat.completions.create(
            messages=[
                {"role": "system", "content": "You are a helpful assistant that follows instructions precisely."},
                {"role": "user", "content": prompt}
            ],
            model=XAI_MODEL_NAME,
            temperature=0.2,
        )

        raw_response_text = chat_completion.choices[0].message.content if chat_completion.choices and chat_completion.choices[0].message else ""
        separator = "---JSON---"
        new_glossary_items = {}
        translation_part = raw_response_text

        if separator in raw_response_text:
            parts = raw_response_text.split(separator, 1)
            translation_part = parts[0].strip()
            json_part = parts[1].strip()
            
            try:
                json_part_cleaned = re.sub(r'```json\s*|\s*```', '', json_part, flags=re.DOTALL).strip()
                if json_part_cleaned:
                    new_glossary_items = json.loads(json_part_cleaned)
                    print(f"  Successfully parsed glossary data from API response.")
            except json.JSONDecodeError as e:
                print(f"  Warning: Failed to parse JSON from API response. Error: {e}")
        else:
            print("  Warning: JSON separator not found in API response.")

        final_translation = re.sub(r'\n---\s*' + target_language.upper() + r'\s*TRANSLATION\s*(END|START)\s*---|\^ENGLISH TRANSLATION ONLY:[\s\n]*', '', translation_part, flags=re.IGNORECASE).strip()
        print(f"Translation API call successful.")
        return final_translation, new_glossary_items

    except (APIError, APITimeoutError, AuthenticationError, RateLimitError) as e:
        error_type = type(e).__name__
        print(f"An API error occurred during translation: {error_type} - {e}")
        return f"[Translation Error ({XAI_MODEL_NAME} - {error_type})]", {}
    except Exception as e:
        error_type = type(e).__name__
        print(f"An unexpected error occurred during translation: {error_type} - {e}")
        return f"[Translation Error ({XAI_MODEL_NAME} - {error_type})]", {}

# --- MODIFIED: Main Processing Logic with Glossary ---
def process_files_for_translation():
    script_dir = os.path.dirname(os.path.abspath(__file__))
    # Using the global variables configured via os.getenv above
    input_dir_full_path = os.path.join(script_dir, INPUT_DIR)
    output_dir_full_path = os.path.join(script_dir, OUTPUT_DIR)
    glossary_json_full_path = os.path.join(script_dir, GLOSSARY_JSON_FILE)
    glossary_data = load_glossary_from_json(glossary_json_full_path)

    if not os.path.exists(input_dir_full_path):
        print(f"Error: Input directory '{input_dir_full_path}' not found."); return
    if not os.path.exists(output_dir_full_path):
        os.makedirs(output_dir_full_path)
        print(f"Created output directory: {output_dir_full_path}")

    files_to_process = sorted([f for f in os.listdir(input_dir_full_path) if f.endswith(".txt")])
    if not files_to_process:
        print(f"No .txt files found in '{input_dir_full_path}'."); return

    print(f"Found {len(files_to_process)} file(s) to process from '{input_dir_full_path}'.")

    for i, filename in enumerate(files_to_process):
        input_filepath = os.path.join(input_dir_full_path, filename)
        output_filepath = os.path.join(output_dir_full_path, filename)
        print(f"\n[{i+1}/{len(files_to_process)}] Checking: {filename}...")

        if os.path.exists(output_filepath):
            try:
                with open(output_filepath, 'r', encoding='utf-8') as f_check:
                    content_check = f_check.read(200)
                    if "[Translation Error" not in content_check and "[ERROR PROCESSING FILE" not in content_check:
                          print(f"Output file '{output_filepath}' exists and is valid. Skipping."); continue
                    else: print(f"Output file contains an error marker. Will re-translate.")
            except Exception: pass

        print(f"Processing for translation: {filename}")

        try:
            with open(input_filepath, 'r', encoding='utf-8') as f: source_content = f.read()
            clean_source_for_api = "\n".join([line for line in source_content.splitlines() if line.strip() and re.search(r'[\u4e00-\u9fff]', line)]).strip()

            if not clean_source_for_api:
                translated_content = "[No Chinese content found in source]"
            else:
                translated_content, new_glossary_items = translate_text_with_xai(clean_source_for_api, glossary_data, target_language="English")

                if new_glossary_items:
                    new_chars = new_glossary_items.get("characters", {})
                    if new_chars:
                        print(f"  Updating master list with {len(new_chars)} new character(s).")
                        for name, details in new_chars.items():
                            if name not in glossary_data["characters"]:
                                glossary_data["characters"][name] = details
                                print(f"    + Added Character: {name} -> {details}")
                    
                    new_places = new_glossary_items.get("places", {})
                    if new_places:
                        print(f"  Updating master list with {len(new_places)} new place(s).")
                        for name, details in new_places.items():
                             if name not in glossary_data["places"]:
                                glossary_data["places"][name] = details
                                print(f"    + Added Place: {name} -> {details}")
            
            final_content_to_write = translated_content if translated_content.startswith("[") else reformat_chapter_title_in_text(translated_content)
            with open(output_filepath, 'w', encoding='utf-8') as f: f.write(final_content_to_write)
            print(f"Saved: {output_filepath}")

            # --- Save glossary after each file is processed ---
            save_glossary_to_json(glossary_json_full_path, glossary_data)

            if i < len(files_to_process) - 1:
                 print(f"Pausing for 5.0 seconds..."); time.sleep(5.0)

        except Exception as e:
            print(f"FATAL Error processing file {filename}: {e}")
            with open(output_filepath, 'w', encoding='utf-8') as f_err: f_err.write(f"[ERROR PROCESSING FILE: {e}]")

    print(f"\n--- Translation Run Summary ---")
    print(f"Total source files checked: {len(files_to_process)}")
    print(f"-----------------------------")


if __name__ == "__main__":
    print(f"Starting Chinese to English translation process...")
    if not os.environ.get("XAI_API_KEY"):
        print("\nCRITICAL ERROR: 'XAI_API_KEY' environment variable not set."); exit()
        
    process_files_for_translation()
    print("Translation process finished.")

--- END OF FILE: ./grok_transelate.py ---

--- START OF FILE: ./gemini_transelate_4.py ---
import os
import time
import re
import json
import sys  # Added for sys.exit()
from google.generativeai.types import HarmCategory, HarmBlockThreshold

# Ensure you have the google-generativeai package installed
# pip install -U google-generativeai

# --- Configuration ---
INPUT_DIR = "SnakeFairy_CH_Qushucheng"
OUTPUT_DIR = "SnakeFairy_EN_transelated"
GLOSSARY_JSON_FILE = "translation_glossary.json"


# --- Helper Function to Load Glossary from JSON ---
def load_glossary_from_json(filepath: str) -> dict:
    """
    Loads a glossary dictionary (for characters and places) from a JSON file.
    If the file doesn't exist or is invalid, it returns a new dictionary structure.
    """
    default_glossary = {"characters": {}, "places": {}}
    if not os.path.exists(filepath):
        print(f"Glossary JSON file not found at '{filepath}'. A new one will be created.")
        return default_glossary
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
            # Ensure both top-level keys exist
            if "characters" not in data:
                data["characters"] = {}
            if "places" not in data:
                data["places"] = {}
            return data
    except (json.JSONDecodeError, IOError) as e:
        print(f"Error reading or parsing JSON file '{filepath}': {e}. Starting fresh.")
        return default_glossary

# --- Helper Function to Save Glossary to JSON ---
def save_glossary_to_json(filepath: str, data: dict):
    """Saves a glossary dictionary to a JSON file with pretty printing."""
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=4, ensure_ascii=False)
        print(f"Successfully saved updated glossary to '{filepath}'.")
    except IOError as e:
        print(f"Error writing to JSON file '{filepath}': {e}")


# --- Helper Function to Reformat Title (unchanged) ---
def reformat_chapter_title_in_text(text_content: str) -> str:
    if not text_content or not text_content.strip(): return text_content
    lines = text_content.split('\n', 1)
    first_line, rest_of_content = lines[0], lines[1] if len(lines) > 1 else ""
    match = re.match(r'^(Chapter\s*\d+)\s*[:\-–—]?\s*(.*)', first_line, re.IGNORECASE)
    if match:
        chapter_part, title_part = match.group(1).strip(), match.group(2).strip()
        reformatted_first_line = f"{chapter_part} - {title_part}" if title_part else chapter_part
        return f"{reformatted_first_line}\n{rest_of_content}"
    numeric_match = re.match(r'^(\d+)\s+(.*)', first_line)
    if numeric_match:
        try:
            chapter_number_int, title_part = int(numeric_match.group(1)), numeric_match.group(2).strip()
            reformatted_first_line = f"Chapter {chapter_number_int} - {title_part}"
            return f"{reformatted_first_line}\n{rest_of_content}"
        except ValueError: pass
    return text_content

# --- MODIFIED: Gemini API Translation Function for Glossary ---
def translate_text_with_gemini(text_to_translate: str, known_glossary_data: dict, target_language: str = "English") -> (str, dict):
    """
    Translates text and extracts new glossary items (characters and places).
    INCLUDES RETRY LOGIC for DeadlineExceeded and AUTO-QUIT for ResourceExhausted.
    """
    try:
        import google.generativeai as genai_sdk
        from google.api_core import exceptions as google_exceptions # Import specific exceptions
    except ImportError:
        return "[Translation Error: Package not installed.]", {}

    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key: return "[Translation Error: 'GEMINI_API_KEY' not set.]", {}

    genai_sdk.configure(api_key=api_key)
    model_name_for_api = "gemini-3-flash-preview"
    
    # --- OPTIMIZATION START: Dynamic Glossary Filtering ---
    filtered_glossary = {"characters": {}, "places": {}}
    
    # 1. Filter Characters
    for name_key, details in known_glossary_data.get("characters", {}).items():
        if name_key in text_to_translate:
            filtered_glossary["characters"][name_key] = details
            
    # 2. Filter Places
    for place_key, details in known_glossary_data.get("places", {}).items():
        if place_key in text_to_translate:
            filtered_glossary["places"][place_key] = details

    # 3. Minify JSON
    known_glossary_json_str = json.dumps(filtered_glossary, ensure_ascii=False, separators=(',', ':'))
    
    total_chars = len(known_glossary_data.get("characters", {}))
    relevant_chars = len(filtered_glossary["characters"])
    print(f"  Glossary Optimization: Sending {relevant_chars}/{total_chars} characters relevant to this chapter.")
    # --- OPTIMIZATION END ---

    print(f"Attempting to translate and extract glossary items from text (length: {len(text_to_translate)} chars)...")

    prompt = (
        f"You are an expert Chinese-to-English translator and data extractor.\n"
        f"Your task is twofold:\n"
        f"1. Translate the Chinese text into high-quality, natural-sounding {target_language}. For names and places, you MUST use the 'english_name' from the 'Relevant Glossary' below if present.\n"
        f"2. Identify new character names and place names in the text NOT already in the glossary, and extract their details.\n\n"
        f"--- RELEVANT GLOSSARY (Specific to this text) ---\n"
        f"{known_glossary_json_str}\n\n"
        f"--- RESPONSE FORMATTING RULES ---\n"
        f"- Your response MUST have two parts separated by '---JSON---'.\n"
        f"- PART 1 (Translation): MUST ONLY contain the final {target_language.upper()} translation.\n"
        f"- PART 2 (Data): MUST start on a new line immediately after '---JSON---' and contain a single JSON object of NEW entities. This object should have two keys: 'characters' and 'places'.\n"
        f"- Under 'characters', provide new characters with their 'pinyin', 'english_name', and 'pronoun'.\n"
        f"- Under 'places', provide new places with their 'pinyin' and 'english_name'.\n"
        f"- Example JSON Format: {{\"characters\": {{\"兰波\": {{\"pinyin\": \"Lan Bo\", \"english_name\": \"Lan Bo\", \"pronoun\": \"he/him\"}}}}, \"places\": {{\"清河市\": {{\"pinyin\": \"Qinghe Shi\", \"english_name\": \"Qinghe City\"}}}}}}\n"
        f"- If NO NEW entities of a type are found, that key's value must be an empty object: e.g., {{\"characters\": {{}}, \"places\": {{...}}}}\n\n"
        f"--- CHINESE TEXT TO PROCESS ---\n"
        f"{text_to_translate}\n"
        f"--- END OF TEXT ---\n\n"
        f"Please provide your response following all rules."
    )

    safety_settings = {
        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,
        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,
        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH,
        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,
    }

    # --- RETRY LOGIC START ---
    max_retries = 3
    retry_delay = 10 # seconds
    
    for attempt in range(max_retries):
        try:
            model = genai_sdk.GenerativeModel(model_name_for_api)
            # Increased timeout to 600
            response = model.generate_content(
                prompt, request_options={"timeout": 600},
                generation_config=genai_sdk.types.GenerationConfig(temperature=0.2),
                safety_settings=safety_settings
            )
            
            # If successful, break out of retry loop and process response
            raw_response_text = response.text
            break 

        except google_exceptions.ResourceExhausted:
            print(f"\nCRITICAL: Resource Exhausted (Quota limit reached).")
            print("Auto-quitting script to prevent further errors.")
            sys.exit(0) # Quit the program entirely

        except google_exceptions.DeadlineExceeded:
            print(f"  Warning: API Deadline Exceeded (Timeout). Retrying {attempt + 1}/{max_retries} in {retry_delay}s...")
            time.sleep(retry_delay)
            retry_delay *= 2 # Increase delay for next retry
            if attempt == max_retries - 1:
                return f"[Translation Error: Deadline Exceeded after {max_retries} attempts]", {}
                
        except Exception as e:
            # Handle other unexpected API errors
            error_type = type(e).__name__
            print(f"  API Error ({error_type}): {e}")
            return f"[Translation Error ({model_name_for_api} - {error_type})]", {}
    # --- RETRY LOGIC END ---

    try:
        separator = "---JSON---"
        new_glossary_items = {}
        translation_part = raw_response_text

        if separator in raw_response_text:
            parts = raw_response_text.split(separator, 1)
            translation_part = parts[0].strip()
            json_part = parts[1].strip()
            
            try:
                json_part_cleaned = re.sub(r'```json\s*|\s*```', '', json_part, flags=re.DOTALL).strip()
                if json_part_cleaned:
                    new_glossary_items = json.loads(json_part_cleaned)
                    print(f"  Successfully parsed glossary data from API response.")
            except json.JSONDecodeError as e:
                print(f"  Warning: Failed to parse JSON from API response. Error: {e}")
        else:
            print("  Warning: JSON separator not found in API response.")

        final_translation = re.sub(r'\n---\s*' + target_language.upper() + r'\s*TRANSLATION\s*(END|START)\s*---|\^ENGLISH TRANSLATION ONLY:[\s\n]*', '', translation_part, flags=re.IGNORECASE).strip()
        print(f"Translation API call successful.")
        return final_translation, new_glossary_items

    except Exception as e:
        print(f"An error occurred during response parsing: {e}")
        return f"[Translation Error (Parsing)]", {}


# --- MODIFIED: Main Processing Logic for Glossary ---
def process_files_for_translation():
    script_dir = os.path.dirname(os.path.abspath(__file__))
    input_dir_full_path = os.path.join(script_dir, INPUT_DIR)
    output_dir_full_path = os.path.join(script_dir, OUTPUT_DIR)
    glossary_json_full_path = os.path.join(script_dir, GLOSSARY_JSON_FILE)
    glossary_data = load_glossary_from_json(glossary_json_full_path)

    if not os.path.exists(input_dir_full_path):
        print(f"Error: Input directory '{input_dir_full_path}' not found."); return
    if not os.path.exists(output_dir_full_path):
        os.makedirs(output_dir_full_path)
        print(f"Created output directory: {output_dir_full_path}")

    files_to_process = sorted([f for f in os.listdir(input_dir_full_path) if f.endswith(".txt")])
    if not files_to_process:
        print(f"No .txt files found in '{input_dir_full_path}'."); return

    print(f"Found {len(files_to_process)} file(s) to process from '{input_dir_full_path}'.")

    for i, filename in enumerate(files_to_process):
        input_filepath = os.path.join(input_dir_full_path, filename)
        output_filepath = os.path.join(output_dir_full_path, filename)
        print(f"\n[{i+1}/{len(files_to_process)}] Checking: {filename}...")

        if os.path.exists(output_filepath):
            try:
                with open(output_filepath, 'r', encoding='utf-8') as f_check:
                    content_check = f_check.read(200)
                    if "[Translation Error" not in content_check and "[ERROR PROCESSING FILE" not in content_check:
                          print(f"Output file '{output_filepath}' exists and is valid. Skipping."); continue
                    else: print(f"Output file contains an error marker. Will re-translate.")
            except Exception: pass

        print(f"Processing for translation: {filename}")

        try:
            with open(input_filepath, 'r', encoding='utf-8') as f: source_content = f.read()
            clean_source_for_api = "\n".join([line for line in source_content.splitlines() if line.strip() and re.search(r'[\u4e00-\u9fff]', line)]).strip()

            if not clean_source_for_api:
                translated_content = "[No Chinese content found in source]"
            else:
                translated_content, new_glossary_items = translate_text_with_gemini(clean_source_for_api, glossary_data, target_language="English")

                if new_glossary_items:
                    new_chars = new_glossary_items.get("characters", {})
                    if new_chars:
                        print(f"  Updating master list with {len(new_chars)} new character(s).")
                        for name, details in new_chars.items():
                            if name not in glossary_data["characters"]:
                                glossary_data["characters"][name] = details
                                print(f"    + Added Character: {name} -> {details}")
                    
                    new_places = new_glossary_items.get("places", {})
                    if new_places:
                        print(f"  Updating master list with {len(new_places)} new place(s).")
                        for name, details in new_places.items():
                             if name not in glossary_data["places"]:
                                glossary_data["places"][name] = details
                                print(f"    + Added Place: {name} -> {details}")
            
            final_content_to_write = translated_content if translated_content.startswith("[") else reformat_chapter_title_in_text(translated_content)
            with open(output_filepath, 'w', encoding='utf-8') as f: f.write(final_content_to_write)
            print(f"Saved: {output_filepath}")

            # --- MODIFIED: Save glossary after each file is processed ---
            save_glossary_to_json(glossary_json_full_path, glossary_data)

            if i < len(files_to_process) - 1:
                 print(f"Pausing for 5.0 seconds..."); time.sleep(5.0)

        except Exception as e:
            print(f"FATAL Error processing file {filename}: {e}")
            with open(output_filepath, 'w', encoding='utf-8') as f_err: f_err.write(f"[ERROR PROCESSING FILE: {e}]")

    print(f"\n--- Translation Run Summary ---")
    print(f"Total source files checked: {len(files_to_process)}")
    print(f"-----------------------------")


if __name__ == "__main__":
    print(f"Starting Chinese to English translation process...")
    if not os.environ.get("GEMINI_API_KEY"):
        print("\nCRITICAL ERROR: 'GEMINI_API_KEY' environment variable not set."); exit()
        
    process_files_for_translation()
    print("Translation process finished.")

--- END OF FILE: ./gemini_transelate_4.py ---

--- START OF FILE: ./metadata_fetcher.py ---
import os
import sys
import json
import requests
import re
import shutil
import subprocess
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# Try to import constants, fallback if missing
try:
    import google.generativeai as genai
    from constants import GEMINI_MODEL_NAME
except ImportError:
    GEMINI_MODEL_NAME = "gemini-3-flash-preview"

# --- HELPER: Image Downloader ---
def download_cover(img_url, save_dir):
    if not img_url: return
    try:
        clean_url = img_url.split('?')[0] # Remove WP resize params
        save_path = os.path.join(save_dir, "cover.jpg")
        
        headers = {'User-Agent': 'Mozilla/5.0'}
        r = requests.get(img_url, headers=headers, stream=True, timeout=10)
        if r.status_code == 200:
            with open(save_path, 'wb') as f:
                r.raw.decode_content = True
                shutil.copyfileobj(r.raw, f)
            print(f"    [Cover] Saved to: {save_path}")
    except Exception as e:
        print(f"    [Error] Cover download failed: {e}")

# --- 1. DEFAULT EXTRACTION LOGIC ---
def default_metadata_extraction(html, url):
    """
    Standard scraper trying OpenGraph and common HTML tags.
    """
    soup = BeautifulSoup(html, 'html.parser')
    data = {"title": "Unknown Title", "author": "Unknown Author", "description": "", "cover_url": ""}

    # Title
    og_title = soup.find("meta", property="og:title")
    if og_title: 
        data["title"] = og_title.get("content", "").replace("– Dobytranslations", "").strip()
    else:
        h1 = soup.select_one("h1.entry-title")
        if h1: data["title"] = h1.get_text(strip=True)

    # Cover
    # Doby specific
    thumb = soup.select_one(".sertothumb img")
    if thumb:
        data["cover_url"] = thumb.get("src", "") or thumb.get("data-src", "")
    else:
        og_image = soup.find("meta", property="og:image")
        if og_image: data["cover_url"] = og_image.get("content", "")

    # Description
    # Doby specific
    desc_div = soup.select_one(".sersys.entry-content") 
    if not desc_div: desc_div = soup.select_one(".entry-content[itemprop='description']")
    
    if desc_div:
        # Cleanup Doby junk (New Free unlock...)
        for junk in desc_div.find_all(['h4', 'strong']):
            if "unlock" in junk.get_text().lower(): junk.decompose()
        data["description"] = desc_div.get_text(separator="\n", strip=True)
    else:
        og_desc = soup.find("meta", property="og:description")
        if og_desc: data["description"] = og_desc.get("content", "")

    # Author
    # Fixed the specific error you saw previously (argument conflict)
    author_meta = soup.find("meta", attrs={"name": "author"})
    if author_meta:
        data["author"] = author_meta.get("content", "")
    else:
        for label in soup.find_all(string=re.compile(r"Author", re.I)):
            parent = label.parent
            if parent:
                text = parent.get_text(strip=True).replace("Author", "").replace(":", "").strip()
                if 1 < len(text) < 50:
                    data["author"] = text
                    break
    
    return data

# --- 2. AI GENERATOR LOGIC ---
def fetch_and_generate_metadata_scraper(index_url, project_dir):
    """
    Fetches HTML -> Sends to Gemini -> Writes custom_metadata_scraper.py
    """
    context_dir = os.path.join(project_dir, "Scraper_Context")
    if not os.path.exists(context_dir): os.makedirs(context_dir)

    print(f"    [AI] Fetching HTML source to analyze...")
    try:
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(index_url, headers=headers, timeout=15)
        html_content = response.text
        # Save for reference
        with open(os.path.join(context_dir, "index_structure.html"), "w", encoding="utf-8") as f:
            f.write(html_content)
    except Exception as e:
        print(f"    [AI] Error fetching URL: {e}")
        return False

    print(f"    [AI] Asking Gemini ({GEMINI_MODEL_NAME}) to write a custom scraper...")
    
    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        print("    [Error] GEMINI_API_KEY not set.")
        return False

    genai.configure(api_key=api_key)
    
    prompt = f"""
    You are an expert Python web scraping developer.
    
    The default scraping method FAILED for this website.
    I need a robust script to extract Novel Metadata from the HTML below.
    
    --- TARGET HTML (Index Page) ---
    {html_content[:55000]} 
    
    --- INSTRUCTIONS ---
    1. Write a Python script using `BeautifulSoup`.
    2. Extract: **Title**, **Author**, **Description**, **Cover Image URL**.
    3. **CRITICAL OUTPUT**:
       - Save the data to `metadata.json` in `os.getenv('SAVE_DIR')`.
       - Download the cover image to `cover.jpg` in `os.getenv('SAVE_DIR')`.
    4. Handle `data-src` or `loading="lazy"` if present for images.
    5. Use `requests` to download the image.
    
    Output ONLY the valid Python code.
    """

    try:
        model = genai.GenerativeModel(GEMINI_MODEL_NAME)
        response = model.generate_content(prompt)
        
        # Extract code block
        code = response.text
        if "```python" in code:
            code = code.split("```python")[1].split("```")[0]
        elif "```" in code:
            code = code.split("```")[1]
        
        output_path = os.path.join(project_dir, "custom_metadata_scraper.py")
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(code)
            
        print(f"    [AI] Success! Generated: {output_path}")
        return True
        
    except Exception as e:
        print(f"    [AI] Gemini API Error: {e}")
        return False

# --- 3. MAIN CONTROLLER ---
def run_metadata_fetch(index_url, project_dir):
    print(f"--- Fetching Metadata for: {os.path.basename(project_dir)} ---")
    
    # A. Check for EXISTING custom script first
    custom_script = os.path.join(project_dir, "custom_metadata_scraper.py")
    if os.path.exists(custom_script):
        print(f"--- Found Custom Script. Executing... ---")
        run_custom_script(custom_script, index_url, project_dir)
        return

    # B. Try Default Method
    try:
        print("    [1] Trying Default Extraction...")
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(index_url, headers=headers, timeout=15)
        response.raise_for_status()
        
        data = default_metadata_extraction(response.text, index_url)
        
        # Validate critical data
        if not data['title'] or data['title'] == "Unknown Title":
            raise Exception("Default extractor failed to find a valid title.")

        # Save success
        json_path = os.path.join(project_dir, "metadata.json")
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=4)
        print(f"    [Meta] Success! Title: {data['title']}")
        
        if data["cover_url"]:
            download_cover(data["cover_url"], project_dir)

    except Exception as e:
        # C. FAILOVER: Trigger AI
        print(f"    [!] Default Method Failed: {e}")
        print(f"    [2] FAILOVER: Initializing AI Auto-Correction...")
        
        success = fetch_and_generate_metadata_scraper(index_url, project_dir)
        
        if success and os.path.exists(custom_script):
            print(f"    [3] Executing newly generated AI script...")
            run_custom_script(custom_script, index_url, project_dir)
        else:
            print("    [Error] AI Adaptation failed.")

def run_custom_script(script_path, url, save_dir):
    """Executes the custom script in a subprocess"""
    try:
        env = os.environ.copy()
        env["TARGET_URL"] = url
        env["SAVE_DIR"] = save_dir
        
        result = subprocess.run(
            [sys.executable, script_path], 
            env=env, 
            capture_output=True, 
            text=True
        )
        print(result.stdout)
        if result.returncode != 0:
            print(f"    [Script Error] {result.stderr}")
    except Exception as e:
        print(f"    [Exec Error] {e}")

if __name__ == "__main__":
    if len(sys.argv) > 3:
        u = sys.argv[1]
        d = sys.argv[2]
        mode = sys.argv[3]
        if mode == "adapt":
            fetch_and_generate_metadata_scraper(u, d)
        else:
            run_metadata_fetch(u, d)
    elif len(sys.argv) > 2:
        run_metadata_fetch(sys.argv[1], sys.argv[2])

--- END OF FILE: ./metadata_fetcher.py ---

--- START OF FILE: ./alltalk_tts_generator_chunky_17.py ---
import requests
import os
import glob
import time
import math
import re
import shutil
import traceback
import sys
import argparse
import json

# --- 1. WINDOWS UNICODE FIX ---
if sys.platform == "win32":
    sys.stdout.reconfigure(encoding='utf-8')

import nltk
from nltk.tokenize import sent_tokenize

# --- NLTK Setup ---
NLTK_SETUP_SUCCESSFUL = False
try:
    nltk.sent_tokenize("This is a test.")
    NLTK_SETUP_SUCCESSFUL = True
except LookupError:
    print("Attempting to download NLTK 'punkt' resource...")
    try:
        nltk.download('punkt', quiet=False)
        nltk.sent_tokenize("This is a test.")
        print("NLTK 'punkt' is now available.")
        NLTK_SETUP_SUCCESSFUL = True
    except Exception as download_e:
        print(f"An error occurred during 'punkt' download: {download_e}")
if not NLTK_SETUP_SUCCESSFUL:
    print("\nNLTK 'punkt' setup failed. Please resolve this issue manually and re-run.")
    exit(1)

# --- Pydub Setup ---
try:
    from pydub import AudioSegment
    from pydub.exceptions import CouldntDecodeError
    PYDUB_AVAILABLE = True
except ImportError:
    print("Warning: pydub library not found. Audio concatenation will not work.")
    PYDUB_AVAILABLE = False

# --- Configuration ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
ALLTALK_API_URL = "http://127.0.0.1:7851/api/tts-generate"
ALLTALK_BASE_URL = "http://127.0.0.1:7851"

TEXT_FILES_DIR = os.getenv("PROJECT_INPUT_TEXT_DIR", os.path.join(BASE_DIR, "BlleatTL_Novels")) 
AUDIO_OUTPUT_DIR = os.getenv("PROJECT_AUDIO_WAV_DIR", os.path.join(BASE_DIR, "generated_audio_MistakenFairy"))
PROJECT_ROOT_DIR = os.path.dirname(os.path.abspath(AUDIO_OUTPUT_DIR))

TEMP_CHUNK_DIR = os.path.join(PROJECT_ROOT_DIR, "temp_audio_chunks")
LOG_FILE = os.path.join(PROJECT_ROOT_DIR, "failed_chunks.log")

CHAPTER_START = 0
CHAPTER_STOP = 0

FALLBACK_TOKEN_LIMIT = 170 
AVG_CHARS_PER_TOKEN = 1.9
FALLBACK_CHAR_LIMIT = FALLBACK_TOKEN_LIMIT * AVG_CHARS_PER_TOKEN
MIN_BYTES_PER_CHAR = 1500 

XTTS_SPEAKER_WAV = None  
XTTS_LANGUAGE = "en"
RVC_ENABLE = False
RVC_MODEL_NAME_FOR_API = None
RVC_PITCH = -2
SPEED = 1.0
OUTPUT_FORMAT = "wav"

def _estimate_tokens(text, avg_chars_per_token=AVG_CHARS_PER_TOKEN):
    if not text: return 0
    return math.ceil(len(text) / max(1.0, avg_chars_per_token))

def normalize_text(text):
    replacements = {
        '“': '"',  '”': '"',  '‘': "'",  '’': "'",
        '…': '...', '—': '-',   '–': '-',
    }
    for old, new in replacements.items():
        text = text.replace(old, new)
    return text

def _split_by_force_chars(text_content, char_limit):
    if len(text_content) <= char_limit:
        return [text_content]
    chunks = []
    current_chunk_start = 0
    while current_chunk_start < len(text_content):
        end_index = min(current_chunk_start + int(char_limit), len(text_content))
        if end_index < len(text_content):
            space_index = text_content.rfind(' ', current_chunk_start, end_index)
            if space_index != -1 and space_index > current_chunk_start:
                end_index = space_index
        chunk = text_content[current_chunk_start:end_index].strip()
        if chunk:
            chunks.append(chunk)
        current_chunk_start = end_index + 1 
        while current_chunk_start < len(text_content) and text_content[current_chunk_start] == ' ':
            current_chunk_start += 1
    return chunks

def _split_by_sentence_groups(text_content, token_limit, avg_chars_token_est):
    final_tts_chunks = []
    char_limit = token_limit * avg_chars_token_est
    try:
        sentences = sent_tokenize(text_content)
    except Exception as e:
        print(f"      [!] NLTK sent_tokenize failed: {e}. Falling back to Lvl 3.")
        return _split_by_force_chars(text_content, char_limit)

    if not sentences: return []
    current_chunk_sentences_list = []
    current_chunk_tokens = 0
    
    for sentence_text in sentences:
        sentence_text = sentence_text.strip()
        if not sentence_text: continue
        estimated_sentence_tokens = _estimate_tokens(sentence_text, avg_chars_token_est)

        if estimated_sentence_tokens > token_limit:
            if current_chunk_sentences_list:
                final_tts_chunks.append(" ".join(current_chunk_sentences_list))
                current_chunk_sentences_list = []
                current_chunk_tokens = 0
            print(f"      [Lvl 2] Sentence too long ({len(sentence_text)} chars). Passing to Lvl 3.")
            final_tts_chunks.extend(_split_by_force_chars(sentence_text, char_limit))
        elif current_chunk_tokens + estimated_sentence_tokens <= token_limit:
            current_chunk_sentences_list.append(sentence_text)
            current_chunk_tokens += estimated_sentence_tokens
        else:
            if current_chunk_sentences_list:
                final_tts_chunks.append(" ".join(current_chunk_sentences_list))
            current_chunk_sentences_list = [sentence_text]
            current_chunk_tokens = estimated_sentence_tokens

    if current_chunk_sentences_list:
        final_tts_chunks.append(" ".join(current_chunk_sentences_list))
    return [chunk for chunk in final_tts_chunks if chunk and chunk.strip()]

def _split_by_line_groups(text_content, token_limit, avg_chars_token_est):
    final_tts_chunks = []
    if not text_content or not text_content.strip():
        return final_tts_chunks

    lines = [line.strip() for line in text_content.split('\n') if line.strip()]
    if not lines: return []

    current_chunk_lines_list = []
    current_chunk_tokens = 0
    
    for line_text in lines:
        estimated_line_tokens = _estimate_tokens(line_text, avg_chars_token_est)
        if estimated_line_tokens > token_limit:
            if current_chunk_lines_list:
                final_tts_chunks.append("\n".join(current_chunk_lines_list))
                current_chunk_lines_list = []
                current_chunk_tokens = 0
            print(f"      [Lvl 1] Line too long ({len(line_text)} chars). Passing to Lvl 2.")
            final_tts_chunks.extend(_split_by_sentence_groups(line_text, token_limit, avg_chars_token_est))
        elif current_chunk_tokens + estimated_line_tokens <= token_limit:
            current_chunk_lines_list.append(line_text)
            current_chunk_tokens += estimated_line_tokens
        else:
            if current_chunk_lines_list:
                final_tts_chunks.append("\n".join(current_chunk_lines_list))
            current_chunk_lines_list = [line_text]
            current_chunk_tokens = estimated_line_tokens

    if current_chunk_lines_list:
        final_tts_chunks.append("\n".join(current_chunk_lines_list))
    return [chunk for chunk in final_tts_chunks if chunk and chunk.strip()]

def download_audio_chunk(server_base_url, relative_audio_url, local_temp_path):
    try:
        full_url = server_base_url.rstrip('/') + "/" + relative_audio_url.lstrip('/')
        # print(f"      Downloading: {full_url}")
        response = requests.get(full_url, stream=True, timeout=300) 
        response.raise_for_status()
        with open(local_temp_path, 'wb') as f:
            shutil.copyfileobj(response.raw, f)
        if os.path.exists(local_temp_path) and os.path.getsize(local_temp_path) > 100: 
            return True
        else:
            print(f"      Error: Downloaded file invalid.")
            if os.path.exists(local_temp_path): os.remove(local_temp_path) 
            return False
    except KeyboardInterrupt:
        raise
    except Exception as e:
        print(f"      Error downloading: {e}")
        return False

def concatenate_audio_chunks(chunk_filepaths, final_output_path):
    if not PYDUB_AVAILABLE: return False
    if not chunk_filepaths: return False
    print(f"  Concatenating {len(chunk_filepaths)} chunks...")
    combined = AudioSegment.empty()
    for filepath in sorted(chunk_filepaths): 
        try:
            combined += AudioSegment.from_wav(filepath) 
        except CouldntDecodeError:
            print(f"      Error: Corrupt chunk {filepath}. Skipping.")
    
    if len(combined) > 0:
        combined.export(final_output_path, format=OUTPUT_FORMAT) 
        print(f"  Saved to: {final_output_path}")
        return True
    return False

def process_chapter_file(text_filepath, final_audio_output_path):
    if not XTTS_SPEAKER_WAV:
        print("[Error] XTTS_SPEAKER_WAV is not set.")
        return False

    print(f"\n--- Processing: {os.path.basename(text_filepath)} ---")
    base_filename_no_ext = os.path.splitext(os.path.basename(text_filepath))[0]
    sanitized_base = re.sub(r'[^\w_.-]', '_', base_filename_no_ext)
    
    chapter_temp_dir = os.path.join(TEMP_CHUNK_DIR, sanitized_base) 
    os.makedirs(chapter_temp_dir, exist_ok=True)

    try:
        with open(text_filepath, 'r', encoding='utf-8') as f:
            full_text_content = f.read()
        full_text_content = normalize_text(full_text_content)
        if not full_text_content.strip():
            print(f"  Skipping empty file.")
            return True
    except Exception as e:
        print(f"  Error reading file: {e}")
        return False

    initial_text_chunks = _split_by_line_groups(full_text_content, FALLBACK_TOKEN_LIMIT, AVG_CHARS_PER_TOKEN)
    if not initial_text_chunks:
        print(f"  Warning: No text chunks generated.")
        return False

    pending_jobs = []
    for i, text_content in enumerate(initial_text_chunks):
        pending_jobs.append({
            "text": text_content,
            "output_suffix": f"l_{i+1:03d}", 
            "fallback_level": 1
        })

    generated_audio_files = [] 
    any_chunk_failed_or_skipped = False 
    job_idx = 0
    
    while job_idx < len(pending_jobs):
        current_job = pending_jobs[job_idx]
        text_to_process = current_job["text"]
        output_suffix = current_job["output_suffix"]
        fallback_level = current_job.get("fallback_level", 1)
        
        chunk_output_basename = f"{sanitized_base}_{output_suffix}"
        local_chunk_filepath = os.path.join(chapter_temp_dir, f"{chunk_output_basename}.{OUTPUT_FORMAT}")

        if os.path.exists(local_chunk_filepath) and os.path.getsize(local_chunk_filepath) > 100:
            generated_audio_files.append(local_chunk_filepath)
            job_idx += 1
            continue

        # --- PREPARE PAYLOAD ---
        payload = {
            "text_input": text_to_process,
            "character_voice_gen": XTTS_SPEAKER_WAV, # Confirmed filename only
            "language": XTTS_LANGUAGE,
            "output_file_name": chunk_output_basename,
            "rvccharacter_voice_gen": RVC_MODEL_NAME_FOR_API if RVC_ENABLE else "", 
            "rvccharacter_pitch": RVC_PITCH,
            "speed": SPEED 
        }
        
        try:
            # --- DEBUG: UNCOMMENT TO SEE EXACTLY WHAT IS SENT ---
            # print(f"DEBUG: Sending Payload: {json.dumps(payload, indent=2)}")
            
            response = requests.post(ALLTALK_API_URL, data=payload, timeout=720)
            response.raise_for_status()
            response_data = response.json()

            if response_data.get('output_file_url'):
                if download_audio_chunk(ALLTALK_BASE_URL, response_data['output_file_url'], local_chunk_filepath):
                    generated_audio_files.append(local_chunk_filepath)
                else:
                    raise Exception("Download failed.")
            else:
                # Print the payload if we get an API error to debug
                print(f"[!] API Error. Payload Sent: {json.dumps(payload)}")
                raise Exception(f"API Error Response: {response_data.get('error')}")

            job_idx += 1 
            time.sleep(0.1)

        except Exception as e:
            print(f"      [!!] Error: {e}")
            time.sleep(2)
            
            new_sub_jobs = []
            if fallback_level == 1:
                print(f"      -> Falling back to Lvl 2 (Sentence Split)")
                chunks = _split_by_sentence_groups(text_to_process, FALLBACK_TOKEN_LIMIT, AVG_CHARS_PER_TOKEN)
                for i, c in enumerate(chunks):
                    new_sub_jobs.append({"text": c, "output_suffix": f"{output_suffix}_s_{i+1:02d}", "fallback_level": 2})
            elif fallback_level == 2:
                print(f"      -> Falling back to Lvl 3 (Force Split)")
                chunks = _split_by_force_chars(text_to_process, FALLBACK_CHAR_LIMIT)
                for i, c in enumerate(chunks):
                    new_sub_jobs.append({"text": c, "output_suffix": f"{output_suffix}_f_{i+1:02d}", "fallback_level": 3})

            if new_sub_jobs:
                pending_jobs = pending_jobs[:job_idx] + new_sub_jobs + pending_jobs[job_idx+1:]
                continue 
            else:
                print(f"      [Fail] Could not recover. Skipping chunk.")
                any_chunk_failed_or_skipped = True
                with open(LOG_FILE, "a", encoding="utf-8") as log_f:
                    log_f.write(f"FAILED: {output_suffix}\nText: {text_to_process}\n\n")
                job_idx += 1

    if not generated_audio_files: return False
    
    if concatenate_audio_chunks(generated_audio_files, final_audio_output_path):
        if not any_chunk_failed_or_skipped:
            try: shutil.rmtree(chapter_temp_dir)
            except: pass
        return True
    return False

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="AllTalk TTS Generator")
    parser.add_argument("--voice_filename", type=str, required=True, help="Filename of the XTTS reference WAV")
    parser.add_argument("--rvc_model", type=str, default=None, help="Name/Path of the RVC model")
    args = parser.parse_args()

    # --- CRITICAL FIX: SANITIZE INPUT ---
    if args.voice_filename:
        # Take the raw input and strip EVERYTHING except the filename
        raw_input = args.voice_filename
        XTTS_SPEAKER_WAV = os.path.basename(raw_input)
        
        print(f"--------------------------------------------------")
        print(f"DEBUG CONFIG CHECK:")
        print(f"Raw Input: '{raw_input}'")
        print(f"Sanitized: '{XTTS_SPEAKER_WAV}'")
        print(f"--------------------------------------------------")
        
    else:
        print("Error: --voice_filename is required.")
        sys.exit(1)

    if args.rvc_model and args.rvc_model.lower() != "none" and args.rvc_model != "":
        RVC_ENABLE = True
        RVC_MODEL_NAME_FOR_API = args.rvc_model
        print(f"[Config] RVC Enabled: {RVC_MODEL_NAME_FOR_API}")
    else:
        RVC_ENABLE = False
        RVC_MODEL_NAME_FOR_API = ""

    if not os.path.exists(TEMP_CHUNK_DIR): os.makedirs(TEMP_CHUNK_DIR)
    if not os.path.exists(AUDIO_OUTPUT_DIR): os.makedirs(AUDIO_OUTPUT_DIR)
    
    text_files = sorted(glob.glob(os.path.join(TEXT_FILES_DIR, "*.txt")))
    if not text_files:
        print(f"No .txt files found in {TEXT_FILES_DIR}"); exit(1)

    print(f"Found {len(text_files)} files.")
    
    chapters_succeeded = 0
    chapters_failed = 0

    try: 
        for idx, text_file_path in enumerate(text_files):
            if CHAPTER_STOP > 0 and (idx + 1) > CHAPTER_STOP: break
            base_name = os.path.splitext(os.path.basename(text_file_path))[0]
            clean_name = re.sub(r'[^\w_.-]', '_', base_name)
            out_path = os.path.join(AUDIO_OUTPUT_DIR, f"{clean_name}.{OUTPUT_FORMAT}")

            if os.path.exists(out_path) and os.path.getsize(out_path) > 1024:
                print(f"Skipping {clean_name} (Exists)")
                chapters_succeeded += 1
                continue
            
            if process_chapter_file(text_file_path, out_path):
                chapters_succeeded += 1
            else:
                chapters_failed += 1
            
    except KeyboardInterrupt: 
        print("\n[STOPPED] User interrupted.")

--- END OF FILE: ./alltalk_tts_generator_chunky_17.py ---

--- START OF FILE: ./constants.py ---
GEMINI_MODEL_NAME = "gemini-3-flash-preview"
CONFIG_FILE = "alltalk_path_config.json"

--- END OF FILE: ./constants.py ---

--- START OF FILE: ./test_env.py ---
import torch
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import undetected_chromedriver as uc
import whisper

def print_separator():
    print("\n" + "="*50 + "\n")

def test_pytorch_cuda():
    print("🧪 TESTING PYTORCH & CUDA...")
    print(f"PyTorch Version: {torch.__version__}")
    cuda_available = torch.cuda.is_available()
    print(f"CUDA Available: {cuda_available}")
    
    if cuda_available:
        print(f"GPU Device: {torch.cuda.get_device_name(0)}")
        # Test a simple tensor operation on GPU
        x = torch.rand(5, 3).cuda()
        print("✅ Success: Tensor successfully loaded to GPU.")
    else:
        print("⚠️ Warning: CUDA is not available. PyTorch is using CPU.")

def test_scraping_tools():
    print("🧪 TESTING SCRAPING TOOLS (BeautifulSoup)...")
    html_doc = "<html><head><title>The Scraper</title></head><body><p class='title'><b>Success!</b></p></body></html>"
    soup = BeautifulSoup(html_doc, 'html.parser')
    print(f"BeautifulSoup parsed title: {soup.title.string}")
    print("✅ Success: BeautifulSoup is working.")

def test_whisper():
    print("🧪 TESTING WHISPER AI...")
    model_size = "tiny" # Using tiny model for quick download and test
    print(f"Loading Whisper '{model_size}' model...")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    try:
        model = whisper.load_model(model_size, device=device)
        print(f"✅ Success: Whisper loaded on {device.upper()}.")
    except Exception as e:
        print(f"❌ Whisper failed to load: {e}")

if __name__ == "__main__":
    print_separator()
    print("🚀 STARTING ENVIRONMENT DIAGNOSTICS")
    print_separator()
    
    test_pytorch_cuda()
    print_separator()
    
    test_scraping_tools()
    print_separator()
    
    test_whisper()
    print_separator()
    
    print("Diagnostics complete! You are ready to scrape. 🕷️")

--- END OF FILE: ./test_env.py ---

--- START OF FILE: ./convert_audio_to_opus_3.py ---
import os
import glob
import sys
from pydub import AudioSegment

# --- 1. WINDOWS UNICODE FIX ---
if sys.platform == "win32":
    sys.stdout.reconfigure(encoding='utf-8')

# --- Configuration ---
# 1. Dynamic Inputs from GUI
# Defaults are fallbacks for testing
WAV_AUDIO_DIR = os.getenv("WAV_AUDIO_DIR", "generated_audio_MistakenFairy")
OPUS_OUTPUT_DIR = os.getenv("OPUS_OUTPUT_DIR", "generated_audio_MistakenFairy_opus")

# Opus Export Settings
# 48k is excellent for speech; 32k is the sweet spot for file size vs quality.
OPUS_BITRATE = "48k" 

# Normalization Settings
ENABLE_NORMALIZATION = True  
NORMALIZATION_TARGET_DBFS = -20.0  # Industry standard for clear, consistent narration.

DELETE_ORIGINAL_WAV = False # Keep as False until you verify the Opus quality
# --- End Configuration ---

def normalize_audio(sound, target_dbfs):
    """Normalizes a pydub AudioSegment object to target dBFS."""
    if sound.dBFS == float('-inf'):
        print("   Warning: Audio segment is silent, skipping normalization.")
        return sound
    change_in_dbfs = target_dbfs - sound.dBFS
    return sound.apply_gain(change_in_dbfs)

def convert_wav_to_opus(wav_filepath, opus_filepath, bitrate="48k", apply_normalization=False, target_dbfs=-20.0):
    """Converts a WAV file to Opus, optionally normalizing and converting to mono."""
    try:
        print(f"Processing: {os.path.basename(wav_filepath)} -> {os.path.basename(opus_filepath)}")
        audio = AudioSegment.from_wav(wav_filepath)

        # 1. Apply Normalization
        if apply_normalization:
            print(f"   Normalizing to {target_dbfs} dBFS...")
            audio = normalize_audio(audio, target_dbfs)

        # 2. Force Mono
        # Audiobooks don't need stereo. Mono cuts Opus file size in half without losing quality.
        if audio.channels > 1:
            print(f"   Info: Converting to mono for smaller file size.")
            audio = audio.set_channels(1)

        # 3. Export to Opus
        # Uses libopus codec via ffmpeg
        print(f"   Exporting Opus ({bitrate})...")
        audio.export(opus_filepath, format="opus", parameters=["-c:a", "libopus", "-b:a", bitrate])

        print(f"   Success.")
        return True
    except Exception as e:
        print(f"   Error processing {wav_filepath}: {e}")
        return False

if __name__ == "__main__":
    print(f"--- Audio Processing & Opus Conversion ---")
    print(f"Input: {WAV_AUDIO_DIR}")
    print(f"Output: {OPUS_OUTPUT_DIR}")
    
    if not os.path.isdir(WAV_AUDIO_DIR):
        print(f"Error: Input directory '{WAV_AUDIO_DIR}' not found.")
        # If run via GUI, we exit cleanly so the pipe catches the error
        sys.exit(1)

    if not os.path.exists(OPUS_OUTPUT_DIR):
        os.makedirs(OPUS_OUTPUT_DIR)

    # Search for files matching any WAV pattern (handling different naming conventions)
    wav_files = sorted(glob.glob(os.path.join(WAV_AUDIO_DIR, "*.wav")))

    if not wav_files:
        print(f"No WAV files found in '{WAV_AUDIO_DIR}'.")
        sys.exit(0) # Not an error, just nothing to do

    print(f"Found {len(wav_files)} WAV files. Normalization: {'ON' if ENABLE_NORMALIZATION else 'OFF'}")

    processed = 0
    skipped = 0
    failed = 0

    for wav_path in wav_files:
        filename_no_ext = os.path.splitext(os.path.basename(wav_path))[0]
        opus_path = os.path.join(OPUS_OUTPUT_DIR, f"{filename_no_ext}.opus")

        # Skip if already converted
        if os.path.exists(opus_path):
            print(f"Skipping: '{filename_no_ext}.opus' already exists.")
            skipped += 1
            continue

        success = convert_wav_to_opus(
            wav_path, 
            opus_path,
            bitrate=OPUS_BITRATE,
            apply_normalization=ENABLE_NORMALIZATION,
            target_dbfs=NORMALIZATION_TARGET_DBFS
        )

        if success:
            processed += 1
            if DELETE_ORIGINAL_WAV:
                try:
                    os.remove(wav_path)
                    print(f"   Deleted original WAV.")
                except Exception as e:
                    print(f"   Warning: Could not delete WAV: {e}")
        else:
            failed += 1

    print(f"\n--- Done ---")
    print(f"Processed: {processed} | Skipped: {skipped} | Failed: {failed}")

--- END OF FILE: ./convert_audio_to_opus_3.py ---

--- START OF FILE: ./txt_to_epub.py ---
import os
import re
import json
import uuid
import html
import sys
from ebooklib import epub

def create_xhtml_chapter(chapter_title, text_content, chapter_file_name_base):
    """
    Converts plain text content to a simple XHTML chapter object.
    """
    file_name = f"{chapter_file_name_base}.xhtml"
    chapter = epub.EpubHtml(title=chapter_title, file_name=file_name, lang='en')

    # Create HTML content
    escaped_title = html.escape(chapter_title)
    xhtml_content_parts = [f"<h1>{escaped_title}</h1>"]
    
    # Split by blank lines to form paragraphs
    # Handle various line ending types
    paragraphs = re.split(r'\n\s*\n+', text_content.strip())
    
    for para_text in paragraphs:
        cleaned_para = para_text.strip()
        if cleaned_para: 
            escaped_para = html.escape(cleaned_para)
            # Preserve internal line breaks within a paragraph
            escaped_para = escaped_para.replace('\r\n', '<br />\n').replace('\n', '<br />\n')
            xhtml_content_parts.append(f"<p>{escaped_para}</p>")
            
    chapter.content = "\n".join(xhtml_content_parts)
    return chapter

def create_epub_project():
    # --- CRITICAL FIX FOR WINDOWS UNICODE ERROR ---
    # Forces the console output to use UTF-8 instead of the default Windows cp1252
    if sys.platform == "win32":
        sys.stdout.reconfigure(encoding='utf-8')

    print("--- Starting EPUB Creation ---")

    # --- 1. Setup Paths ---
    # The GUI passes 'EPUB_INPUT_DIR' (e.g., Novels/MyBook/01_Raw_Text)
    # The GUI passes 'EPUB_OUTPUT_FILE' (e.g., Novels/MyBook/MyBook.epub)
    
    TEXT_INPUT_DIR = os.getenv("EPUB_INPUT_DIR")
    OUTPUT_FILE = os.getenv("EPUB_OUTPUT_FILE")

    # Standalone fallback (for testing without GUI)
    if not TEXT_INPUT_DIR:
        print("Warning: Running standalone. Using default relative paths.")
        TEXT_INPUT_DIR = "BlleatTL_Novels" 
        OUTPUT_FILE = "Output.epub"

    if not os.path.exists(TEXT_INPUT_DIR):
        print(f"Error: Input directory not found: {TEXT_INPUT_DIR}")
        return

    # Determine Project Root (One level up from text dir)
    # If text dir is ".../Novels/Title/01_Raw_Text", root is ".../Novels/Title"
    PROJECT_ROOT = os.path.dirname(os.path.abspath(TEXT_INPUT_DIR))
    
    METADATA_JSON = os.path.join(PROJECT_ROOT, "metadata.json")
    COVER_IMAGE = os.path.join(PROJECT_ROOT, "cover.jpg")
    CHAPTERS_JSON = os.path.join(TEXT_INPUT_DIR, "chapters.json")

    # --- 2. Load Metadata ---
    # Default values
    book_meta = {
        "title": os.getenv("EPUB_TITLE", "Unknown Title"),
        "author": "Unknown Author",
        "description": "Generated by Auto-Audiobook Pipeline"
    }

    # Override with metadata.json if exists
    if os.path.exists(METADATA_JSON):
        print(f"Loading metadata from: {METADATA_JSON}")
        try:
            with open(METADATA_JSON, 'r', encoding='utf-8') as f:
                loaded_meta = json.load(f)
                # Only update keys that contain data
                if loaded_meta.get("title"): book_meta["title"] = loaded_meta["title"]
                if loaded_meta.get("author"): book_meta["author"] = loaded_meta["author"]
                if loaded_meta.get("description"): book_meta["description"] = loaded_meta["description"]
        except Exception as e:
            print(f"Warning: Failed to parse metadata.json: {e}")

    # Safe Print to avoid crash if reconfigure fails for some reason
    try:
        print(f"Book Title: {book_meta['title']}")
        print(f"Book Author: {book_meta['author']}")
    except UnicodeEncodeError:
        print(f"Book Author: [Complex Characters Hidden]")

    # --- 3. Setup EPUB Book Object ---
    book = epub.EpubBook()
    book.set_identifier(str(uuid.uuid4()))
    book.set_title(book_meta['title'])
    book.set_language('en')
    book.add_author(book_meta['author'])
    
    if book_meta['description']:
        book.add_metadata('DC', 'description', book_meta['description'])

    # Handle Cover
    if os.path.exists(COVER_IMAGE):
        try:
            with open(COVER_IMAGE, 'rb') as f:
                book.set_cover("cover.jpg", f.read())
            print(f"Attached cover image: {COVER_IMAGE}")
        except Exception as e:
            print(f"Error attaching cover: {e}")
    else:
        print("No cover.jpg found in project root.")

    # --- 4. Load Chapter Order ---
    chapter_entries = []
    
    # Try loading strictly ordered list from scraper
    if os.path.exists(CHAPTERS_JSON):
        try:
            with open(CHAPTERS_JSON, 'r', encoding='utf-8') as f:
                data = json.load(f)
                chapter_entries = [entry for entry in data if entry.get('file')]
            print(f"Loaded order from chapters.json ({len(chapter_entries)} chapters)")
        except Exception as e:
            print(f"Error loading chapters.json: {e}")
    
    # Fallback: Alphabetical sort of text files
    if not chapter_entries:
        print("Falling back to alphabetical file sort.")
        txt_files = sorted([f for f in os.listdir(TEXT_INPUT_DIR) if f.lower().endswith('.txt')])
        chapter_entries = [{'file': f} for f in txt_files]

    if not chapter_entries:
        print("No .txt files found to compile.")
        return

    # --- 5. Process Chapters ---
    epub_chapters = []
    toc_links = []      

    print(f"Processing text files...")
    for i, entry in enumerate(chapter_entries):
        txt_filename = entry['file']
        txt_filepath = os.path.join(TEXT_INPUT_DIR, txt_filename)
        
        if not os.path.exists(txt_filepath):
            print(f"  [Skipped] Missing file: {txt_filename}")
            continue

        try:
            with open(txt_filepath, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                if not lines: continue
                
                # Heuristic: The first line is often the Title (saved by scraper)
                # If the first line is very short, treat it as title. Otherwise default.
                first_line = lines[0].strip()
                if len(first_line) < 200:
                    final_title = first_line
                    body_content = "".join(lines[1:]).strip()
                else:
                    final_title = f"Chapter {i+1}"
                    body_content = "".join(lines).strip()

            # Create XHTML Chapter Object
            chapter_obj = create_xhtml_chapter(final_title, body_content, f"chapter_{i+1:04d}")
            book.add_item(chapter_obj)
            epub_chapters.append(chapter_obj)
            toc_links.append(epub.Link(chapter_obj.file_name, final_title, f"chapter_{i+1:04d}"))
            
        except Exception as e:
            print(f"  [Error] Failed to process {txt_filename}: {e}")

    # --- 6. Finalize EPUB ---
    book.toc = tuple(toc_links) 
    book.add_item(epub.EpubNcx())
    book.add_item(epub.EpubNav())
    
    # CSS Styling
    css = '''
    body { margin: 5%; font-family: serif; font-size: 1.1em; line-height: 1.6; }
    h1 { text-align: center; margin-top: 2em; margin-bottom: 1em; font-weight: bold; border-bottom: 1px solid #ddd; padding-bottom: 0.5em;}
    p { text-indent: 1.5em; margin-bottom: 0.5em; text-align: justify; }
    '''
    style_item = epub.EpubItem(uid="style_default", file_name="style/default.css", media_type="text/css", content=css)
    book.add_item(style_item)
    
    book.spine = ['nav'] + epub_chapters
    for ch in epub_chapters: ch.add_item(style_item)

    # Write file
    try:
        epub.write_epub(OUTPUT_FILE, book, {})
        print(f"SUCCESS! EPUB saved to: {OUTPUT_FILE}")
    except Exception as e:
        print(f"Error saving EPUB file: {e}")

if __name__ == "__main__":
    create_epub_project()

--- END OF FILE: ./txt_to_epub.py ---

--- START OF FILE: ./fetch_metadata.py ---
import os
import json
import requests
import shutil
from bs4 import BeautifulSoup

# --- CONFIGURATION ---
# The URL of the main page (Novel Info / Table of Contents)
NOVEL_INDEX_URL = "https://www.blleattl.site/story/fairy/" 
# ---------------------

def fetch_and_save_metadata(index_url, project_dir):
    print(f"--- Fetching Metadata from: {index_url} ---")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    }

    try:
        response = requests.get(index_url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')

        # --- 1. Extract Metadata using OpenGraph (Most Reliable) ---
        metadata = {
            "title": "Unknown Title",
            "author": "Unknown Author",
            "cover_url": None,
            "description": ""
        }

        # Title
        og_title = soup.find("meta", property="og:title")
        if og_title:
            metadata["title"] = og_title["content"].strip()
        else:
            # Fallback to standard H1
            h1 = soup.find("h1")
            if h1: metadata["title"] = h1.get_text(strip=True)

        # Cover Image
        og_image = soup.find("meta", property="og:image")
        if og_image:
            metadata["cover_url"] = og_image["content"]

        # Author (Site specific fallback)
        # Try to find common "Author" labels
        author_el = soup.find(string=lambda t: t and "Author" in t)
        if author_el:
            # Often extracting parent text helps, e.g. <div>Author: Name</div>
            parent_text = author_el.find_parent().get_text(strip=True)
            # Simple cleanup: remove "Author" and colons
            metadata["author"] = parent_text.replace("Author", "").replace(":", "").strip()
        
        # Description
        og_desc = soup.find("meta", property="og:description")
        if og_desc:
            metadata["description"] = og_desc["content"].strip()

        print(f"Found Metadata:\n Title: {metadata['title']}\n Author: {metadata['author']}\n Cover: {metadata['cover_url']}")

        # --- 2. Download Cover Image ---
        cover_filename = None
        if metadata["cover_url"]:
            try:
                # Resolve relative URLs if necessary
                if not metadata["cover_url"].startswith("http"):
                    from urllib.parse import urljoin
                    metadata["cover_url"] = urljoin(index_url, metadata["cover_url"])

                print(f"Downloading cover image...")
                img_resp = requests.get(metadata["cover_url"], headers=headers, stream=True)
                if img_resp.status_code == 200:
                    ext = os.path.splitext(metadata["cover_url"])[1].split('?')[0] # Get .jpg/.png
                    if not ext: ext = ".jpg"
                    
                    cover_filename = f"cover{ext}"
                    cover_path = os.path.join(project_dir, cover_filename)
                    
                    with open(cover_path, 'wb') as f:
                        shutil.copyfileobj(img_resp.raw, f)
                    
                    print(f"Cover saved to: {cover_path}")
                    metadata["local_cover_path"] = cover_filename
            except Exception as e:
                print(f"Failed to download cover: {e}")

        # --- 3. Save to JSON ---
        json_path = os.path.join(project_dir, "project_metadata.json")
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(metadata, f, indent=4)
        
        print(f"--- Success! Metadata saved to {json_path} ---")

    except Exception as e:
        print(f"Error fetching metadata: {e}")

if __name__ == "__main__":
    # Determine base directory dynamically
    BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    
    # We save metadata in the Project Root (BASE_DIR), not the text folder
    fetch_and_save_metadata(NOVEL_INDEX_URL, BASE_DIR)

--- END OF FILE: ./fetch_metadata.py ---

--- START OF FILE: ./pipe_system_gui.py ---
import tkinter as tk
from tkinter import ttk, scrolledtext, messagebox, simpledialog, filedialog
import subprocess
import threading
import sys
import os
import shutil
import json
import glob

# --- CONFIGURATION ---
NOVELS_ROOT_DIR = "Novels"
CONFIG_FILE = "alltalk_path_config.json"

try:
    from constants import GEMINI_MODEL_NAME
except ImportError:
    GEMINI_MODEL_NAME = "gemini-3-flash-preview"

SCRIPTS = {
    "Scraper": "scraper_2.py",
    "Metadata": "metadata_fetcher.py",
    "Translate (Gemini)": "gemini_transelate_4.py",
    "Translate (Grok)": "grok_transelate.py",
    "TTS Generator": "alltalk_tts_generator_chunky_17.py",
    "Audio Converter": "convert_audio_to_opus_3.py",
    "Tag Audio": "tag_audiobook_files_opus_3.py",
    "EPUB Creator": "txt_to_epub.py",
}

class PipelineGUI:
    def __init__(self, root):
        self.root = root
        self.root.title("Audiobook Pipe System")
        self.root.geometry("900x900")
        
        if not os.path.exists(NOVELS_ROOT_DIR):
            os.makedirs(NOVELS_ROOT_DIR)

        self.current_project = tk.StringVar()
        self.index_url = tk.StringVar() # For Metadata
        
        # Source Selection
        self.input_source_var = tk.StringVar(value="Raw") 

        # AllTalk Config Vars
        self.alltalk_path_var = tk.StringVar()
        self.selected_voice_var = tk.StringVar()
        self.selected_rvc_var = tk.StringVar()

        self.pipeline_vars = {
            "scraper": tk.BooleanVar(value=True),
            "translate": tk.BooleanVar(value=False),
            "epub": tk.BooleanVar(value=True),
            "tts": tk.BooleanVar(value=True),
            "convert": tk.BooleanVar(value=True),
            "tag": tk.BooleanVar(value=True),
        }
        self.trans_engine = tk.StringVar(value="Translate (Gemini)")
        
        # Adapt Vars
        self.adapt_url_var = tk.StringVar()
        self.adapt_type_var = tk.StringVar(value="Chapter Scraper")
        
        self.current_process = None
        self.stop_requested = False

        # Load Config
        self.load_config()

        self.create_ui()
        self.refresh_project_list()

        # Initial Scan
        if self.alltalk_path_var.get():
            self.scan_alltalk_content()

    def create_ui(self):
        # --- TOP BAR ---
        top_frame = ttk.LabelFrame(self.root, text="Project Management")
        top_frame.pack(fill="x", padx=10, pady=5)
        
        ttk.Label(top_frame, text="Project:").pack(side="left", padx=5)
        self.project_dropdown = ttk.Combobox(top_frame, textvariable=self.current_project, state="readonly", width=25)
        self.project_dropdown.pack(side="left", padx=5)
        self.project_dropdown.bind("<<ComboboxSelected>>", self.on_project_change)

        ttk.Button(top_frame, text="New", command=self.create_new_project, width=5).pack(side="left", padx=2)
        ttk.Button(top_frame, text="Folder", command=self.open_project_folder, width=6).pack(side="left", padx=2)
        
        # Metadata URL Input
        ttk.Label(top_frame, text=" |  Index URL:").pack(side="left", padx=5)
        ttk.Entry(top_frame, textvariable=self.index_url, width=30).pack(side="left", padx=5)
        ttk.Button(top_frame, text="Get Meta", command=self.run_metadata_fetch).pack(side="left", padx=2)

        # --- TABS ---
        tabs = ttk.Notebook(self.root)
        self.tab_run = ttk.Frame(tabs)
        self.tab_adapt = ttk.Frame(tabs)
        tabs.add(self.tab_run, text="Run Pipeline")
        tabs.add(self.tab_adapt, text="AI Adapter")
        tabs.pack(expand=True, fill="both", padx=10, pady=5)

        # --- TAB 1: PIPELINE ---
        
        # 1. AllTalk Integration Section (New)
        at_frame = ttk.LabelFrame(self.tab_run, text="External AllTalk TTS Setup")
        at_frame.pack(fill="x", padx=10, pady=5)

        # Path Selection
        path_frame = ttk.Frame(at_frame)
        path_frame.pack(fill="x", padx=5, pady=5)
        
        ttk.Label(path_frame, text="AllTalk Root Dir:").pack(side="left")
        ttk.Entry(path_frame, textvariable=self.alltalk_path_var).pack(side="left", padx=5, fill="x", expand=True)
        ttk.Button(path_frame, text="Browse", command=self.browse_alltalk).pack(side="left")
        ttk.Button(path_frame, text="Scan Voices", command=self.scan_alltalk_content).pack(side="left", padx=5)

        # Dropdowns
        opts_frame = ttk.Frame(at_frame)
        opts_frame.pack(fill="x", padx=5, pady=5)

        ttk.Label(opts_frame, text="XTTS Voice:").pack(side="left")
        self.voice_combo = ttk.Combobox(opts_frame, textvariable=self.selected_voice_var, state="readonly", width=30)
        self.voice_combo.pack(side="left", padx=5)

        ttk.Label(opts_frame, text="RVC Model:").pack(side="left", padx=(15, 0))
        self.rvc_combo = ttk.Combobox(opts_frame, textvariable=self.selected_rvc_var, state="readonly", width=35)
        self.rvc_combo.pack(side="left", padx=5)

        # 2. Source Selection Frame
        source_frame = ttk.LabelFrame(self.tab_run, text="Source Content for TTS & EPUB")
        source_frame.pack(fill="x", padx=10, pady=5)
        
        rb1 = ttk.Radiobutton(source_frame, text="Original Scraped Text (01_Raw_Text)", variable=self.input_source_var, value="Raw")
        rb1.pack(side="left", padx=20, pady=5)
        
        rb2 = ttk.Radiobutton(source_frame, text="Translated Text (02_Translated)", variable=self.input_source_var, value="Translated")
        rb2.pack(side="left", padx=20, pady=5)

        # 3. Steps Selection
        chk_frame = ttk.LabelFrame(self.tab_run, text="Select Steps")
        chk_frame.pack(fill="x", padx=10, pady=5)

        steps = [
            ("1. Scrape Chapters", "scraper"),
            ("2. Translate (Optional)", "translate"),
            ("3. Create EPUB", "epub"),
            ("4. Generate TTS", "tts"),
            ("5. Convert to Opus", "convert"),
            ("6. Tag Audio", "tag")
        ]

        for i, (text, key) in enumerate(steps):
            row = i // 2
            col = i % 2
            if key == "translate":
                f = ttk.Frame(chk_frame)
                f.grid(row=row, column=col, sticky="w", padx=10, pady=2)
                ttk.Checkbutton(f, text=text, variable=self.pipeline_vars[key]).pack(side="left")
                ttk.Combobox(f, textvariable=self.trans_engine, values=["Translate (Gemini)", "Translate (Grok)"], state="readonly", width=15).pack(side="left", padx=5)
            else:
                ttk.Checkbutton(chk_frame, text=text, variable=self.pipeline_vars[key]).grid(row=row, column=col, sticky="w", padx=10, pady=2)

        # 4. Control Buttons
        btn_frame = ttk.Frame(self.tab_run)
        btn_frame.pack(pady=10, fill="x", padx=50)
        self.btn_run = ttk.Button(btn_frame, text="START PROCESSING", command=self.start_pipeline_thread)
        self.btn_run.pack(side="left", fill="x", expand=True, padx=5)
        self.btn_stop = ttk.Button(btn_frame, text="STOP / TERMINATE", command=self.stop_process, state="disabled")
        self.btn_stop.pack(side="right", fill="x", expand=True, padx=5)

        # 5. Logs
        log_label_frame = ttk.LabelFrame(self.tab_run, text="Process Logs")
        log_label_frame.pack(fill="both", expand=True, padx=10, pady=5)
        
        self.log_area = scrolledtext.ScrolledText(log_label_frame, height=15, state='normal', font=("Consolas", 9))
        self.log_area.pack(fill="both", expand=True, padx=5, pady=5)
        self.log_area.bind("<Key>", self.prevent_typing) 

        ttk.Button(log_label_frame, text="Copy All Logs", command=self.copy_all_logs).pack(pady=2)

        # --- TAB 2: ADAPTER ---
        lbl = ttk.Label(self.tab_adapt, text="Use AI to write custom scripts for new websites.", font=("Arial", 10, "bold"))
        lbl.pack(pady=10)
        
        adapt_frame = ttk.Frame(self.tab_adapt)
        adapt_frame.pack(pady=5)
        
        ttk.Label(adapt_frame, text="Target URL:").grid(row=0, column=0, padx=5, sticky="e")
        ttk.Entry(adapt_frame, textvariable=self.adapt_url_var, width=50).grid(row=0, column=1, padx=5)
        
        ttk.Label(adapt_frame, text="Generate For:").grid(row=1, column=0, padx=5, sticky="e")
        ttk.Combobox(adapt_frame, textvariable=self.adapt_type_var, values=["Chapter Scraper", "Metadata Scraper"], state="readonly").grid(row=1, column=1, padx=5, sticky="w")
        
        ttk.Button(self.tab_adapt, text="Ask Gemini to Write Script", command=self.run_adapt_tool).pack(pady=15)
        self.adapt_status = ttk.Label(self.tab_adapt, text="Ready", foreground="gray")
        self.adapt_status.pack()

    # --- ALLTALK LOGIC ---
    def load_config(self):
        try:
            if os.path.exists(CONFIG_FILE):
                with open(CONFIG_FILE, "r") as f:
                    data = json.load(f)
                    self.alltalk_path_var.set(data.get("alltalk_path", ""))
        except Exception as e:
            print(f"Config Error: {e}")

    def save_config(self):
        try:
            with open(CONFIG_FILE, "w") as f:
                json.dump({"alltalk_path": self.alltalk_path_var.get()}, f)
        except Exception as e:
            print(f"Config Save Error: {e}")

    def browse_alltalk(self):
        d = filedialog.askdirectory(title="Select AllTalk Root Directory")
        if d:
            self.alltalk_path_var.set(d)
            self.save_config()
            self.scan_alltalk_content()

    def scan_alltalk_content(self):
        base = self.alltalk_path_var.get()
        if not base or not os.path.exists(base):
            return

        # 1. Scan Voices (look in /voices)
        voices_dir = os.path.join(base, "voices")
        if os.path.exists(voices_dir):
            wavs = glob.glob(os.path.join(voices_dir, "*.wav"))
            # Just store the filename!
            voice_names = [os.path.basename(w) for w in wavs]
            self.voice_combo['values'] = voice_names
            if voice_names: 
                self.voice_combo.current(0)
            else:
                self.voice_combo.set("No .wav files found")
        else:
            self.voice_combo.set("Voices dir not found")

        # 2. Scan RVC Models (Look for .pth files inside subdirectories)
        rvc_search_roots = [
            os.path.join(base, "models", "rvc_voices"), 
            os.path.join(base, "rvc_models"),
            os.path.join(base, "models", "rvc")
        ]
        
        rvc_models_found = ["None"]
        
        valid_root = None
        for p in rvc_search_roots:
            if os.path.exists(p):
                valid_root = p
                break
        
        if valid_root:
            try:
                subdirs = [d for d in os.listdir(valid_root) if os.path.isdir(os.path.join(valid_root, d))]
                
                for subdir in subdirs:
                    subdir_path = os.path.join(valid_root, subdir)
                    pth_files = glob.glob(os.path.join(subdir_path, "*.pth"))
                    
                    for pth in pth_files:
                        filename = os.path.basename(pth)
                        # We construct the relative path "folder_name/file_name.pth"
                        rel_path = os.path.join(subdir, filename)
                        rvc_models_found.append(rel_path)
                        
            except Exception as e:
                print(f"Error scanning RVC folders: {e}")
        
        self.rvc_combo['values'] = rvc_models_found
        if len(rvc_models_found) > 1:
             self.rvc_combo.current(1) 
        else:
             self.rvc_combo.current(0)

    # --- GENERAL LOGIC ---
    def prevent_typing(self, event):
        if (event.state & 4) and event.keysym.lower() in ['c', 'a']: return None
        if event.keysym in ["Up", "Down", "Left", "Right", "Home", "End", "Prior", "Next"]: return None
        return "break"

    def log(self, msg):
        print(msg)
        self.log_area.insert(tk.END, msg + "\n")
        self.log_area.see(tk.END)

    def copy_all_logs(self):
        content = self.log_area.get("1.0", tk.END)
        self.root.clipboard_clear()
        self.root.clipboard_append(content)
        messagebox.showinfo("Copied", "Logs copied to clipboard.")

    def refresh_project_list(self):
        projects = [d for d in os.listdir(NOVELS_ROOT_DIR) if os.path.isdir(os.path.join(NOVELS_ROOT_DIR, d))]
        self.project_dropdown['values'] = sorted(projects)
        if projects and not self.current_project.get():
            self.current_project.set(projects[0])

    def create_new_project(self):
        name = simpledialog.askstring("New Project", "Enter Novel Name:")
        if name:
            safe_name = "".join([c for c in name if c.isalnum() or c in (' ', '_')]).strip().replace(" ", "_")
            path = os.path.join(NOVELS_ROOT_DIR, safe_name)
            if not os.path.exists(path):
                os.makedirs(os.path.join(path, "01_Raw_Text"))
                os.makedirs(os.path.join(path, "02_Translated"))
                os.makedirs(os.path.join(path, "03_Audio_WAV"))
                os.makedirs(os.path.join(path, "04_Audio_Opus"))
                self.log(f"Created project: {safe_name}")
                self.refresh_project_list()
                self.current_project.set(safe_name)
            else:
                messagebox.showerror("Error", "Project exists.")

    def on_project_change(self, event):
        self.log(f"Selected: {self.current_project.get()}")

    def open_project_folder(self):
        proj = self.current_project.get()
        if not proj: return
        path = os.path.abspath(os.path.join(NOVELS_ROOT_DIR, proj))
        os.startfile(path) if os.name == 'nt' else subprocess.call(['xdg-open', path])

    def get_env_for_project(self):
        proj = self.current_project.get()
        base = os.path.abspath(os.path.join(NOVELS_ROOT_DIR, proj))
        
        dir_raw = os.path.join(base, "01_Raw_Text")
        dir_trans = os.path.join(base, "02_Translated")
        dir_wav = os.path.join(base, "03_Audio_WAV")
        dir_opus = os.path.join(base, "04_Audio_Opus")
        
        if self.input_source_var.get() == "Translated":
            tts_input = dir_trans
            self.log(f"[Config] Using TRANSLATED text as source.")
        else:
            tts_input = dir_raw
            self.log(f"[Config] Using RAW text as source.")

        env = os.environ.copy()
        env["PROJECT_RAW_TEXT_DIR"] = dir_raw
        env["PROJECT_TRANS_INPUT_DIR"] = dir_raw
        env["PROJECT_TRANS_OUTPUT_DIR"] = dir_trans
        env["PROJECT_INPUT_TEXT_DIR"] = tts_input
        env["PROJECT_AUDIO_WAV_DIR"] = dir_wav
        env["WAV_AUDIO_DIR"] = dir_wav
        env["OPUS_OUTPUT_DIR"] = dir_opus
        env["EPUB_INPUT_DIR"] = tts_input
        env["EPUB_OUTPUT_FILE"] = os.path.join(base, f"{proj}.epub")
        env["EPUB_TITLE"] = proj.replace("_", " ")
        return env

    def stop_process(self):
        if self.current_process and self.current_process.poll() is None:
            self.stop_requested = True
            self.log("\n!!! STOPPING PROCESS... !!!")
            try:
                self.current_process.terminate()
            except: pass

    def run_script(self, script_key):
        if self.stop_requested: return False
        
        script_path = SCRIPTS.get(script_key)
        proj = self.current_project.get()

        if script_key == "Scraper" and proj:
            custom_path = os.path.join(NOVELS_ROOT_DIR, proj, "custom_scraper.py")
            if os.path.exists(custom_path):
                script_path = custom_path
                self.log(f"--- Using Custom Chapter Scraper ---")

        if script_key.startswith("Translate"):
            script_path = SCRIPTS.get(self.trans_engine.get())

        if not script_path or not os.path.exists(script_path):
            self.log(f"Error: {script_path} not found.")
            return False

        self.log(f"--- Running {os.path.basename(script_path)} ---")
        env = self.get_env_for_project()

        cmd = [sys.executable, script_path]

        # --- Inject Arguments for TTS Generator ---
        if script_key == "TTS Generator":
            # [FIX] Force basename here to prevent path leakage
            full_voice_val = self.selected_voice_var.get()
            voice_filename = os.path.basename(full_voice_val) 
            rvc = self.selected_rvc_var.get()
            
            if not voice_filename or "No" in voice_filename:
                self.log("Error: Invalid voice selection.")
                return False
            
            cmd.extend(["--voice_filename", voice_filename])
            
            if rvc and rvc != "None":
                cmd.extend(["--rvc_model", rvc])
        # ------------------------------------------

        try:
            self.current_process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                env=env,
                text=True,
                bufsize=1,
                universal_newlines=True
            )
            for line in self.current_process.stdout:
                if self.stop_requested: 
                    self.current_process.terminate()
                    break
                self.log(line.strip())
            
            self.current_process.wait()
            if self.stop_requested: 
                self.log("--- STOPPED ---")
                return False
            return self.current_process.returncode == 0
        except Exception as e:
            self.log(f"Error: {e}")
            return False
        finally:
            self.current_process = None

    def start_pipeline_thread(self):
        if not self.current_project.get():
            messagebox.showwarning("Warning", "Select a project.")
            return
        self.stop_requested = False
        self.btn_run.config(state="disabled")
        self.btn_stop.config(state="normal")
        threading.Thread(target=self.run_pipeline).start()

    def run_pipeline(self):
        try:
            if self.pipeline_vars["scraper"].get():
                if not self.run_script("Scraper"): raise Exception("Scraping Failed")
            
            if self.pipeline_vars["translate"].get():
                if not self.run_script("Translate"): raise Exception("Translation Failed")

            if self.pipeline_vars["epub"].get():
                if not self.run_script("EPUB Creator"): raise Exception("EPUB Failed")

            if self.pipeline_vars["tts"].get():
                if not self.run_script("TTS Generator"): raise Exception("TTS Failed")

            if self.pipeline_vars["convert"].get():
                if not self.run_script("Audio Converter"): raise Exception("Conversion Failed")

            if self.pipeline_vars["tag"].get():
                if not self.run_script("Tag Audio"): raise Exception("Tagging Failed")

            self.log("=== COMPLETED ===")
        except Exception as e:
            self.log(f"=== PIPELINE ENDED: {e} ===")
        finally:
            self.btn_run.config(state="normal")
            self.btn_stop.config(state="disabled")

    # --- METADATA & ADAPTER LOGIC ---
    def run_metadata_fetch(self):
        proj = self.current_project.get()
        url = self.index_url.get().strip()
        if not proj or not url:
            messagebox.showwarning("Info", "Select Project and enter Index URL.")
            return
        
        self.log(f"--- Fetching Metadata ---")
        
        def _worker():
            try:
                proj_dir = os.path.join(NOVELS_ROOT_DIR, proj)
                cmd = [sys.executable, SCRIPTS["Metadata"], url, proj_dir]
                
                proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)
                for line in proc.stdout:
                    self.log(line.strip())
                proc.wait()
                
                if proc.returncode == 0:
                    self.log("Metadata Fetch Complete.")
                else:
                    self.log("Metadata Fetch Failed.")
            except Exception as e:
                self.log(f"Meta Error: {e}")

        threading.Thread(target=_worker).start()

    def run_adapt_tool(self):
        proj = self.current_project.get()
        url = self.adapt_url_var.get().strip()
        mode = self.adapt_type_var.get()
        
        if not proj or not url:
            messagebox.showerror("Error", "Select project and URL.")
            return
        if not os.environ.get("GEMINI_API_KEY"):
            messagebox.showerror("Error", "GEMINI_API_KEY missing.")
            return

        self.adapt_status.config(text=f"Generating {mode}... wait...", foreground="blue")
        
        def _worker():
            try:
                proj_dir = os.path.join(NOVELS_ROOT_DIR, proj)
                
                if mode == "Chapter Scraper":
                    import scraper_context_fetcher
                    scraper_context_fetcher.fetch_and_generate_scraper(url, proj_dir)
                    target = "custom_scraper.py"
                else:
                    import metadata_fetcher
                    metadata_fetcher.fetch_and_generate_metadata_scraper(url, proj_dir)
                    target = "custom_metadata_scraper.py"
                
                if os.path.exists(os.path.join(proj_dir, target)):
                    self.root.after(0, lambda: self.adapt_status.config(text=f"Success! {target} created.", foreground="green"))
                    self.root.after(0, lambda: messagebox.showinfo("Done", f"Created {target}"))
                else:
                    self.root.after(0, lambda: self.adapt_status.config(text="Generation failed.", foreground="red"))
            except Exception as e:
                self.root.after(0, lambda: self.adapt_status.config(text=f"Error: {e}", foreground="red"))

        threading.Thread(target=_worker).start()

if __name__ == "__main__":
    root = tk.Tk()
    app = PipelineGUI(root)
    root.mainloop()


--- END OF FILE: ./pipe_system_gui.py ---
